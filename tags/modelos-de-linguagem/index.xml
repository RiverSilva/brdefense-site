<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Modelos De Linguagem on BR Defense Center</title><link>https://brdefense.center/tags/modelos-de-linguagem/</link><description>Recent content in Modelos De Linguagem on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Wed, 04 Feb 2026 19:01:22 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/modelos-de-linguagem/index.xml" rel="self" type="application/rss+xml"/><item><title>Microsoft desenvolve scanner para detectar backdoors em LLMs</title><link>https://brdefense.center/news/microsoft-desenvolve-scanner-para-detectar-backdoo/</link><pubDate>Wed, 04 Feb 2026 19:01:22 -0300</pubDate><guid>https://brdefense.center/news/microsoft-desenvolve-scanner-para-detectar-backdoo/</guid><description>&lt;p>A Microsoft anunciou o desenvolvimento de um scanner leve capaz de detectar backdoors em modelos de linguagem de grande escala (LLMs), visando aumentar a confiança em sistemas de inteligência artificial (IA). A equipe de segurança da IA da empresa identificou três sinais observáveis que podem indicar a presença de backdoors, mantendo uma baixa taxa de falsos positivos. Esses sinais incluem padrões de atenção distintos em respostas a frases de gatilho, a memorização de dados de envenenamento e a ativação de backdoors por gatilhos &amp;lsquo;fuzzy&amp;rsquo;. O scanner não requer treinamento adicional e pode ser aplicado em modelos comuns, mas tem limitações, como a incapacidade de funcionar em modelos proprietários. A Microsoft também está expandindo seu Ciclo de Vida de Desenvolvimento Seguro (SDL) para abordar preocupações de segurança específicas da IA, reconhecendo que sistemas de IA criam múltiplos pontos de entrada para inputs inseguros. Essa inovação é um passo significativo para a detecção prática de backdoors, mas a colaboração na comunidade de segurança da IA é essencial para o progresso contínuo.&lt;/p></description></item><item><title>Campanhas de ciberataques visam modelos de linguagem de IA</title><link>https://brdefense.center/news/campanhas-de-ciberataques-visam-modelos-de-linguag/</link><pubDate>Tue, 13 Jan 2026 18:58:59 -0300</pubDate><guid>https://brdefense.center/news/campanhas-de-ciberataques-visam-modelos-de-linguag/</guid><description>&lt;p>Com a crescente popularidade das ferramentas de inteligência artificial (IA), cibercriminosos estão direcionando suas atenções para a exploração de vulnerabilidades em modelos de linguagem de grande escala (LLMs). Pesquisadores da GreyNoise identificaram duas campanhas de ataque que, juntas, contabilizam quase 100 mil tentativas de exploração. Os ataques, que ocorreram entre outubro de 2025 e janeiro de 2026, visaram principalmente empresas que utilizam esses modelos em suas operações diárias. A primeira campanha consistiu na injeção de domínios maliciosos, enquanto a segunda, considerada mais perigosa, focou em testar APIs de serviços de IA de grandes empresas como OpenAI e Google, buscando identificar quais modelos poderiam ser manipulados sem acionar alertas de segurança. Os especialistas alertam que esses ataques representam riscos significativos para a segurança corporativa, especialmente com a adoção crescente de IAs. Recomenda-se que as empresas implementem medidas de segurança mais robustas, como o bloqueio de endereços suspeitos e a configuração de alertas para respostas rápidas a possíveis ameaças.&lt;/p></description></item><item><title>Microsoft revela ataque de canal lateral que compromete LLMs</title><link>https://brdefense.center/news/microsoft-revela-ataque-de-canal-lateral-que-compr/</link><pubDate>Sat, 08 Nov 2025 12:58:05 -0300</pubDate><guid>https://brdefense.center/news/microsoft-revela-ataque-de-canal-lateral-que-compr/</guid><description>&lt;p>A Microsoft divulgou detalhes sobre um novo ataque de canal lateral, denominado Whisper Leak, que pode permitir que adversários passem a observar o tráfego de rede para inferir tópicos de conversação em modelos de linguagem remotos, mesmo com a proteção de criptografia. Este ataque é particularmente preocupante, pois pode expor dados trocados entre usuários e modelos de linguagem em modo de streaming, colocando em risco a privacidade das comunicações de usuários e empresas. Pesquisadores da Microsoft explicaram que atacantes em posição de monitorar o tráfego criptografado, como agências governamentais ou provedores de internet, podem identificar se um usuário está discutindo tópicos sensíveis, como lavagem de dinheiro ou dissidência política, apenas analisando o tamanho dos pacotes e os tempos de chegada. A técnica foi testada com modelos de aprendizado de máquina, alcançando taxas de precisão superiores a 98% em identificar tópicos específicos. Embora a Microsoft e outras empresas tenham implementado medidas de mitigação, a eficácia do ataque pode aumentar com a coleta de mais amostras ao longo do tempo. A empresa recomenda que os usuários evitem discutir assuntos sensíveis em redes não confiáveis e considerem o uso de VPNs para proteção adicional.&lt;/p></description></item><item><title>Zero Trust uma solução comprovada para os novos desafios de segurança da IA</title><link>https://brdefense.center/news/zero-trust-uma-solucao-comprovada-para-os-novos-de/</link><pubDate>Tue, 07 Oct 2025 07:00:50 -0300</pubDate><guid>https://brdefense.center/news/zero-trust-uma-solucao-comprovada-para-os-novos-de/</guid><description>&lt;p>À medida que as organizações buscam aproveitar o potencial produtivo dos modelos de linguagem de grande escala (LLMs) e da IA autônoma, surge uma preocupação com a segurança: como garantir que essas ferramentas poderosas não causem vazamentos de dados ou ações maliciosas? O artigo destaca que a arquitetura de Zero Trust, que se baseia na premissa de &amp;rsquo;nunca confiar, sempre verificar&amp;rsquo;, é essencial para proteger interações complexas entre usuários, agentes de IA e dados sensíveis. O uso de LLMs pode multiplicar os riscos de exposição, pois cada interação pode resultar em vazamentos em larga escala. Portanto, é crucial implementar controles dinâmicos e baseados em identidade, garantindo que cada agente de IA tenha suas permissões rigorosamente gerenciadas. O Zero Trust deve ser aplicado em fluxos de trabalho de IA, vinculando agentes a identidades verificadas e utilizando controles contextuais para limitar o acesso. A adoção desse modelo não apenas protege os dados, mas também permite que as organizações inovem com segurança, atendendo às crescentes exigências regulatórias em torno do uso da IA.&lt;/p></description></item><item><title>Mais de 1.100 servidores de IA da Ollama expostos online, 20 vulneráveis</title><link>https://brdefense.center/news/mais-de-1100-servidores-de-ia-da-ollama-expostos-o/</link><pubDate>Wed, 03 Sep 2025 18:58:34 -0300</pubDate><guid>https://brdefense.center/news/mais-de-1100-servidores-de-ia-da-ollama-expostos-o/</guid><description>&lt;p>Pesquisadores da Cisco Talos identificaram mais de 1.100 instâncias do framework Ollama, utilizado para hospedar modelos de linguagem, acessíveis publicamente na internet. Aproximadamente 20% desses servidores estavam operando sem qualquer forma de autenticação, tornando-os vulneráveis a ataques severos. Em uma varredura rápida usando o Shodan, foram encontrados 1.139 endpoints expostos, dos quais 214 permitiam consultas de modelos sem credenciais. Essa falta de controle de acesso possibilita ataques de extração de modelo e a injeção de conteúdo malicioso. Mesmo os 80% restantes, que estavam inativos no momento da descoberta, apresentam riscos significativos, como uploads não autorizados de modelos e ataques de exaustão de recursos. A análise geoespacial revelou que a maioria dos servidores expostos está localizada nos Estados Unidos, China e Alemanha, evidenciando falhas na segurança da infraestrutura de IA. Para mitigar essas vulnerabilidades, recomenda-se a implementação de mecanismos de autenticação robustos, isolamento de rede e auditorias regulares de exposição. Essas medidas são essenciais para proteger a integridade dos modelos de IA e evitar abusos.&lt;/p></description></item><item><title>Nova ferramenta BruteForceAI ataca páginas de login com inteligência</title><link>https://brdefense.center/news/nova-ferramenta-bruteforceai-ataca-paginas-de-logi/</link><pubDate>Wed, 03 Sep 2025 06:58:45 -0300</pubDate><guid>https://brdefense.center/news/nova-ferramenta-bruteforceai-ataca-paginas-de-logi/</guid><description>&lt;p>O BruteForceAI é uma nova ferramenta de teste de penetração que utiliza Modelos de Linguagem de Grande Escala (LLMs) para automatizar e otimizar ataques de força bruta em páginas de login. Desenvolvido pelo pesquisador de cibersegurança Mor David, o BruteForceAI apresenta um fluxo de trabalho de ataque em duas etapas: a primeira envolve uma análise inteligente do formulário de login, onde um LLM identifica elementos como nome de usuário e senha, reduzindo a necessidade de configuração manual. A segunda etapa executa ataques de alta velocidade, utilizando modos como força bruta clássica e Password Spray, que aplica uma única senha a múltiplos usuários.&lt;/p></description></item></channel></rss>