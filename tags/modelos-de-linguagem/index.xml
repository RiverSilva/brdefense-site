<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Modelos De Linguagem on BR Defense Center</title><link>https://brdefense.center/tags/modelos-de-linguagem/</link><description>Recent content in Modelos De Linguagem on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Tue, 07 Oct 2025 07:00:50 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/modelos-de-linguagem/index.xml" rel="self" type="application/rss+xml"/><item><title>Zero Trust uma solução comprovada para os novos desafios de segurança da IA</title><link>https://brdefense.center/news/zero-trust-uma-solucao-comprovada-para-os-novos-de/</link><pubDate>Tue, 07 Oct 2025 07:00:50 -0300</pubDate><guid>https://brdefense.center/news/zero-trust-uma-solucao-comprovada-para-os-novos-de/</guid><description>&lt;p>À medida que as organizações buscam aproveitar o potencial produtivo dos modelos de linguagem de grande escala (LLMs) e da IA autônoma, surge uma preocupação com a segurança: como garantir que essas ferramentas poderosas não causem vazamentos de dados ou ações maliciosas? O artigo destaca que a arquitetura de Zero Trust, que se baseia na premissa de &amp;rsquo;nunca confiar, sempre verificar&amp;rsquo;, é essencial para proteger interações complexas entre usuários, agentes de IA e dados sensíveis. O uso de LLMs pode multiplicar os riscos de exposição, pois cada interação pode resultar em vazamentos em larga escala. Portanto, é crucial implementar controles dinâmicos e baseados em identidade, garantindo que cada agente de IA tenha suas permissões rigorosamente gerenciadas. O Zero Trust deve ser aplicado em fluxos de trabalho de IA, vinculando agentes a identidades verificadas e utilizando controles contextuais para limitar o acesso. A adoção desse modelo não apenas protege os dados, mas também permite que as organizações inovem com segurança, atendendo às crescentes exigências regulatórias em torno do uso da IA.&lt;/p></description></item><item><title>Mais de 1.100 servidores de IA da Ollama expostos online, 20 vulneráveis</title><link>https://brdefense.center/news/mais-de-1100-servidores-de-ia-da-ollama-expostos-o/</link><pubDate>Wed, 03 Sep 2025 18:58:34 -0300</pubDate><guid>https://brdefense.center/news/mais-de-1100-servidores-de-ia-da-ollama-expostos-o/</guid><description>&lt;p>Pesquisadores da Cisco Talos identificaram mais de 1.100 instâncias do framework Ollama, utilizado para hospedar modelos de linguagem, acessíveis publicamente na internet. Aproximadamente 20% desses servidores estavam operando sem qualquer forma de autenticação, tornando-os vulneráveis a ataques severos. Em uma varredura rápida usando o Shodan, foram encontrados 1.139 endpoints expostos, dos quais 214 permitiam consultas de modelos sem credenciais. Essa falta de controle de acesso possibilita ataques de extração de modelo e a injeção de conteúdo malicioso. Mesmo os 80% restantes, que estavam inativos no momento da descoberta, apresentam riscos significativos, como uploads não autorizados de modelos e ataques de exaustão de recursos. A análise geoespacial revelou que a maioria dos servidores expostos está localizada nos Estados Unidos, China e Alemanha, evidenciando falhas na segurança da infraestrutura de IA. Para mitigar essas vulnerabilidades, recomenda-se a implementação de mecanismos de autenticação robustos, isolamento de rede e auditorias regulares de exposição. Essas medidas são essenciais para proteger a integridade dos modelos de IA e evitar abusos.&lt;/p></description></item><item><title>Nova ferramenta BruteForceAI ataca páginas de login com inteligência</title><link>https://brdefense.center/news/nova-ferramenta-bruteforceai-ataca-paginas-de-logi/</link><pubDate>Wed, 03 Sep 2025 06:58:45 -0300</pubDate><guid>https://brdefense.center/news/nova-ferramenta-bruteforceai-ataca-paginas-de-logi/</guid><description>&lt;p>O BruteForceAI é uma nova ferramenta de teste de penetração que utiliza Modelos de Linguagem de Grande Escala (LLMs) para automatizar e otimizar ataques de força bruta em páginas de login. Desenvolvido pelo pesquisador de cibersegurança Mor David, o BruteForceAI apresenta um fluxo de trabalho de ataque em duas etapas: a primeira envolve uma análise inteligente do formulário de login, onde um LLM identifica elementos como nome de usuário e senha, reduzindo a necessidade de configuração manual. A segunda etapa executa ataques de alta velocidade, utilizando modos como força bruta clássica e Password Spray, que aplica uma única senha a múltiplos usuários.&lt;/p></description></item></channel></rss>