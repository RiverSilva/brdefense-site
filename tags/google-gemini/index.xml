<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Google Gemini on BR Defense Center</title><link>https://brdefense.center/tags/google-gemini/</link><description>Recent content in Google Gemini on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Thu, 12 Feb 2026 07:31:49 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/google-gemini/index.xml" rel="self" type="application/rss+xml"/><item><title>Hackers apoiados por Estados usam IA do Google para ataques cibernéticos</title><link>https://brdefense.center/news/hackers-apoiados-por-estados-usam-ia-do-google-par/</link><pubDate>Thu, 12 Feb 2026 07:31:49 -0300</pubDate><guid>https://brdefense.center/news/hackers-apoiados-por-estados-usam-ia-do-google-par/</guid><description>&lt;p>Hackers apoiados por estados, incluindo grupos da China, Irã, Coreia do Norte e Rússia, estão utilizando o modelo de IA Gemini do Google para facilitar todas as etapas de ataques cibernéticos, desde a fase de reconhecimento até ações pós-comprometimento. O relatório do Google Threat Intelligence Group (GTIG) revela que esses atores maliciosos empregam a IA para criar perfis de alvos, gerar iscas de phishing, traduzir textos, realizar testes de vulnerabilidades e até desenvolver malware. Por exemplo, um ator da China simulou um cenário para automatizar a análise de vulnerabilidades, enquanto um grupo iraniano usou a IA para acelerar a criação de ferramentas maliciosas personalizadas. Além disso, a IA está sendo utilizada em campanhas de engenharia social, como as campanhas ClickFix, que distribuem malware para roubo de informações. O relatório também destaca tentativas de extração e destilação do modelo de IA, o que representa um risco significativo para a propriedade intelectual e o modelo de negócios de IA como serviço. O Google está tomando medidas para mitigar esses abusos, mas a ameaça continua crescente, especialmente com o aumento do interesse de cibercriminosos por ferramentas de IA.&lt;/p></description></item><item><title>Falha de segurança no Google Gemini permite roubo de dados via convites de calendário</title><link>https://brdefense.center/news/falha-de-seguranca-no-google-gemini-permite-roubo/</link><pubDate>Wed, 21 Jan 2026 01:37:15 -0300</pubDate><guid>https://brdefense.center/news/falha-de-seguranca-no-google-gemini-permite-roubo/</guid><description>&lt;p>Pesquisadores de segurança descobriram uma vulnerabilidade no Google Gemini que permite a execução de ataques de injeção de prompt através de convites do Google Calendar. Esse tipo de ataque ocorre quando um ator malicioso insere um comando oculto em uma mensagem aparentemente inofensiva. Ao receber um convite de calendário que contém esse comando, a vítima pode inadvertidamente permitir que o AI do Gemini execute ações que resultam na extração de dados sensíveis, como informações de reuniões privadas. O ataque é particularmente preocupante porque não requer interação direta do usuário, permitindo que os invasores acessem dados sem que a vítima perceba. A vulnerabilidade foi mitigada, reduzindo o risco imediato de exploração, mas destaca a necessidade de vigilância contínua em relação a novas técnicas de ataque que podem comprometer a segurança de dados em plataformas amplamente utilizadas. A pesquisa enfatiza a importância de educar os usuários sobre os riscos associados a interações com sistemas de IA e a necessidade de medidas de segurança robustas para proteger informações sensíveis.&lt;/p></description></item><item><title>Google não planeja anúncios no Gemini, enquanto ChatGPT testa publicidade</title><link>https://brdefense.center/news/google-nao-planeja-anuncios-no-gemini-enquanto-cha/</link><pubDate>Wed, 21 Jan 2026 01:35:59 -0300</pubDate><guid>https://brdefense.center/news/google-nao-planeja-anuncios-no-gemini-enquanto-cha/</guid><description>&lt;p>Recentemente, a OpenAI começou a testar anúncios no ChatGPT nos Estados Unidos, tanto para usuários da conta gratuita quanto para assinantes do plano Go, que custa US$ 8 por mês. Em contraste, o CEO da Google DeepMind, Demis Hassabis, afirmou que o Gemini, a nova plataforma de inteligência artificial da Google, não terá anúncios por enquanto. Durante o Fórum Econômico Mundial em Davos, Hassabis comentou que achou interessante a decisão da OpenAI de introduzir anúncios tão cedo, sugerindo que isso pode ser uma estratégia para aumentar a receita. O ChatGPT planeja implementar anúncios em breve, mas apenas para usuários gratuitos e do plano Go, enquanto as contas pagas, como Plus e Pro, permanecerão livres de publicidade. Os anúncios aparecerão apenas quando houver produtos ou serviços patrocinados relevantes ao tema da conversa, e os usuários poderão entender o motivo da exibição do anúncio e fornecer feedback. No entanto, não haverá anúncios em discussões sobre saúde, saúde mental ou política. Essa movimentação levanta questões sobre a monetização de plataformas de IA e suas implicações para os usuários e o mercado.&lt;/p></description></item><item><title>Falha no Google Gemini permite roubo de dados via Calendário</title><link>https://brdefense.center/news/falha-no-google-gemini-permite-roubo-de-dados-via/</link><pubDate>Tue, 20 Jan 2026 19:00:23 -0300</pubDate><guid>https://brdefense.center/news/falha-no-google-gemini-permite-roubo-de-dados-via/</guid><description>&lt;p>Pesquisadores da Miggo Security identificaram uma vulnerabilidade no Google Gemini que possibilita a injeção indireta de comandos maliciosos, utilizando o Calendário Google como vetor de ataque. Os criminosos criam um evento de calendário e, na descrição, inserem um prompt em linguagem natural que manipula a inteligência artificial do Gemini. Quando a vítima interage com o chatbot, fazendo perguntas sobre sua agenda, o sistema pode acabar extraindo dados privados e os repassando aos atacantes sem que o usuário perceba. Embora a Google já tenha corrigido a falha, o incidente ressalta os riscos associados ao uso de agentes de IA na automação de tarefas, especialmente quando lidam com informações sensíveis. Pesquisadores alertam que chatbots ainda não são totalmente seguros para gerenciar dados pessoais sem diretrizes rigorosas. Este caso destaca a necessidade de vigilância constante e de práticas de segurança robustas ao utilizar tecnologias de IA em ambientes corporativos.&lt;/p></description></item><item><title>Ataque ao Google Gemini vaza dados privados do Calendário</title><link>https://brdefense.center/news/ataque-ao-google-gemini-vaza-dados-privados-do-cal/</link><pubDate>Tue, 20 Jan 2026 18:58:16 -0300</pubDate><guid>https://brdefense.center/news/ataque-ao-google-gemini-vaza-dados-privados-do-cal/</guid><description>&lt;p>Pesquisadores da Miggo Security descobriram uma vulnerabilidade no assistente de IA Google Gemini, que permite a exfiltração de dados privados do Calendário através de um ataque de injeção de prompt. O ataque começa com o envio de um convite para um evento, contendo uma descrição maliciosa que atua como um payload. Quando a vítima pergunta ao assistente sobre sua agenda, o Gemini processa o evento malicioso e pode vazar informações sensíveis, como resumos de reuniões privadas. Essa técnica se aproveita da capacidade do Gemini de interpretar dados de eventos para ser útil, mas que pode ser manipulada por atacantes. Apesar de o Google ter implementado medidas de segurança após um ataque semelhante em 2025, a nova abordagem dos pesquisadores destaca a dificuldade em prever novas formas de exploração em sistemas de IA. A Miggo compartilhou suas descobertas com o Google, que já está trabalhando em novas mitigação para bloquear esses ataques, mas a complexidade da linguagem natural continua a representar um desafio significativo para a segurança.&lt;/p></description></item><item><title>Vulnerabilidade no Google Gemini permite extração de dados do Calendar</title><link>https://brdefense.center/news/vulnerabilidade-no-google-gemini-permite-extracao/</link><pubDate>Mon, 19 Jan 2026 18:57:16 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-no-google-gemini-permite-extracao/</guid><description>&lt;p>Pesquisadores de cibersegurança revelaram uma falha de segurança que permite a injeção de comandos no Google Gemini, possibilitando a extração de dados do Google Calendar. A vulnerabilidade, identificada por Liad Eliyahu, da Miggo Security, permite que um invasor crie um evento de calendário malicioso que, ao ser consultado pelo usuário, ativa um comando oculto que extrai informações privadas de reuniões. O ataque é ativado quando o usuário faz uma pergunta aparentemente inocente sobre sua agenda, levando o chatbot a gerar um novo evento que contém um resumo das reuniões privadas do usuário. Embora a falha tenha sido corrigida, o incidente destaca como as funcionalidades baseadas em inteligência artificial podem ampliar a superfície de ataque e introduzir novos riscos de segurança. Além disso, a pesquisa aponta que vulnerabilidades não se limitam mais ao código, mas também se manifestam na linguagem e no comportamento da IA em tempo real. O artigo também menciona outras vulnerabilidades em sistemas de IA, reforçando a necessidade de auditorias regulares e controles de segurança adequados em ambientes corporativos que utilizam essas tecnologias.&lt;/p></description></item><item><title>Hackers podem roubar dados com comandos escondidos em imagens processadas por IA</title><link>https://brdefense.center/news/hackers-podem-roubar-dados-com-comandos-escondidos/</link><pubDate>Thu, 28 Aug 2025 13:01:49 -0300</pubDate><guid>https://brdefense.center/news/hackers-podem-roubar-dados-com-comandos-escondidos/</guid><description>&lt;p>Pesquisadores do grupo Trail of Bits revelaram uma nova vulnerabilidade que permite a hackers roubar dados de usuários ao injetar comandos maliciosos em imagens processadas por sistemas de inteligência artificial (IA), como o Gemini da Google. A técnica utiliza esteganografia, onde instruções invisíveis ao olho humano são incorporadas em imagens de alta resolução. Quando essas imagens são redimensionadas por algoritmos de IA, os comandos ocultos podem se tornar visíveis e ser interpretados como parte das solicitações do usuário.&lt;/p></description></item></channel></rss>