<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Injeção De Prompt on BR Defense Center</title><link>https://brdefense.center/tags/inje%C3%A7%C3%A3o-de-prompt/</link><description>Recent content in Injeção De Prompt on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Wed, 21 Jan 2026 01:37:15 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/inje%C3%A7%C3%A3o-de-prompt/index.xml" rel="self" type="application/rss+xml"/><item><title>Falha de segurança no Google Gemini permite roubo de dados via convites de calendário</title><link>https://brdefense.center/news/falha-de-seguranca-no-google-gemini-permite-roubo/</link><pubDate>Wed, 21 Jan 2026 01:37:15 -0300</pubDate><guid>https://brdefense.center/news/falha-de-seguranca-no-google-gemini-permite-roubo/</guid><description>&lt;p>Pesquisadores de segurança descobriram uma vulnerabilidade no Google Gemini que permite a execução de ataques de injeção de prompt através de convites do Google Calendar. Esse tipo de ataque ocorre quando um ator malicioso insere um comando oculto em uma mensagem aparentemente inofensiva. Ao receber um convite de calendário que contém esse comando, a vítima pode inadvertidamente permitir que o AI do Gemini execute ações que resultam na extração de dados sensíveis, como informações de reuniões privadas. O ataque é particularmente preocupante porque não requer interação direta do usuário, permitindo que os invasores acessem dados sem que a vítima perceba. A vulnerabilidade foi mitigada, reduzindo o risco imediato de exploração, mas destaca a necessidade de vigilância contínua em relação a novas técnicas de ataque que podem comprometer a segurança de dados em plataformas amplamente utilizadas. A pesquisa enfatiza a importância de educar os usuários sobre os riscos associados a interações com sistemas de IA e a necessidade de medidas de segurança robustas para proteger informações sensíveis.&lt;/p></description></item><item><title>Ataque de IA do Microsoft Copilot compromete usuários com um clique</title><link>https://brdefense.center/news/ataque-de-ia-do-microsoft-copilot-compromete-usuar/</link><pubDate>Thu, 15 Jan 2026 13:21:14 -0300</pubDate><guid>https://brdefense.center/news/ataque-de-ia-do-microsoft-copilot-compromete-usuar/</guid><description>&lt;p>Pesquisadores de segurança da Varonis descobriram um novo método de ataque de injeção de prompt, chamado &amp;lsquo;Reprompt&amp;rsquo;, que compromete usuários do Microsoft Copilot com apenas um clique. Diferente de ataques anteriores que utilizavam e-mails maliciosos, essa nova técnica explora parâmetros de URL para injetar comandos prejudiciais. Quando um usuário clica em um link aparentemente legítimo que contém um parâmetro &amp;lsquo;q&amp;rsquo;, o Copilot interpreta esse conteúdo como um comando a ser executado, permitindo que dados sensíveis sejam vazados. A Microsoft já corrigiu essa vulnerabilidade, bloqueando a possibilidade de injeção de prompt via URLs. Essa descoberta destaca a necessidade de vigilância contínua em ferramentas de IA generativa, que ainda não conseguem distinguir adequadamente entre comandos e dados a serem lidos, tornando-as suscetíveis a ataques. A situação ressalta a importância de medidas de segurança robustas para proteger informações sensíveis em ambientes corporativos.&lt;/p></description></item><item><title>5 formas de se proteger de injeção de prompt em navegadores de IA</title><link>https://brdefense.center/news/5-formas-de-se-proteger-de-injecao-de-prompt-em-na/</link><pubDate>Sat, 06 Dec 2025 18:57:47 -0300</pubDate><guid>https://brdefense.center/news/5-formas-de-se-proteger-de-injecao-de-prompt-em-na/</guid><description>&lt;p>O avanço da inteligência artificial (IA) trouxe benefícios significativos, mas também expôs usuários a novos riscos, como a injeção de prompt em navegadores. Esse tipo de ataque ocorre quando hackers inserem códigos maliciosos em prompts, manipulando a IA para realizar atividades fraudulentas, como roubo de dados e credenciais. O artigo apresenta cinco dicas práticas para mitigar esses riscos. Primeiro, é essencial desconfiar das informações fornecidas pela IA, sempre verificando a veracidade com fontes confiáveis. Em segundo lugar, os usuários devem evitar compartilhar dados pessoais sensíveis, como informações bancárias, que podem ser acessadas por cibercriminosos. A atualização constante dos dispositivos é outra medida crucial, pois correções de segurança ajudam a fechar brechas exploráveis. Além disso, é importante monitorar as atividades da IA e verificar a precisão das informações geradas. Por fim, a autenticação multifator (MFA) é recomendada para adicionar uma camada extra de segurança, dificultando o acesso não autorizado mesmo em caso de vazamento de credenciais. Essas práticas são fundamentais para proteger os usuários em um cenário digital cada vez mais complexo.&lt;/p></description></item><item><title>Vulnerabilidade em IA da ServiceNow permite ataques de injeção</title><link>https://brdefense.center/news/vulnerabilidade-em-ia-da-servicenow-permite-ataque/</link><pubDate>Wed, 19 Nov 2025 12:59:12 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-em-ia-da-servicenow-permite-ataque/</guid><description>&lt;p>Um novo alerta de segurança destaca como a plataforma de inteligência artificial Now Assist da ServiceNow pode ser explorada por agentes maliciosos. Segundo a AppOmni, configurações padrão da plataforma permitem ataques de injeção de prompt de segunda ordem, onde agentes podem descobrir e recrutar uns aos outros para realizar ações não autorizadas. Isso inclui a cópia e exfiltração de dados sensíveis, modificação de registros e escalonamento de privilégios. O problema não é um bug, mas sim um comportamento esperado devido às configurações padrão que facilitam a comunicação entre agentes. A vulnerabilidade é particularmente preocupante, pois as ações ocorrem em segundo plano, sem que a organização afetada perceba. Para mitigar esses riscos, recomenda-se configurar o modo de execução supervisionada para agentes privilegiados, desabilitar a propriedade de sobreposição autônoma e monitorar o comportamento dos agentes de IA. A ServiceNow reconheceu a questão e atualizou sua documentação, mas a situação ressalta a necessidade de uma proteção mais robusta para agentes de IA em ambientes corporativos.&lt;/p></description></item><item><title>Nova falha no ChatGPT permite roubo de dados e histórico de conversas</title><link>https://brdefense.center/news/nova-falha-no-chatgpt-permite-roubo-de-dados-e-his/</link><pubDate>Tue, 11 Nov 2025 18:59:16 -0300</pubDate><guid>https://brdefense.center/news/nova-falha-no-chatgpt-permite-roubo-de-dados-e-his/</guid><description>&lt;p>Um relatório da Tenable Research revelou sete falhas de segurança na plataforma ChatGPT da OpenAI, que podem ser exploradas por cibercriminosos para roubar dados dos usuários e até controlar o chatbot. A principal vulnerabilidade identificada é a &amp;lsquo;injeção de prompt&amp;rsquo;, onde hackers enviam instruções maliciosas ao ChatGPT sem que o usuário perceba. Os especialistas demonstraram duas formas de ataque: a primeira envolve a inserção de um comentário malicioso em um blog, que pode ser ativado quando o usuário pede um resumo ao ChatGPT. A segunda é um ataque de clique zero, onde o hacker cria um site que, ao ser indexado pelo ChatGPT, pode comprometer o usuário sem qualquer interação. Além disso, a falha de &amp;lsquo;injeção de memória&amp;rsquo; permite que comandos maliciosos sejam salvos no histórico do usuário, possibilitando o roubo de dados sensíveis em interações futuras. A OpenAI foi notificada sobre essas vulnerabilidades, que afetam os modelos ChatGPT 4o e GPT-5, mas ainda não há informações sobre correções.&lt;/p></description></item><item><title>Navegador OpenAI Atlas vulnerável a ataques de injeção de prompt</title><link>https://brdefense.center/news/navegador-openai-atlas-vulneravel-a-ataques-de-inj/</link><pubDate>Mon, 27 Oct 2025 06:58:53 -0300</pubDate><guid>https://brdefense.center/news/navegador-openai-atlas-vulneravel-a-ataques-de-inj/</guid><description>&lt;p>O navegador OpenAI Atlas, recém-lançado, foi identificado como vulnerável a um ataque de injeção de prompt, onde um prompt malicioso pode ser disfarçado como um URL aparentemente inofensivo. Segundo um relatório da NeuralTrust, o omnibox do navegador, que combina a barra de endereço e de busca, interpreta entradas como URLs ou comandos em linguagem natural. Isso permite que um atacante crie um link que, ao ser inserido, faz com que o navegador execute instruções prejudiciais. Por exemplo, um URL malformado pode redirecionar o usuário para um site controlado pelo atacante, potencialmente levando a páginas de phishing ou até comandos que excluem arquivos de aplicativos conectados, como o Google Drive. A falta de distinção rigorosa entre entradas de usuário confiáveis e conteúdo não confiável no Atlas é uma falha crítica. A situação é agravada por técnicas como o &amp;lsquo;AI Sidebar Spoofing&amp;rsquo;, onde extensões maliciosas podem enganar usuários a fornecer dados ou instalar malware. Embora a OpenAI tenha implementado medidas de segurança, a injeção de prompt continua a ser um problema de segurança não resolvido, exigindo atenção contínua da indústria de cibersegurança.&lt;/p></description></item><item><title>Técnica de Injeção de Prompt Permite Bypass do Framework Guardrails da OpenAI</title><link>https://brdefense.center/news/tecnica-de-injecao-de-prompt-permite-bypass-do-fra/</link><pubDate>Tue, 14 Oct 2025 06:59:18 -0300</pubDate><guid>https://brdefense.center/news/tecnica-de-injecao-de-prompt-permite-bypass-do-fra/</guid><description>&lt;p>Pesquisadores de segurança revelaram uma vulnerabilidade crítica no framework Guardrails da OpenAI, que pode ser explorada através de métodos simples de injeção de prompt. Essa técnica permite que atacantes manipulem os modelos de linguagem que deveriam garantir a segurança do comportamento da IA, possibilitando a inserção de conteúdo malicioso sem ser detectado. O Guardrails, introduzido em 6 de outubro, utiliza modelos de linguagem como &amp;lsquo;juízes&amp;rsquo; para avaliar a segurança de entradas e saídas, mas a pesquisa mostrou que essa abordagem cria um ciclo de segurança &amp;lsquo;cega&amp;rsquo;. Os atacantes podem enganar esses juízes, manipulando os limiares de confiança e permitindo a execução de instruções perigosas. Os métodos de bypass demonstrados incluem a inserção de instruções maliciosas em templates que imitam avaliações aprovadas e a ocultação de código malicioso em comentários HTML. Essa vulnerabilidade, classificada como &amp;lsquo;composta&amp;rsquo;, sugere que os juízes baseados em LLM são tão suscetíveis à manipulação quanto os modelos que protegem. Para mitigar esses riscos, as organizações devem implementar defesas em camadas e sistemas de validação independentes, além de monitoramento contínuo.&lt;/p></description></item><item><title>Novo ataque CometJacking compromete navegadores de IA</title><link>https://brdefense.center/news/novo-ataque-cometjacking-compromete-navegadores-de/</link><pubDate>Sat, 04 Oct 2025 12:57:38 -0300</pubDate><guid>https://brdefense.center/news/novo-ataque-cometjacking-compromete-navegadores-de/</guid><description>&lt;p>Pesquisadores de cibersegurança revelaram um novo ataque denominado CometJacking, que visa o navegador de IA Comet da Perplexity. Este ataque utiliza links maliciosos que, ao serem clicados, injetam comandos ocultos no navegador, permitindo que dados sensíveis, como informações de e-mail e calendário, sejam extraídos sem o conhecimento do usuário. A técnica de injeção de prompt se aproveita de uma URL manipulada que, em vez de direcionar o usuário para um site legítimo, instrui o assistente de IA a acessar sua memória e coletar dados, que são então codificados em Base64 e enviados para um servidor controlado pelo atacante. Embora a Perplexity tenha minimizado o impacto da descoberta, o ataque destaca a vulnerabilidade de ferramentas nativas de IA, que podem contornar as proteções tradicionais de segurança. Especialistas alertam que os navegadores de IA representam um novo campo de batalha para a segurança cibernética, exigindo que as organizações implementem controles rigorosos para detectar e neutralizar esses tipos de ataques antes que se tornem comuns.&lt;/p></description></item></channel></rss>