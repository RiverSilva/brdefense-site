<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Inteligência Artificial on BR Defense Center</title><link>https://brdefense.center/tags/intelig%C3%AAncia-artificial/</link><description>Recent content in Inteligência Artificial on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Thu, 12 Feb 2026 19:16:30 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/intelig%C3%AAncia-artificial/index.xml" rel="self" type="application/rss+xml"/><item><title>Grupo ligado à Coreia do Norte usa IA para espionagem cibernética</title><link>https://brdefense.center/news/grupo-ligado-a-coreia-do-norte-usa-ia-para-espiona/</link><pubDate>Thu, 12 Feb 2026 19:16:30 -0300</pubDate><guid>https://brdefense.center/news/grupo-ligado-a-coreia-do-norte-usa-ia-para-espiona/</guid><description>&lt;p>O Google revelou que o grupo de hackers UNC2970, vinculado à Coreia do Norte, está utilizando seu modelo de inteligência artificial generativa, Gemini, para realizar atividades de reconhecimento em alvos estratégicos. De acordo com o relatório do Google Threat Intelligence Group (GTIG), o grupo tem se concentrado em mapear informações sobre empresas de cibersegurança e defesa, além de perfis de cargos técnicos e dados salariais. Essa prática, que mistura pesquisa profissional com reconhecimento malicioso, permite que o grupo crie personas de phishing personalizadas e identifique alvos vulneráveis para compromissos iniciais.&lt;/p></description></item><item><title>Falha em extensões do Claude expõe 10 mil usuários a execução remota de códigos</title><link>https://brdefense.center/news/falha-em-extensoes-do-claude-expoe-10-mil-usuarios/</link><pubDate>Thu, 12 Feb 2026 13:45:27 -0300</pubDate><guid>https://brdefense.center/news/falha-em-extensoes-do-claude-expoe-10-mil-usuarios/</guid><description>&lt;p>Uma falha crítica de segurança foi identificada nas extensões do Claude, ferramenta de inteligência artificial da Anthropic, afetando mais de 10 mil usuários. Pesquisadores da LayerX descobriram uma vulnerabilidade de clique zero que permite a execução remota de códigos maliciosos. O problema está relacionado à forma como as extensões interagem com o Google Agenda, onde um evento corrompido pode ser utilizado para executar comandos sem a necessidade de autorização do usuário. Essa falha ocorre devido ao acesso total que as extensões têm ao sistema do dispositivo, permitindo ações automáticas que podem ser exploradas por atacantes. A execução de um simples comando, como a verificação de eventos, pode desencadear uma cadeia de ações prejudiciais, comprometendo a segurança do sistema. Até o momento, a Anthropic não divulgou uma correção para a vulnerabilidade, que é considerada complexa e relacionada à infraestrutura da ferramenta. A situação exige atenção, pois a falta de um patch pode deixar os usuários vulneráveis a ataques.&lt;/p></description></item><item><title>ZAST.AI recebe investimento e promete revolucionar segurança de aplicações</title><link>https://brdefense.center/news/zastai-recebe-investimento-e-promete-revolucionar/</link><pubDate>Tue, 10 Feb 2026 13:47:52 -0300</pubDate><guid>https://brdefense.center/news/zastai-recebe-investimento-e-promete-revolucionar/</guid><description>&lt;p>A ZAST.AI, uma startup de segurança cibernética, anunciou a conclusão de uma rodada de financiamento pré-A de US$ 6 milhões, totalizando quase US$ 10 milhões em investimentos. A empresa, reconhecida por sua inovação em ferramentas de segurança, visa reduzir drasticamente as altas taxas de falsos positivos que afligem as equipes de segurança. Em 2025, a ZAST.AI identificou centenas de vulnerabilidades zero-day em projetos de código aberto populares, resultando em 119 atribuições de CVE. A abordagem da ZAST.AI combina geração automatizada de Proof-of-Concept (PoC) e validação automatizada, permitindo que as equipes de segurança se concentrem em vulnerabilidades reais, em vez de perder tempo com alertas falsos. A empresa já atende a clientes de grande porte, incluindo empresas da lista Fortune Global 500, e planeja usar os novos fundos para expandir sua tecnologia e presença no mercado global. A ZAST.AI promete transformar a análise de segurança de código, oferecendo uma solução que não apenas identifica vulnerabilidades, mas também as valida de forma eficaz, garantindo que as equipes de segurança possam agir com confiança.&lt;/p></description></item><item><title>Especialistas alertam sobre falhas críticas de segurança no Moltbook</title><link>https://brdefense.center/news/especialistas-alertam-sobre-falhas-criticas-de-seg/</link><pubDate>Mon, 09 Feb 2026 19:26:41 -0300</pubDate><guid>https://brdefense.center/news/especialistas-alertam-sobre-falhas-criticas-de-seg/</guid><description>&lt;p>O Moltbook, uma rede social exclusiva para agentes de inteligência artificial (IA), enfrenta sérias vulnerabilidades que podem comprometer a segurança dos dados de seus usuários. Especialistas em segurança cibernética identificaram que a exposição de uma API pública permitiu o acesso não autorizado a informações sensíveis armazenadas pela plataforma, resultado de um desenvolvimento inseguro. Embora o Moltbook não permita a participação de humanos, a possibilidade de qualquer pessoa criar um bot e conectá-lo à rede aumenta os riscos de uso malicioso. Com mais de 1 milhão de bots registrados, a estrutura frágil da plataforma expõe dados pessoais e facilita o sequestro de bots. Apesar de correções terem sido implementadas, outras vulnerabilidades, como a injeção de prompts maliciosos, permanecem, podendo afetar todos os bots em uma infecção em cadeia. A situação levanta preocupações sobre a segurança e a privacidade dos dados, especialmente em um cenário onde a IA está se tornando cada vez mais integrada ao cotidiano.&lt;/p></description></item><item><title>OpenClaw e VirusTotal Parceria para Aumentar a Segurança de Skills</title><link>https://brdefense.center/news/openclaw-e-virustotal-parceria-para-aumentar-a-seg/</link><pubDate>Sun, 08 Feb 2026 07:01:22 -0300</pubDate><guid>https://brdefense.center/news/openclaw-e-virustotal-parceria-para-aumentar-a-seg/</guid><description>&lt;p>A OpenClaw, plataforma de inteligência artificial, anunciou uma parceria com o VirusTotal, pertencente ao Google, para aumentar a segurança de seu marketplace, o ClawHub. Todas as habilidades (skills) publicadas na plataforma agora passam por uma verificação utilizando a inteligência de ameaças do VirusTotal, incluindo a nova funcionalidade Code Insight. Cada skill é convertida em um hash SHA-256 e verificada em um banco de dados. Skills com avaliação &amp;lsquo;benigna&amp;rsquo; são aprovadas automaticamente, enquanto as suspeitas recebem um alerta e as maliciosas são bloqueadas. A OpenClaw também reavalia diariamente todas as skills ativas para identificar possíveis mudanças de status. Apesar dessas medidas, os mantenedores alertam que a verificação não é infalível, pois algumas habilidades maliciosas podem escapar da detecção. A plataforma também planeja divulgar um modelo de ameaças abrangente e um roteiro de segurança público, após a descoberta de centenas de skills maliciosas que se disfarçam como ferramentas legítimas. A crescente popularidade do OpenClaw levanta preocupações sobre a segurança, especialmente em ambientes corporativos, onde a falta de controle de TI pode facilitar o acesso não autorizado a dados sensíveis.&lt;/p></description></item><item><title>Agentes de IA quase capazes de realizar ciberataques sozinhos</title><link>https://brdefense.center/news/agentes-de-ia-quase-capazes-de-realizar-ciberataqu/</link><pubDate>Fri, 06 Feb 2026 13:30:58 -0300</pubDate><guid>https://brdefense.center/news/agentes-de-ia-quase-capazes-de-realizar-ciberataqu/</guid><description>&lt;p>O segundo Relatório Internacional de Segurança da IA revela que, embora agentes de inteligência artificial ainda não consigam conduzir ciberataques de forma autônoma, eles têm se tornado ferramentas valiosas para cibercriminosos. O estudo, liderado pelo cientista da computação Yoshua Bengio, analisou a evolução da IA em relação ao ano anterior, destacando sua capacidade de automatizar ataques e identificar vulnerabilidades em softwares. Um exemplo alarmante é o uso da ferramenta Claude Code AI por espiões chineses para automatizar ataques em empresas e órgãos governamentais. Os cibercriminosos utilizam a IA para escanear softwares em busca de falhas e para desenvolver códigos maliciosos, como evidenciado pelo uso da HexStrike AI em ataques a dispositivos Citrix NetScaler. Apesar de os agentes de IA ainda precisarem de intervenção humana para realizar ataques complexos, a evolução contínua da tecnologia levanta preocupações sobre a possibilidade de que, em breve, esses sistemas possam operar de forma independente. O relatório sugere que a preparação para essa eventualidade é crucial, especialmente considerando o aumento da automação em ciberataques.&lt;/p></description></item><item><title>Modelo de IA descobre falhas críticas em bibliotecas open-source</title><link>https://brdefense.center/news/modelo-de-ia-descobre-falhas-criticas-em-bibliotec/</link><pubDate>Fri, 06 Feb 2026 07:26:23 -0300</pubDate><guid>https://brdefense.center/news/modelo-de-ia-descobre-falhas-criticas-em-bibliotec/</guid><description>&lt;p>A empresa de inteligência artificial Anthropic anunciou que seu novo modelo de linguagem, Claude Opus 4.6, identificou mais de 500 falhas de segurança de alta severidade em bibliotecas open-source, como Ghostscript, OpenSC e CGIF. Lançado em 6 de fevereiro de 2026, o modelo apresenta habilidades aprimoradas em revisão de código e depuração, além de melhorias em análises financeiras e criação de documentos. Segundo a Anthropic, o Claude Opus 4.6 é capaz de descobrir vulnerabilidades sem a necessidade de ferramentas específicas ou instruções detalhadas, analisando o código de forma semelhante a um pesquisador humano. Durante testes, a equipe de segurança da empresa validou cada falha encontrada, garantindo que não eram falsas. Entre as vulnerabilidades identificadas, destaca-se uma falha de buffer overflow no OpenSC e uma vulnerabilidade no CGIF que requer um entendimento do algoritmo LZW. A Anthropic enfatizou a importância de corrigir rapidamente as falhas conhecidas, especialmente em um cenário onde o uso de IA em fluxos de trabalho cibernéticos está se tornando mais comum. A empresa também se comprometeu a atualizar suas salvaguardas à medida que novas ameaças forem descobertas.&lt;/p></description></item><item><title>Avast lança detector de deepfake para Windows que analisa vídeos em tempo real</title><link>https://brdefense.center/news/avast-lanca-detector-de-deepfake-para-windows-que/</link><pubDate>Thu, 05 Feb 2026 13:31:59 -0300</pubDate><guid>https://brdefense.center/news/avast-lanca-detector-de-deepfake-para-windows-que/</guid><description>&lt;p>A Avast, empresa de cibersegurança, anunciou uma atualização significativa para seu antivírus, incluindo o novo recurso Avast Deepfake Guard, que visa proteger os usuários contra fraudes envolvendo deepfakes. Essa tecnologia utiliza inteligência artificial para analisar vídeos em tempo real, identificando conteúdos manipulados que possam ser utilizados em golpes. A crescente utilização de deepfakes por golpistas representa uma ameaça séria, conforme destacado por Leena Elias, Chief Product Officer da Gen, que observa que esses conteúdos não são intrinsecamente prejudiciais, mas podem ser explorados para manipular e enganar as vítimas. A Avast registrou um aumento alarmante de 159.378 ocorrências de fraudes com deepfakes no último trimestre de 2025, com plataformas como YouTube, Facebook e X (antigo Twitter) sendo os principais alvos. A nova funcionalidade do Avast é uma resposta direta a esse cenário, buscando aumentar a conscientização dos usuários sobre a manipulação de conteúdo e incentivando decisões mais seguras ao consumir informações em vídeo.&lt;/p></description></item><item><title>Microsoft desenvolve scanner para detectar backdoors em LLMs</title><link>https://brdefense.center/news/microsoft-desenvolve-scanner-para-detectar-backdoo/</link><pubDate>Wed, 04 Feb 2026 19:01:22 -0300</pubDate><guid>https://brdefense.center/news/microsoft-desenvolve-scanner-para-detectar-backdoo/</guid><description>&lt;p>A Microsoft anunciou o desenvolvimento de um scanner leve capaz de detectar backdoors em modelos de linguagem de grande escala (LLMs), visando aumentar a confiança em sistemas de inteligência artificial (IA). A equipe de segurança da IA da empresa identificou três sinais observáveis que podem indicar a presença de backdoors, mantendo uma baixa taxa de falsos positivos. Esses sinais incluem padrões de atenção distintos em respostas a frases de gatilho, a memorização de dados de envenenamento e a ativação de backdoors por gatilhos &amp;lsquo;fuzzy&amp;rsquo;. O scanner não requer treinamento adicional e pode ser aplicado em modelos comuns, mas tem limitações, como a incapacidade de funcionar em modelos proprietários. A Microsoft também está expandindo seu Ciclo de Vida de Desenvolvimento Seguro (SDL) para abordar preocupações de segurança específicas da IA, reconhecendo que sistemas de IA criam múltiplos pontos de entrada para inputs inseguros. Essa inovação é um passo significativo para a detecção prática de backdoors, mas a colaboração na comunidade de segurança da IA é essencial para o progresso contínuo.&lt;/p></description></item><item><title>Vulnerabilidade crítica no assistente de IA Ask Gordon do Docker</title><link>https://brdefense.center/news/vulnerabilidade-critica-no-assistente-de-ia-ask-go/</link><pubDate>Tue, 03 Feb 2026 19:16:42 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-critica-no-assistente-de-ia-ask-go/</guid><description>&lt;p>Pesquisadores de cibersegurança revelaram uma vulnerabilidade crítica, chamada DockerDash, que afeta o assistente de inteligência artificial Ask Gordon, integrado ao Docker Desktop e à interface de linha de comando (CLI) do Docker. Essa falha, corrigida na versão 4.50.0 lançada em novembro de 2025, permite que um invasor execute código malicioso e exfiltre dados sensíveis. O problema surge da forma como o Ask Gordon interpreta metadados não verificados como comandos executáveis, permitindo que um ataque simples em três etapas comprometa o ambiente Docker. O ataque começa com a publicação de uma imagem Docker contendo instruções maliciosas em campos de metadados. Quando o assistente é consultado, ele processa essas instruções sem validação, enviando-as ao MCP Gateway, que as executa com privilégios do Docker do usuário. Além disso, a vulnerabilidade também permite a exfiltração de dados sensíveis do ambiente do usuário. A situação destaca a necessidade de uma abordagem de validação de zero confiança para mitigar esse tipo de ataque, que pode ter impactos significativos em ambientes de nuvem e aplicações de desktop.&lt;/p></description></item><item><title>Promotores franceses investigam ferramenta de IA da X por conteúdo ilegal</title><link>https://brdefense.center/news/promotores-franceses-investigam-ferramenta-de-ia-d/</link><pubDate>Tue, 03 Feb 2026 13:40:02 -0300</pubDate><guid>https://brdefense.center/news/promotores-franceses-investigam-ferramenta-de-ia-d/</guid><description>&lt;p>Na terça-feira, promotores franceses realizaram uma operação nas instalações da X em Paris, no âmbito de uma investigação criminal sobre a ferramenta de inteligência artificial Grok, amplamente utilizada para gerar imagens sexualmente explícitas. A investigação, iniciada em janeiro de 2025, foi ampliada após denúncias de que a Grok estaria gerando conteúdo ilegal e que a plataforma X estaria sendo utilizada para compartilhar deepfakes sexuais e conteúdo de negação do Holocausto. A operação foi conduzida pela unidade de cibercrime da Gendarmaria Nacional, com apoio de oficiais da Europol. Além disso, Elon Musk e a CEO da X, Linda Yaccarino, foram convocados para entrevistas voluntárias em abril de 2025, junto a outros funcionários da empresa. A investigação abrange sete delitos, incluindo a posse e distribuição de pornografia infantil e fraudes relacionadas à extração de dados. A Comissão Europeia também iniciou uma investigação para verificar se a X cumpriu as obrigações do Digital Services Act antes de implementar a ferramenta Grok. A X já foi multada em 120 milhões de euros por violações de transparência sob a mesma legislação.&lt;/p></description></item><item><title>Mozilla permite desativar inteligência artificial no Firefox</title><link>https://brdefense.center/news/mozilla-permite-desativar-inteligencia-artificial/</link><pubDate>Tue, 03 Feb 2026 07:26:22 -0300</pubDate><guid>https://brdefense.center/news/mozilla-permite-desativar-inteligencia-artificial/</guid><description>&lt;p>A Mozilla anunciou uma nova seção de controles nas configurações do navegador Firefox, permitindo que os usuários desativem completamente as funcionalidades de inteligência artificial generativa (GenAI). Segundo Ajit Varma, chefe do Firefox, essa nova funcionalidade oferece um local único para bloquear tanto as características atuais quanto as futuras relacionadas à IA. Os usuários poderão gerenciar individualmente as configurações de IA, como traduções, descrições acessíveis em PDFs, agrupamento de abas aprimorado por IA, prévias de links e um chatbot na barra lateral. A implementação dessas funcionalidades está prevista para a versão 148 do Firefox, que será lançada em 24 de fevereiro de 2026. A Mozilla enfatiza a importância da escolha do usuário, permitindo que aqueles que não desejam utilizar recursos de IA possam desativá-los facilmente. O novo CEO da Mozilla, Anthony Enzor-DeMeo, reforçou o compromisso da empresa em ser uma companhia de software confiável, onde privacidade e uso de dados são claros e compreensíveis. Essa abordagem visa garantir que a IA seja sempre uma opção, e não uma imposição.&lt;/p></description></item><item><title>Mozilla permite desativar recursos de IA no Firefox a partir da versão 148</title><link>https://brdefense.center/news/mozilla-permite-desativar-recursos-de-ia-no-firefo/</link><pubDate>Tue, 03 Feb 2026 02:11:32 -0300</pubDate><guid>https://brdefense.center/news/mozilla-permite-desativar-recursos-de-ia-no-firefo/</guid><description>&lt;p>A Mozilla anunciou que a próxima versão do Firefox, a 148, permitirá que os usuários desativem completamente as funcionalidades de inteligência artificial (IA) ou gerenciem-nas individualmente. A nova opção &amp;lsquo;Bloquear melhorias de IA&amp;rsquo; estará disponível a partir de 24 de fevereiro e permitirá que os usuários impeçam a utilização de recursos gerativos de IA no navegador. A decisão foi tomada em resposta ao feedback dos usuários, que expressaram preocupações sobre a integração da IA. O chefe do Firefox, Ajit Varma, destacou que a empresa está comprometida em oferecer escolhas aos usuários, permitindo que aqueles que desejam evitar a IA possam fazê-lo facilmente. Além de bloquear a IA, os usuários poderão gerenciar cinco funcionalidades específicas que utilizam IA, como traduções de navegador e geração de texto alternativo para imagens em PDFs. A configuração das preferências de IA será mantida mesmo após atualizações do navegador. A nova seção de controles de IA será inicialmente disponibilizada para usuários do Firefox Nightly, antes de ser liberada para todos os usuários de desktop. Essa mudança reflete a intenção da Mozilla de garantir que os usuários tenham controle sobre como a IA é utilizada em suas experiências de navegação.&lt;/p></description></item><item><title>Vulnerabilidade crítica no OpenClaw permite execução remota de código</title><link>https://brdefense.center/news/vulnerabilidade-critica-no-openclaw-permite-execuc/</link><pubDate>Tue, 03 Feb 2026 02:10:07 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-critica-no-openclaw-permite-execuc/</guid><description>&lt;p>Uma vulnerabilidade de alta severidade foi identificada no OpenClaw, um assistente pessoal de inteligência artificial de código aberto, que pode permitir a execução remota de código (RCE) através de um link malicioso. A falha, registrada como CVE-2026-25253 e com uma pontuação CVSS de 8.8, foi corrigida na versão 2026.1.29, lançada em 30 de janeiro de 2026. O problema reside na falta de validação do parâmetro &amp;lsquo;gatewayUrl&amp;rsquo; na interface de controle, que permite que um link malicioso envie um token de autenticação para um servidor controlado por um atacante. Isso possibilita que o invasor conecte-se ao gateway local da vítima, altere configurações e execute comandos com privilégios elevados. O ataque pode ser realizado com um único clique, tornando-o extremamente perigoso. A vulnerabilidade afeta qualquer implantação do OpenClaw onde o usuário tenha se autenticado na interface de controle, permitindo acesso ao API do gateway e a execução de código arbitrário. O impacto é significativo, pois mesmo configurações que limitam o acesso a localhost podem ser contornadas, devido à maneira como o navegador da vítima inicia a conexão. A rápida popularidade do OpenClaw, com mais de 149 mil estrelas no GitHub, aumenta a urgência para que as organizações que utilizam essa ferramenta implementem a atualização imediatamente.&lt;/p></description></item><item><title>Engenheiro do Google é condenado por roubo de dados de IA</title><link>https://brdefense.center/news/engenheiro-do-google-e-condenado-por-roubo-de-dado/</link><pubDate>Sat, 31 Jan 2026 18:58:04 -0300</pubDate><guid>https://brdefense.center/news/engenheiro-do-google-e-condenado-por-roubo-de-dado/</guid><description>&lt;p>Um júri federal dos EUA condenou Linwei Ding, ex-engenheiro de software do Google, por roubar dados de supercomputadores de IA da empresa e compartilhá-los secretamente com empresas de tecnologia da China. Ding foi indiciado em março de 2024 após mentir e não cooperar com a investigação interna do Google, resultando em sua prisão na Califórnia. Entre maio de 2022 e abril de 2023, ele roubou mais de 2.000 páginas de materiais confidenciais relacionados à IA, que foram armazenados em sua conta pessoal do Google Cloud. Os documentos continham informações cruciais sobre a infraestrutura de supercomputação da Google, tecnologias proprietárias de TPU e GPU, software de orquestração para cargas de trabalho de IA em larga escala e tecnologia de rede SmartNIC. Além de sua função no Google, Ding tinha vínculos secretos com duas empresas de tecnologia baseadas na China e chegou a negociar um cargo de CTO em uma delas. Ele fundou sua própria empresa de IA na China e buscou ajudar o governo chinês a desenvolver infraestrutura de computação em nível internacional. Após um julgamento de 11 dias, Ding foi condenado por espionagem econômica e roubo de segredos comerciais, com penas que podem chegar a 15 anos de prisão. A sentença ainda não foi anunciada.&lt;/p></description></item><item><title>Campanha de Ciberespionagem Alinhada ao Irã Alvo de ONGs e Indivíduos</title><link>https://brdefense.center/news/campanha-de-ciberespionagem-alinhada-ao-ira-alvo-d/</link><pubDate>Sat, 31 Jan 2026 13:00:19 -0300</pubDate><guid>https://brdefense.center/news/campanha-de-ciberespionagem-alinhada-ao-ira-alvo-d/</guid><description>&lt;p>Um novo ataque cibernético, denominado RedKitten, foi identificado como uma campanha de ciberespionagem atribuída a um ator de ameaça que fala farsi e está alinhado aos interesses do estado iraniano. O ataque visa organizações não governamentais e indivíduos que documentam abusos de direitos humanos no Irã, especialmente em meio a protestos contra a inflação e a desvalorização da moeda. O malware utilizado neste ataque se aproveita de arquivos do Microsoft Excel com macros maliciosas, que, ao serem ativadas, instalam um backdoor chamado SloppyMIO. Este backdoor utiliza GitHub e Google Drive para obter suas configurações e módulos, além de se comunicar via Telegram. A análise sugere que o código VBA malicioso pode ter sido gerado por modelos de linguagem de inteligência artificial, o que representa uma nova tendência nas táticas de ataque. A campanha também explora a vulnerabilidade emocional das vítimas, que buscam informações sobre pessoas desaparecidas, utilizando dados falsificados para enganar os alvos. Este incidente destaca a crescente complexidade das ameaças cibernéticas, especialmente com a utilização de ferramentas de IA, dificultando a identificação de atores maliciosos.&lt;/p></description></item><item><title>Hackers criam plataforma com IA para checar cartões roubados</title><link>https://brdefense.center/news/hackers-criam-plataforma-com-ia-para-checar-cartoe/</link><pubDate>Fri, 30 Jan 2026 13:23:02 -0300</pubDate><guid>https://brdefense.center/news/hackers-criam-plataforma-com-ia-para-checar-cartoe/</guid><description>&lt;p>Uma nova plataforma ilegal chamada &amp;lsquo;E-Fraud&amp;rsquo; foi descoberta pela empresa de cibersegurança Swarmy, operando no Brasil e utilizando inteligência artificial para verificar cartões de crédito obtidos de forma ilícita. Localizada em um servidor nos Estados Unidos, a plataforma funciona como um hub de crime digital, oferecendo serviços que incluem a validação de cartões roubados. Os usuários podem adquirir créditos para acessar os serviços, com pacotes que variam de R$ 100 a R$ 4 mil. A interface sofisticada sugere um uso avançado de IA, com dashboards que facilitam a integração de sistemas e pagamentos. Além disso, a plataforma utiliza estratégias de marketing que imitam empresas legítimas, criando uma falsa sensação de segurança para os usuários. O E-Fraud também permite transações via QR Codes e oferece bônus em depósitos, dificultando a rastreabilidade financeira. Este cenário evidencia a crescente sofisticação do crime organizado online e a necessidade de medidas de segurança mais robustas para proteger dados financeiros dos brasileiros.&lt;/p></description></item><item><title>IA descobre 12 falhas no OpenSSL que estavam ocultas desde 1998</title><link>https://brdefense.center/news/ia-descobre-12-falhas-no-openssl-que-estavam-ocult/</link><pubDate>Fri, 30 Jan 2026 13:22:43 -0300</pubDate><guid>https://brdefense.center/news/ia-descobre-12-falhas-no-openssl-que-estavam-ocult/</guid><description>&lt;p>Um estudo recente da empresa de cibersegurança Aisle revelou 12 vulnerabilidades no OpenSSL, um dos principais padrões de segurança da internet, utilizando inteligência artificial. Essas falhas, algumas com mais de 20 anos, não foram detectadas por revisões manuais anteriores. As vulnerabilidades variam em severidade, desde alta até moderada, e incluem problemas críticos como transbordamento de buffer e falta de validação, que podem permitir a execução remota de códigos maliciosos. O OpenSSL é amplamente utilizado para implementar protocolos de segurança TLS e SSL, sendo essencial para a proteção de sites com HTTPS. A Aisle já disponibilizou correções para as falhas identificadas, em colaboração com a OpenSSL. O uso de IA na detecção de vulnerabilidades tem se mostrado eficaz, pois consegue analisar o código de forma contextual, priorizando as ameaças e reduzindo falsos positivos. Este avanço na cibersegurança é crucial, especialmente considerando que a indústria enfrenta um aumento nas ameaças digitais que também utilizam inteligência artificial.&lt;/p></description></item><item><title>Engenheiro do Google é condenado por espionagem econômica nos EUA</title><link>https://brdefense.center/news/engenheiro-do-google-e-condenado-por-espionagem-ec/</link><pubDate>Fri, 30 Jan 2026 07:19:05 -0300</pubDate><guid>https://brdefense.center/news/engenheiro-do-google-e-condenado-por-espionagem-ec/</guid><description>&lt;p>Um ex-engenheiro do Google, Linwei Ding, foi condenado nos Estados Unidos por roubo de segredos comerciais e espionagem econômica, após ter transferido mais de 2.000 documentos confidenciais da empresa para sua conta pessoal no Google Cloud. Os documentos continham informações cruciais sobre a tecnologia de inteligência artificial (IA) da empresa, incluindo detalhes sobre chips personalizados e infraestrutura de supercomputação. Ding, que trabalhou na Google de 2019 até sua demissão em 2023, estava em processo de fundar uma startup na China focada em IA. A investigação revelou que ele tomou medidas enganosas para ocultar suas ações, como copiar dados para um aplicativo de notas e apresentar-se como ainda empregado da Google enquanto estava na China. O Departamento de Justiça dos EUA destacou a importância de proteger a propriedade intelectual americana contra interesses estrangeiros, especialmente em um setor tão estratégico como o de IA. Ding enfrenta penas de até 15 anos de prisão por cada acusação de espionagem econômica e 10 anos por roubo de segredos comerciais.&lt;/p></description></item><item><title>A Nova Realidade da Conformidade em Cibersegurança com IA</title><link>https://brdefense.center/news/a-nova-realidade-da-conformidade-em-ciberseguranca/</link><pubDate>Wed, 28 Jan 2026 13:20:40 -0300</pubDate><guid>https://brdefense.center/news/a-nova-realidade-da-conformidade-em-ciberseguranca/</guid><description>&lt;p>O artigo de Itamar Apelblat discute como a evolução da inteligência artificial (IA) está desafiando os tradicionais frameworks de conformidade, que foram construídos sob a premissa de que humanos são os principais atores em processos de negócios. Com a incorporação de agentes de IA em fluxos de trabalho regulados, surgem novos riscos de identidade, acesso e conformidade. Esses agentes não apenas assistem, mas agem de forma autônoma, o que pode levar a falhas de conformidade, já que suas decisões são baseadas em algoritmos que mudam constantemente. Isso representa um desafio significativo para os Chief Information Security Officers (CISOs), que agora podem ser responsabilizados não apenas por violações de segurança, mas também por falhas de conformidade resultantes do comportamento da IA. O artigo destaca a necessidade de uma governança robusta sobre identidades não humanas e a importância de controles de acesso rigorosos para garantir a integridade dos dados e a conformidade com regulamentações como SOX, GDPR, PCI DSS e HIPAA. À medida que a IA se torna um ator operacional, a linha entre segurança e conformidade se torna cada vez mais tênue, exigindo que os CISOs adaptem suas estratégias de segurança para incluir esses novos desafios.&lt;/p></description></item><item><title>Extensões maliciosas de IA para o VSCode podem ter afetado milhões</title><link>https://brdefense.center/news/extensoes-maliciosas-de-ia-para-o-vscode-podem-ter/</link><pubDate>Tue, 27 Jan 2026 13:05:40 -0300</pubDate><guid>https://brdefense.center/news/extensoes-maliciosas-de-ia-para-o-vscode-podem-ter/</guid><description>&lt;p>Cerca de 1,5 milhão de usuários do Visual Studio Code (VSCode) podem ter sido impactados por duas extensões maliciosas que se apresentavam como assistentes de inteligência artificial. Pesquisadores da Koi Security identificaram que essas extensões, chamadas &amp;lsquo;ChatGPT – 中文版&amp;rsquo; e &amp;lsquo;ChatMoss (CodeMoss)&amp;rsquo;, foram criadas por hackers chineses e têm como objetivo roubar dados sensíveis, como senhas e informações de criptomoedas. Apesar de oferecerem funcionalidades legítimas, as extensões enviam dados para um servidor malicioso na China sem o conhecimento dos usuários. O ataque é parte de uma campanha denominada &amp;lsquo;MaliciousCorgi&amp;rsquo;, que utiliza três métodos distintos para exfiltrar informações: monitoramento em tempo real dos arquivos abertos, captura silenciosa de até 50 arquivos e um iframe invisível que rastreia o comportamento do usuário. A Microsoft está ciente do problema e está avaliando a situação, mas as extensões ainda estão disponíveis para download no marketplace do VSCode. Este incidente destaca a necessidade de vigilância constante em relação a ferramentas de desenvolvimento amplamente utilizadas e a importância de verificar a origem das extensões instaladas.&lt;/p></description></item><item><title>Comissão Europeia investiga uso de IA pela X após geração de deepfakes</title><link>https://brdefense.center/news/comissao-europeia-investiga-uso-de-ia-pela-x-apos/</link><pubDate>Mon, 26 Jan 2026 19:01:05 -0300</pubDate><guid>https://brdefense.center/news/comissao-europeia-investiga-uso-de-ia-pela-x-apos/</guid><description>&lt;p>A Comissão Europeia anunciou a abertura de um processo formal sob a Lei de Serviços Digitais (DSA) para investigar se a plataforma X avaliou adequadamente os riscos antes de implementar sua ferramenta de inteligência artificial, Grok. A preocupação surge após relatos de que a ferramenta foi utilizada para gerar imagens sexualmente explícitas manipuladas, incluindo conteúdos que podem ser classificados como material de abuso sexual infantil. A comissária de tecnologia da UE, Henna Virkkunen, destacou que os deepfakes sexuais são uma forma inaceitável de degradação. Além disso, as autoridades do Reino Unido também estão investigando a plataforma, após o uso do chatbot Grok para criar imagens de usuários nus e material de abuso sexual infantil. Em resposta às preocupações, a X anunciou que restringiria as capacidades de geração e edição de imagens do Grok apenas para assinantes pagos, uma decisão criticada por autoridades britânicas como desrespeitosa às vítimas de violência sexual. Como uma plataforma online de grande porte, a X deve mitigar riscos sistêmicos conforme definido pela DSA, incluindo a disseminação de conteúdo ilegal. A Comissão Europeia já multou a X em €120 milhões por violações anteriores de obrigações de transparência sob a DSA.&lt;/p></description></item><item><title>Hackers norte-coreanos espalham backdoor em PowerShell gerado por IA</title><link>https://brdefense.center/news/hackers-norte-coreanos-espalham-backdoor-em-powers/</link><pubDate>Mon, 26 Jan 2026 13:05:56 -0300</pubDate><guid>https://brdefense.center/news/hackers-norte-coreanos-espalham-backdoor-em-powers/</guid><description>&lt;p>Hackers do grupo Konni, da Coreia do Norte, estão utilizando ferramentas de inteligência artificial para criar e espalhar um malware em PowerShell, visando roubar informações de equipes de engenharia de blockchain. A campanha de spear-phishing, iniciada em janeiro de 2026, tem se concentrado em países como Japão, Austrália e Índia. Os ataques se disfarçam como alertas financeiros, levando as vítimas a baixar arquivos ZIP que contêm um atalho do Windows disfarçado de documento PDF. Esse atalho executa um script AutoIt que instala o trojan EndRAT, que, por sua vez, ativa um loader do PowerShell para extrair documentos do Microsoft Word, distraindo a vítima enquanto um backdoor é instalado. Este backdoor permite que os hackers elevem seus privilégios no sistema e se comuniquem com um servidor C2 criptografado, enviando metadados do usuário. O uso de IA para gerar o malware indica uma evolução nas técnicas de ataque, permitindo uma automação maior e uma padronização do código malicioso, o que pode aumentar a eficácia dos ataques.&lt;/p></description></item><item><title>Hackers usam LLMs para criar novos ataques de phishing</title><link>https://brdefense.center/news/hackers-usam-llms-para-criar-novos-ataques-de-phis/</link><pubDate>Mon, 26 Jan 2026 13:05:07 -0300</pubDate><guid>https://brdefense.center/news/hackers-usam-llms-para-criar-novos-ataques-de-phis/</guid><description>&lt;p>Pesquisadores da Palo Alto Networks, através da unidade Unit 42, alertam que hackers estão utilizando Inteligência Artificial Generativa (GenAI) para desenvolver ataques de phishing mais sofisticados e personalizados. A técnica envolve a criação de páginas de phishing dinâmicas que se adaptam ao usuário, utilizando APIs de Modelos de Linguagem de Grande Escala (LLMs) para gerar códigos JavaScript únicos em tempo real. Isso dificulta a detecção por métodos tradicionais, pois o código malicioso não é entregue de forma estática, mas sim gerado na hora, tornando a análise prévia quase impossível. Embora ainda seja uma prova de conceito, a pesquisa indica que os fundamentos para esses ataques já estão sendo explorados, com um aumento no uso de malware e ransomware assistidos por LLMs. Os especialistas recomendam que as empresas restrinjam o uso de serviços LLM não autorizados e implementem medidas de segurança mais robustas para prevenir esses novos tipos de ataques.&lt;/p></description></item><item><title>A evolução das ameaças cibernéticas com inteligência artificial</title><link>https://brdefense.center/news/a-evolucao-das-ameacas-ciberneticas-com-inteligenc/</link><pubDate>Mon, 26 Jan 2026 13:04:01 -0300</pubDate><guid>https://brdefense.center/news/a-evolucao-das-ameacas-ciberneticas-com-inteligenc/</guid><description>&lt;p>O cenário da cibersegurança está passando por uma transformação significativa devido ao uso crescente de inteligência artificial (IA) por atacantes. Um relatório do Google Threat Intelligence Group destaca como adversários estão utilizando Modelos de Linguagem Grande (LLMs) para ocultar códigos e gerar scripts maliciosos em tempo real, dificultando a detecção por defesas convencionais. Em novembro de 2025, a Anthropic revelou a primeira campanha de espionagem cibernética orquestrada por IA, onde o ataque foi realizado de forma quase autônoma. Além disso, técnicas de esteganografia estão sendo empregadas para esconder malware em arquivos de imagem, enganando usuários e permitindo a instalação de trojans de acesso remoto (RATs) e outros malwares.&lt;/p></description></item><item><title>Grupo Konni usa malware PowerShell com IA para atacar blockchain</title><link>https://brdefense.center/news/grupo-konni-usa-malware-powershell-com-ia-para-ata/</link><pubDate>Mon, 26 Jan 2026 07:02:31 -0300</pubDate><guid>https://brdefense.center/news/grupo-konni-usa-malware-powershell-com-ia-para-ata/</guid><description>&lt;p>O grupo de ameaças cibernéticas norte-coreano conhecido como Konni tem sido observado utilizando malware PowerShell gerado por ferramentas de inteligência artificial (IA) para atacar desenvolvedores e equipes de engenharia no setor de blockchain. A campanha de phishing, que se expandiu para além da Coreia do Sul, agora mira países como Japão, Austrália e Índia. Konni, ativo desde 2014, é conhecido por suas táticas de engenharia social, como o uso de e-mails de spear-phishing que disfarçam links maliciosos como URLs de publicidade legítimas. Recentemente, a campanha denominada Operação Poseidon tem se concentrado em imitar organizações de direitos humanos e instituições financeiras sul-coreanas. Os ataques utilizam sites WordPress mal configurados para distribuir malware e infraestrutura de comando e controle. O malware, chamado EndRAT, é entregue através de arquivos ZIP que contêm atalhos do Windows projetados para executar scripts maliciosos. A análise sugere que a estrutura modular do backdoor PowerShell pode ter sido criada com a ajuda de ferramentas de IA, indicando uma evolução nas táticas do grupo, que agora busca comprometer ambientes de desenvolvimento para acesso mais amplo a projetos e serviços.&lt;/p></description></item><item><title>Agentes de IA e os Desafios de Segurança nas Empresas</title><link>https://brdefense.center/news/agentes-de-ia-e-os-desafios-de-seguranca-nas-empre/</link><pubDate>Sat, 24 Jan 2026 06:57:33 -0300</pubDate><guid>https://brdefense.center/news/agentes-de-ia-e-os-desafios-de-seguranca-nas-empre/</guid><description>&lt;p>Os agentes de inteligência artificial (IA) estão transformando a forma como as empresas operam, aumentando a produtividade ao automatizar tarefas como agendamento de reuniões e acesso a dados. No entanto, essa rápida adoção levanta preocupações significativas de segurança, especialmente em relação à aprovação e responsabilidade pelo uso desses agentes. Diferente de usuários humanos ou contas de serviço tradicionais, os agentes de IA operam com autoridade delegada, permitindo-lhes agir em nome de múltiplos usuários sem supervisão contínua. Isso resulta em um fenômeno conhecido como &amp;lsquo;desvio de acesso&amp;rsquo;, onde os agentes acumulam permissões que podem exceder o que um usuário individual estaria autorizado a fazer. O artigo classifica os agentes de IA em três categorias: pessoais, de terceiros e organizacionais, sendo estes últimos os mais arriscados devido à falta de um proprietário claro e à possibilidade de ações não autorizadas. Para mitigar esses riscos, as organizações precisam redefinir a gestão de acesso, estabelecendo propriedade clara e mapeando como os usuários interagem com os agentes. A segurança dos agentes de IA deve ser tratada como uma prioridade, considerando seu potencial de criar caminhos de autorização que podem ser explorados maliciosamente.&lt;/p></description></item><item><title>Extensões maliciosas no VSCode Marketplace comprometem dados de desenvolvedores</title><link>https://brdefense.center/news/extensoes-maliciosas-no-vscode-marketplace-comprom/</link><pubDate>Fri, 23 Jan 2026 18:57:05 -0300</pubDate><guid>https://brdefense.center/news/extensoes-maliciosas-no-vscode-marketplace-comprom/</guid><description>&lt;p>Duas extensões maliciosas disponíveis no Marketplace do Visual Studio Code (VSCode) foram instaladas 1,5 milhão de vezes e estão exfiltrando dados de desenvolvedores para servidores localizados na China. Ambas as extensões, apresentadas como assistentes de codificação baseados em inteligência artificial, não informam os usuários sobre a coleta de dados nem solicitam consentimento. Pesquisadores da Koi Security identificaram a campanha &amp;lsquo;MaliciousCorgi&amp;rsquo;, que utiliza o mesmo código para roubar informações. As extensões, &amp;lsquo;ChatGPT – 中文版&amp;rsquo; e &amp;lsquo;ChatMoss&amp;rsquo;, empregam três mecanismos distintos para coletar dados: monitoramento em tempo real de arquivos abertos, comandos de coleta de arquivos e carregamento de SDKs de análise comercial para rastrear o comportamento do usuário. O uso dessas extensões pode expor códigos-fonte privados, arquivos de configuração e credenciais de serviços em nuvem, representando um risco significativo para a segurança dos desenvolvedores. A Microsoft foi contatada sobre a presença dessas extensões, mas ainda não se manifestou. Este incidente destaca a necessidade de vigilância constante em relação a complementos de software, especialmente aqueles que prometem funcionalidades avançadas sem transparência.&lt;/p></description></item><item><title>Hackers utilizam IA para disseminar malware em anúncios no Android</title><link>https://brdefense.center/news/hackers-utilizam-ia-para-disseminar-malware-em-anu/</link><pubDate>Fri, 23 Jan 2026 13:06:26 -0300</pubDate><guid>https://brdefense.center/news/hackers-utilizam-ia-para-disseminar-malware-em-anu/</guid><description>&lt;p>Um novo tipo de malware para dispositivos Android está utilizando inteligência artificial (IA) para contornar sistemas de segurança convencionais. De acordo com informações do Tech Radar, cibercriminosos estão empregando a plataforma de machine learning TensorFlow, desenvolvida pelo Google, para distribuir trojans através de anúncios maliciosos. A técnica envolve a criação de aplicativos falsos que são disseminados principalmente pela loja GetApps da Xiaomi, além de outros canais como redes sociais e Telegram.&lt;/p></description></item><item><title>Curl encerra programa de recompensas por vulnerabilidades devido a relatórios ruins</title><link>https://brdefense.center/news/curl-encerra-programa-de-recompensas-por-vulnerabi/</link><pubDate>Thu, 22 Jan 2026 19:00:21 -0300</pubDate><guid>https://brdefense.center/news/curl-encerra-programa-de-recompensas-por-vulnerabi/</guid><description>&lt;p>O projeto curl, uma popular ferramenta de linha de comando para transferência de dados, anunciou o fim de seu programa de recompensas por vulnerabilidades na HackerOne, a partir do final deste mês. A decisão foi motivada pelo aumento de relatórios de baixa qualidade, muitos dos quais gerados por inteligência artificial, que sobrecarregaram a equipe de segurança do projeto. Daniel Stenberg, fundador e desenvolvedor principal do curl, destacou que, até o final de janeiro de 2026, ainda serão aceitas submissões, mas a partir de fevereiro, os pesquisadores deverão relatar problemas de segurança diretamente pelo GitHub. O projeto não oferecerá mais recompensas financeiras e advertiu que relatórios considerados de baixa qualidade resultarão em banimento e ridicularização pública. Essa mudança reflete uma preocupação com a saúde mental da equipe e a necessidade de manter a eficiência do projeto, que é mantido por um número limitado de colaboradores. Stenberg também observou um aumento significativo nas submissões de vulnerabilidades em comparação com outros projetos de código aberto, o que reforça a urgência da decisão.&lt;/p></description></item><item><title>Novos recursos de IA no Notepad e Paint do Windows 11</title><link>https://brdefense.center/news/novos-recursos-de-ia-no-notepad-e-paint-do-windows/</link><pubDate>Thu, 22 Jan 2026 13:18:24 -0300</pubDate><guid>https://brdefense.center/news/novos-recursos-de-ia-no-notepad-e-paint-do-windows/</guid><description>&lt;p>A Microsoft está implementando novas funcionalidades de inteligência artificial nas aplicações Notepad e Paint para usuários do Windows 11 que fazem parte do programa Insider. As atualizações, disponíveis nas versões Canary e Dev, trazem melhorias significativas. O Notepad, agora na versão 11.2512.10.0, introduz ferramentas de escrita, reescrita e resumo que geram resultados de forma mais rápida, permitindo que os usuários vejam prévias sem esperar pela resposta completa. Para acessar essas funcionalidades, é necessário fazer login com uma conta Microsoft. Além disso, o Notepad expande seu suporte a formatação leve com novas opções de sintaxe Markdown, como listas aninhadas e texto riscado.&lt;/p></description></item><item><title>VoidLink malware nativo à nuvem criado com inteligência artificial</title><link>https://brdefense.center/news/voidlink-malware-nativo-a-nuvem-criado-com-intelig/</link><pubDate>Wed, 21 Jan 2026 19:03:38 -0300</pubDate><guid>https://brdefense.center/news/voidlink-malware-nativo-a-nuvem-criado-com-intelig/</guid><description>&lt;p>Pesquisadores da Check Point Research (CPR) identificaram um novo malware chamado VoidLink, que opera em ambientes Linux e foi desenvolvido quase que inteiramente com o auxílio de inteligência artificial (IA). O malware possui uma estrutura complexa, incluindo loaders, módulos de rootkit e uma variedade de plugins. A criação do VoidLink foi realizada em apenas uma semana, utilizando a TRAE SOLO, um assistente de IA que ajudou o desenvolvedor a gerar um código-fonte de 88.000 linhas. Embora o hacker não tenha conseguido ocultar completamente suas atividades, falhas em sua implementação permitiram que os pesquisadores acessassem o código e a documentação do projeto. O VoidLink é considerado o primeiro exemplo documentado de um malware avançado gerado por IA, o que levanta preocupações sobre a capacidade de indivíduos com conhecimentos técnicos limitados de criar ameaças cibernéticas sofisticadas. Essa nova era de desenvolvimento de malware pode alterar significativamente o cenário da cibersegurança, tornando mais fácil para cibercriminosos desenvolverem ferramentas complexas sem a necessidade de grandes equipes de desenvolvimento.&lt;/p></description></item><item><title>Phishing A Ameaça que Pode Atingir Qualquer Um</title><link>https://brdefense.center/news/phishing-a-ameaca-que-pode-atingir-qualquer-um/</link><pubDate>Wed, 21 Jan 2026 13:21:07 -0300</pubDate><guid>https://brdefense.center/news/phishing-a-ameaca-que-pode-atingir-qualquer-um/</guid><description>&lt;p>O phishing é uma técnica de engenharia social que visa enganar usuários para que revelem informações sensíveis, como dados de pagamento e credenciais. O artigo destaca que mesmo profissionais experientes em cibersegurança podem ser vítimas desse tipo de ataque, especialmente em momentos de distração ou estresse emocional. As mensagens de phishing, que podem chegar por e-mail, SMS ou aplicativos de mensagens, frequentemente imitam interações digitais comuns, como notificações de pacotes ou alertas de segurança, tornando-se cada vez mais convincentes. Além disso, a pesquisa revela que o phishing se transformou em uma economia industrializada, com plataformas de phishing como serviço (PhaaS) que permitem a qualquer pessoa, independentemente de habilidade técnica, realizar ataques sofisticados. O uso de inteligência artificial para gerar mensagens personalizadas e contextuais aumenta ainda mais a eficácia desses ataques. A urgência e a distração são fatores críticos que facilitam a ação dos atacantes, tornando a conscientização e a vigilância hábitos essenciais para todos os usuários.&lt;/p></description></item><item><title>A Revolução da Cibersegurança Como a IA Está Transformando MSSPs</title><link>https://brdefense.center/news/a-revolucao-da-ciberseguranca-como-a-ia-esta-trans/</link><pubDate>Wed, 21 Jan 2026 13:19:34 -0300</pubDate><guid>https://brdefense.center/news/a-revolucao-da-ciberseguranca-como-a-ia-esta-trans/</guid><description>&lt;p>Em 2026, os provedores de segurança gerenciados (MSSPs) enfrentam um desafio crescente: a quantidade excessiva de alertas e a escassez de analistas, enquanto os clientes exigem proteção de nível CISO com orçamentos de pequenas e médias empresas (PMEs). A solução para esse dilema pode estar na automação por inteligência artificial (IA), que promete revolucionar a entrega de serviços de segurança. Em vez de simplesmente adicionar mais analistas a cada novo cliente, a IA pode realizar avaliações, benchmarking e relatórios em minutos, permitindo que as equipes se concentrem em estratégias mais complexas. O caso de Chad Robinson, CISO da Secure Cyber Defense, ilustra essa mudança: ao implementar a plataforma de IA da Cynomi, sua equipe não apenas automatizou relatórios, mas também transformou analistas juniores em &amp;lsquo;CISOs virtuais&amp;rsquo;, aumentando a cobertura e a receita de serviços de consultoria. Os primeiros adotantes dessa tecnologia já estão observando ganhos significativos nas margens e ciclos de integração mais rápidos, sem a necessidade de aumentar a equipe. O artigo destaca a importância de adotar a IA para escalar negócios de segurança sem aumentar a folha de pagamento, enfatizando que os MSSPs que prosperarão em 2026 não serão necessariamente os maiores, mas os mais inteligentes.&lt;/p></description></item><item><title>Malware VoidLink A Revolução da IA na Cibersegurança</title><link>https://brdefense.center/news/malware-voidlink-a-revolucao-da-ia-na-ciberseguran/</link><pubDate>Wed, 21 Jan 2026 07:02:35 -0300</pubDate><guid>https://brdefense.center/news/malware-voidlink-a-revolucao-da-ia-na-ciberseguran/</guid><description>&lt;p>O malware VoidLink, um sofisticado framework para Linux, foi desenvolvido com a ajuda de um modelo de inteligência artificial (IA), segundo a Check Point Research. Identificado como um dos primeiros exemplos de malware avançado gerado em grande parte por IA, o VoidLink possui mais de 88.000 linhas de código e foi projetado para acesso furtivo a ambientes em nuvem baseados em Linux. A análise sugere que um desenvolvedor experiente, possivelmente de origem chinesa, utilizou um agente de codificação chamado TRAE SOLO para acelerar o desenvolvimento, que levou menos de uma semana para criar um protótipo funcional. A pesquisa também revelou que a documentação interna do projeto, escrita em chinês, apresenta características típicas de conteúdo gerado por IA, como formatação consistente e detalhes meticulosos. Embora ainda não tenham sido observadas infecções reais, o desenvolvimento do VoidLink representa uma mudança significativa na forma como malware avançado pode ser criado, permitindo que indivíduos com menos recursos realizem ataques complexos de forma rápida e eficiente. Especialistas alertam que a IA está transformando a cibercriminalidade, tornando ferramentas sofisticadas acessíveis a qualquer um com um cartão de crédito.&lt;/p></description></item><item><title>Google não planeja anúncios no Gemini, enquanto ChatGPT testa publicidade</title><link>https://brdefense.center/news/google-nao-planeja-anuncios-no-gemini-enquanto-cha/</link><pubDate>Wed, 21 Jan 2026 01:35:59 -0300</pubDate><guid>https://brdefense.center/news/google-nao-planeja-anuncios-no-gemini-enquanto-cha/</guid><description>&lt;p>Recentemente, a OpenAI começou a testar anúncios no ChatGPT nos Estados Unidos, tanto para usuários da conta gratuita quanto para assinantes do plano Go, que custa US$ 8 por mês. Em contraste, o CEO da Google DeepMind, Demis Hassabis, afirmou que o Gemini, a nova plataforma de inteligência artificial da Google, não terá anúncios por enquanto. Durante o Fórum Econômico Mundial em Davos, Hassabis comentou que achou interessante a decisão da OpenAI de introduzir anúncios tão cedo, sugerindo que isso pode ser uma estratégia para aumentar a receita. O ChatGPT planeja implementar anúncios em breve, mas apenas para usuários gratuitos e do plano Go, enquanto as contas pagas, como Plus e Pro, permanecerão livres de publicidade. Os anúncios aparecerão apenas quando houver produtos ou serviços patrocinados relevantes ao tema da conversa, e os usuários poderão entender o motivo da exibição do anúncio e fornecer feedback. No entanto, não haverá anúncios em discussões sobre saúde, saúde mental ou política. Essa movimentação levanta questões sobre a monetização de plataformas de IA e suas implicações para os usuários e o mercado.&lt;/p></description></item><item><title>Malware VoidLink Ameaça Avançada Desenvolvida com IA</title><link>https://brdefense.center/news/malware-voidlink-ameaca-avancada-desenvolvida-com/</link><pubDate>Tue, 20 Jan 2026 18:57:36 -0300</pubDate><guid>https://brdefense.center/news/malware-voidlink-ameaca-avancada-desenvolvida-com/</guid><description>&lt;p>O malware VoidLink, recentemente descoberto, é um framework avançado focado em ambientes de nuvem, desenvolvido por um único criador com auxílio de um modelo de inteligência artificial. De acordo com a Check Point Research, o VoidLink é um malware para Linux que inclui carregadores personalizados, implantes, módulos de rootkit para evasão e uma variedade de plugins que ampliam suas funcionalidades. A pesquisa sugere que o desenvolvedor possui forte proficiência em várias linguagens de programação, possivelmente originando-se da China.&lt;/p></description></item><item><title>ALOHA reduz tempo de análise de ciberataques de semanas para horas</title><link>https://brdefense.center/news/aloha-reduz-tempo-de-analise-de-ciberataques-de-se/</link><pubDate>Tue, 20 Jan 2026 13:22:21 -0300</pubDate><guid>https://brdefense.center/news/aloha-reduz-tempo-de-analise-de-ciberataques-de-se/</guid><description>&lt;p>O Laboratório Nacional do Noroeste do Pacífico (PNNL) dos Estados Unidos desenvolveu um sistema inovador chamado ALOHA, que utiliza inteligência artificial para emular ameaças digitais de forma mais eficiente. Com essa nova ferramenta, o tempo necessário para recriar ataques e testar defesas foi reduzido de meses para apenas algumas horas. O ALOHA, que se baseia no modelo de linguagem Claude, automatiza a emulação de ameaças e gera até 20 táticas diferentes a partir de um malware original, permitindo que pesquisadores identifiquem vulnerabilidades e testem novas defesas de maneira mais rápida e eficaz. Essa inovação é especialmente relevante em um cenário onde a cibersegurança se tornou uma corrida armamentista entre hackers e organizações. O uso de IA generativa já é uma prática comum, mas o ALOHA promete otimizar ainda mais esse processo, ajudando equipes de segurança a responderem rapidamente a novas ameaças. A ferramenta também se integra com a plataforma Caldera, da MITRE, que já realiza parte desse trabalho, mas de forma mais lenta e detalhada. Com a crescente complexidade dos ataques cibernéticos, a implementação de soluções como o ALOHA pode ser um divisor de águas na proteção de sistemas críticos.&lt;/p></description></item><item><title>Governança em escala é essencial para IA nas organizações</title><link>https://brdefense.center/news/governanca-em-escala-e-essencial-para-ia-nas-organ/</link><pubDate>Mon, 19 Jan 2026 07:03:40 -0300</pubDate><guid>https://brdefense.center/news/governanca-em-escala-e-essencial-para-ia-nas-organ/</guid><description>&lt;p>A inteligência artificial (IA) se tornou uma parte integral das operações empresariais, automatizando processos e auxiliando na tomada de decisões. No entanto, à medida que a IA acessa dados sensíveis e executa ações, ela se transforma em um vetor de risco potencial. Pesquisas da Tenable indicam que a IA pode ser manipulada para facilitar ataques internos, utilizando técnicas como injeção indireta de instruções. Isso altera a abordagem tradicional de segurança, que se focava apenas em invasões externas. A popularização de ferramentas no-code, que permitem que mais colaboradores criem agentes de IA, aumenta a exposição a riscos, como vazamentos de dados e fraudes financeiras. Para mitigar esses riscos, é crucial que as lideranças empresariais respondam a três perguntas fundamentais sobre o uso da IA em suas organizações. A governança deve ser proporcional ao impacto, envolvendo práticas como mapeamento de ferramentas, classificação de dados e monitoramento de interações. A maturidade em IA será medida não pela rapidez de adoção, mas pela capacidade de controle e segurança na sua implementação.&lt;/p></description></item><item><title>OpenAI começará a exibir anúncios no ChatGPT para usuários nos EUA</title><link>https://brdefense.center/news/openai-comecara-a-exibir-anuncios-no-chatgpt-para/</link><pubDate>Sat, 17 Jan 2026 06:57:07 -0300</pubDate><guid>https://brdefense.center/news/openai-comecara-a-exibir-anuncios-no-chatgpt-para/</guid><description>&lt;p>A OpenAI anunciou que começará a exibir anúncios no ChatGPT para usuários adultos logados nos Estados Unidos, tanto na versão gratuita quanto na versão ChatGPT Go, nas próximas semanas. A empresa garantiu que os dados e conversas dos usuários estão protegidos e não serão vendidos a anunciantes. A introdução de anúncios visa tornar os benefícios da inteligência artificial mais acessíveis e ajudar pequenas empresas a competir. Os anúncios aparecerão no final das conversas e serão claramente rotulados, sem influenciar as respostas do chatbot. Usuários em planos mais caros, como Plus e Pro, não verão anúncios. A OpenAI não especificou quais dados serão coletados para personalizar os anúncios, mas os usuários poderão entender o motivo pelo qual estão vendo determinados anúncios e poderão desativar a personalização. A decisão de incluir anúncios representa uma mudança significativa na estratégia da OpenAI, que até agora dependia principalmente de assinaturas. O CEO Sam Altman comentou que a empresa não aceitará dinheiro para influenciar as respostas do ChatGPT, enfatizando que a publicidade é uma alternativa para sustentar o alto custo do desenvolvimento da inteligência artificial.&lt;/p></description></item><item><title>OpenAI confirma anúncios no ChatGPT para usuários gratuitos</title><link>https://brdefense.center/news/openai-confirma-anuncios-no-chatgpt-para-usuarios/</link><pubDate>Sat, 17 Jan 2026 01:21:33 -0300</pubDate><guid>https://brdefense.center/news/openai-confirma-anuncios-no-chatgpt-para-usuarios/</guid><description>&lt;p>A OpenAI anunciou que o ChatGPT começará a exibir anúncios nas próximas semanas, mas assegurou que esses anúncios não influenciarão as respostas geradas pela inteligência artificial. Os anúncios aparecerão dentro das respostas, provavelmente na parte inferior, e serão visíveis apenas para usuários de contas gratuitas ou da versão ChatGPT Go. A empresa, apoiada pela Microsoft, argumenta que a inclusão de anúncios ajudará a financiar suas ambições em Inteligência Geral Artificial (AGI), que visa beneficiar toda a humanidade. Os anúncios serão claramente rotulados e não aparecerão em conversas que abordem tópicos sensíveis, como saúde ou política. Além disso, a OpenAI garantiu que as conversas dos usuários permanecerão privadas e que os dados não serão vendidos a anunciantes. Para aqueles que preferirem não ver anúncios, a empresa sugere considerar a atualização para a assinatura de $20 ou a mudança para alternativas como Claude. Essa mudança pode impactar a experiência do usuário e levanta questões sobre privacidade e monetização de dados.&lt;/p></description></item><item><title>PagBank intensifica segurança contra fraudes digitais em 2026</title><link>https://brdefense.center/news/pagbank-intensifica-seguranca-contra-fraudes-digit/</link><pubDate>Fri, 16 Jan 2026 13:03:42 -0300</pubDate><guid>https://brdefense.center/news/pagbank-intensifica-seguranca-contra-fraudes-digit/</guid><description>&lt;p>O PagBank anunciou um reforço nas suas medidas de cibersegurança no início de 2026, em resposta ao aumento das fraudes digitais no Brasil. O país, que ocupa a segunda posição mundial em ciberataques, registrou quase 7 milhões de tentativas de fraude no primeiro semestre de 2025, um aumento de 29,5% em relação ao ano anterior. Para combater essa situação, o PagBank implementou diversas funcionalidades de segurança, como alertas de login em dispositivos de risco, que notificam os usuários sobre tentativas de acesso suspeitas, e o uso de QR Codes para autenticação de transações. Além disso, a empresa introduziu a rede &amp;lsquo;Wi-Fi Seguro&amp;rsquo;, que protege os usuários em conexões públicas, e utiliza inteligência artificial para detectar comportamentos suspeitos e tentativas de engenharia social. A importância da vigilância constante por parte dos clientes também foi enfatizada, destacando que, apesar das tecnologias avançadas, o cuidado individual é crucial na prevenção de fraudes.&lt;/p></description></item><item><title>Riscos de Segurança em Fluxos de Trabalho com Inteligência Artificial</title><link>https://brdefense.center/news/riscos-de-seguranca-em-fluxos-de-trabalho-com-inte/</link><pubDate>Thu, 15 Jan 2026 13:20:05 -0300</pubDate><guid>https://brdefense.center/news/riscos-de-seguranca-em-fluxos-de-trabalho-com-inte/</guid><description>&lt;p>Com a crescente integração de assistentes de IA nas atividades diárias, a segurança cibernética deve ir além da proteção dos modelos de IA. Recentes incidentes revelaram que o maior risco reside nos fluxos de trabalho que cercam esses modelos. Dois complementos do Chrome, disfarçados de assistentes de IA, foram identificados como responsáveis por roubar dados de chat de mais de 900 mil usuários do ChatGPT e DeepSeek. Além disso, pesquisadores demonstraram como injeções de comandos ocultas em repositórios de código podem enganar assistentes de codificação da IBM, fazendo com que executem malware. Esses ataques não comprometeram os algoritmos de IA, mas exploraram o contexto em que operam. À medida que as empresas utilizam IA para automatizar tarefas, a segurança deve se concentrar na proteção dos fluxos de trabalho, e não apenas nos modelos. Isso implica em entender onde a IA é utilizada, restringir acessos desnecessários e monitorar comportamentos anômalos. Ferramentas como a Reco estão surgindo para ajudar a proteger esses fluxos de trabalho em tempo real, oferecendo visibilidade e controle sobre o uso de IA nas organizações.&lt;/p></description></item><item><title>Falsificação de identidade pode gerar prejuízo de US 17 bilhões em criptomoedas</title><link>https://brdefense.center/news/falsificacao-de-identidade-pode-gerar-prejuizo-de/</link><pubDate>Wed, 14 Jan 2026 18:59:00 -0300</pubDate><guid>https://brdefense.center/news/falsificacao-de-identidade-pode-gerar-prejuizo-de/</guid><description>&lt;p>Fraudes por falsificação de identidade em criptomoedas estão causando perdas significativas, com um prejuízo estimado em US$ 17 bilhões para 2026, conforme relatório da Chainalysis. Em 2025, foram desviados cerca de US$ 14 bilhões para contas criminosas, um aumento em relação aos US$ 13 bilhões de 2024. O crescimento alarmante de 1400% nos casos de falsificação de identidade é impulsionado por táticas de phishing e engenharia social, além do uso crescente de inteligência artificial (IA) por criminosos, que facilita ataques mais rápidos e coordenados. Os golpes que utilizam IA geraram em média US$ 3,2 milhões em criptomoedas por operação, evidenciando a industrialização da fraude. Especialistas alertam que a situação pode se agravar, exigindo atenção redobrada de usuários e empresas que operam no setor de moedas digitais.&lt;/p></description></item><item><title>Agentes de IA Riscos de Acesso e Escalonamento de Privilégios</title><link>https://brdefense.center/news/agentes-de-ia-riscos-de-acesso-e-escalonamento-de/</link><pubDate>Wed, 14 Jan 2026 18:57:51 -0300</pubDate><guid>https://brdefense.center/news/agentes-de-ia-riscos-de-acesso-e-escalonamento-de/</guid><description>&lt;p>Os agentes de inteligência artificial (IA) estão se tornando componentes essenciais nas operações diárias de segurança, engenharia e TI, atuando como intermediários de acesso em diversos sistemas. Esses agentes, que podem automatizar tarefas como provisionamento de contas e gerenciamento de mudanças, são projetados para operar com permissões amplas, o que pode obscurecer a visibilidade sobre quem está acessando o quê. Essa configuração, embora aumente a produtividade, introduz riscos significativos de escalonamento de privilégios, onde usuários com acesso limitado podem, indiretamente, acessar dados ou realizar ações que normalmente não teriam permissão. A falta de controle sobre as permissões dos agentes e a atribuição de atividades a eles, em vez de aos usuários, dificulta a detecção de abusos e a aplicação de políticas de segurança. Para mitigar esses riscos, é crucial que as equipes de segurança monitorem continuamente as permissões dos agentes e a relação entre as identidades dos usuários e os ativos críticos. A adoção segura de agentes de IA requer visibilidade e monitoramento contínuo para evitar que se tornem pontos cegos de segurança.&lt;/p></description></item><item><title>Campanhas de ciberataques visam modelos de linguagem de IA</title><link>https://brdefense.center/news/campanhas-de-ciberataques-visam-modelos-de-linguag/</link><pubDate>Tue, 13 Jan 2026 18:58:59 -0300</pubDate><guid>https://brdefense.center/news/campanhas-de-ciberataques-visam-modelos-de-linguag/</guid><description>&lt;p>Com a crescente popularidade das ferramentas de inteligência artificial (IA), cibercriminosos estão direcionando suas atenções para a exploração de vulnerabilidades em modelos de linguagem de grande escala (LLMs). Pesquisadores da GreyNoise identificaram duas campanhas de ataque que, juntas, contabilizam quase 100 mil tentativas de exploração. Os ataques, que ocorreram entre outubro de 2025 e janeiro de 2026, visaram principalmente empresas que utilizam esses modelos em suas operações diárias. A primeira campanha consistiu na injeção de domínios maliciosos, enquanto a segunda, considerada mais perigosa, focou em testar APIs de serviços de IA de grandes empresas como OpenAI e Google, buscando identificar quais modelos poderiam ser manipulados sem acionar alertas de segurança. Os especialistas alertam que esses ataques representam riscos significativos para a segurança corporativa, especialmente com a adoção crescente de IAs. Recomenda-se que as empresas implementem medidas de segurança mais robustas, como o bloqueio de endereços suspeitos e a configuração de alertas para respostas rápidas a possíveis ameaças.&lt;/p></description></item><item><title>Velho Manual, Nova Escala Ataques Otimizados em 2025</title><link>https://brdefense.center/news/velho-manual-nova-escala-ataques-otimizados-em-202/</link><pubDate>Tue, 13 Jan 2026 18:57:52 -0300</pubDate><guid>https://brdefense.center/news/velho-manual-nova-escala-ataques-otimizados-em-202/</guid><description>&lt;p>O artigo destaca que, apesar da segurança cibernética frequentemente discutir novas ameaças, os ataques mais eficazes em 2025 são semelhantes aos de 2015. Os invasores continuam a explorar pontos de entrada conhecidos, mas com maior eficiência. A cadeia de suprimentos é um foco crítico, como demonstrado pela campanha Shai Hulud NPM, onde um único pacote comprometido pode afetar milhares de projetos. A inteligência artificial facilitou a execução de ataques, permitindo que até indivíduos realizem operações complexas que antes exigiam grandes equipes. O phishing permanece uma ameaça significativa, pois os humanos continuam sendo o elo mais fraco, exemplificado por um ataque recente que comprometeu pacotes com milhões de downloads. Além disso, extensões de navegador maliciosas continuam a contornar os mecanismos de segurança das lojas oficiais. O artigo conclui que, em vez de buscar novas estratégias de defesa, é essencial corrigir os modelos de permissão e fortalecer a verificação da cadeia de suprimentos, priorizando a segurança básica.&lt;/p></description></item><item><title>Empresas finalmente agem para mitigar riscos de segurança em IA</title><link>https://brdefense.center/news/empresas-finalmente-agem-para-mitigar-riscos-de-se/</link><pubDate>Tue, 13 Jan 2026 13:05:59 -0300</pubDate><guid>https://brdefense.center/news/empresas-finalmente-agem-para-mitigar-riscos-de-se/</guid><description>&lt;p>Um novo relatório do Fórum Econômico Mundial (WEF) revela que as empresas estão começando a levar a sério os riscos de segurança associados à inteligência artificial (IA). Quase dois terços (64%) das empresas agora avaliam os riscos antes de implementar ferramentas de IA, um aumento significativo em relação a 37% no ano anterior. A pesquisa, realizada em colaboração com a Accenture, indica que 94% dos executivos acreditam que as ferramentas de IA serão o principal motor de mudança em suas estratégias de cibersegurança até 2026. Apesar do aumento na conscientização sobre as vulnerabilidades relacionadas à IA, como vazamentos de dados e fraudes, as empresas também estão adotando IA para combater essas ameaças, com 77% utilizando a tecnologia para melhorar a segurança cibernética. As aplicações mais comuns incluem a detecção de phishing (52%) e a automação de operações de segurança (43%). No entanto, desafios como a falta de habilidades e a necessidade de validação humana ainda impedem a adoção mais ampla da IA na segurança. O WEF prevê que ameaças como phishing convincente e fraudes automatizadas se tornarão mais prevalentes, destacando a necessidade urgente de as empresas se adaptarem a esse novo cenário de ameaças.&lt;/p></description></item><item><title>Agentes de IA Riscos de Segurança em Protocolos de Controle</title><link>https://brdefense.center/news/agentes-de-ia-riscos-de-seguranca-em-protocolos-de/</link><pubDate>Tue, 13 Jan 2026 13:02:24 -0300</pubDate><guid>https://brdefense.center/news/agentes-de-ia-riscos-de-seguranca-em-protocolos-de/</guid><description>&lt;p>Os agentes de inteligência artificial (IA) estão evoluindo rapidamente, não apenas escrevendo código, mas também executando-o. Ferramentas como Copilot, Claude Code e Codex agora conseguem construir, testar e implantar software em questão de minutos, o que, embora acelere o desenvolvimento, também cria lacunas de segurança que muitas equipes não percebem até que ocorra um problema. Um aspecto crítico que muitas organizações não estão protegendo adequadamente são os Protocolos de Controle de Máquinas (MCPs), que determinam quais comandos um agente de IA pode executar e quais ferramentas e APIs pode acessar. A falha CVE-2025-6514 exemplifica esse risco, onde um proxy OAuth confiável foi transformado em um vetor de execução remota de código, permitindo que a automação realizasse ações não intencionais em larga escala. Este cenário destaca a necessidade urgente de as equipes de segurança entenderem e protegerem esses sistemas de controle, pois, se um agente de IA pode executar comandos, também pode ser usado para realizar ataques. Um webinar está sendo oferecido para discutir esses riscos e como as organizações podem implementar controles práticos para garantir a segurança sem comprometer a velocidade de desenvolvimento.&lt;/p></description></item><item><title>Nova funcionalidade de IA promete melhorar a saúde dos usuários</title><link>https://brdefense.center/news/nova-funcionalidade-de-ia-promete-melhorar-a-saude/</link><pubDate>Mon, 12 Jan 2026 07:02:00 -0300</pubDate><guid>https://brdefense.center/news/nova-funcionalidade-de-ia-promete-melhorar-a-saude/</guid><description>&lt;p>A Anthropic lançou uma nova iniciativa chamada Claude for Healthcare, que permite aos usuários da plataforma Claude, nos EUA, acessar e entender melhor suas informações de saúde. Os assinantes dos planos Claude Pro e Max podem conectar seus resultados de exames e registros médicos através das integrações com HealthEx e Function, com suporte para Apple Health e Android Health Connect previsto para ser lançado em breve. A ferramenta é capaz de resumir o histórico médico dos usuários, explicar resultados de exames em linguagem simples, detectar padrões em métricas de saúde e preparar perguntas para consultas médicas. A Anthropic enfatiza que as integrações são projetadas para serem privadas, permitindo que os usuários escolham quais informações compartilhar e que os dados de saúde não são utilizados para treinar modelos de IA. Este desenvolvimento ocorre em um contexto de crescente preocupação sobre a precisão das informações de saúde fornecidas por sistemas de IA, especialmente após a remoção de resumos de IA do Google que apresentavam informações incorretas. A Anthropic também ressalta que os resultados gerados devem ser revisados por profissionais qualificados antes de qualquer decisão de saúde.&lt;/p></description></item><item><title>Inteligência Artificial vai potencializar golpes em 2026 saiba como se proteger</title><link>https://brdefense.center/news/inteligencia-artificial-vai-potencializar-golpes-e/</link><pubDate>Mon, 12 Jan 2026 01:40:38 -0300</pubDate><guid>https://brdefense.center/news/inteligencia-artificial-vai-potencializar-golpes-e/</guid><description>&lt;p>Em 2026, os golpes digitais devem se intensificar, impulsionados pelo uso da inteligência artificial (IA) por criminosos. Daniel Barbosa, pesquisador de segurança da ESET no Brasil, destaca que a IA está sendo utilizada para criar e-mails e sites falsos com maior precisão, tornando as fraudes mais convincentes. Os golpistas coletam dados de vazamentos para personalizar suas abordagens, tornando-as mais atraentes para as vítimas. Além disso, a evolução dos deepfakes dificulta a identificação de fraudes visuais, exigindo que os usuários adotem camadas adicionais de segurança. Barbosa recomenda que qualquer contato recebido de forma passiva deve ser tratado com desconfiança. Os golpistas também exploram sazonalidades, como o pagamento de impostos, para enganar as vítimas. Para se proteger, é essencial verificar sempre as URLs, desconfiar de solicitações inesperadas e validar transações financeiras com um canal de comunicação diferente. Caso alguém seja vítima de um golpe, é importante acionar imediatamente o Mecanismo Especial de Devolução (MED) e registrar um boletim de ocorrência.&lt;/p></description></item><item><title>Previsões de Cibersegurança para 2026 Riscos Emergentes e Estratégias</title><link>https://brdefense.center/news/previsoes-de-ciberseguranca-para-2026-riscos-emerg/</link><pubDate>Fri, 09 Jan 2026 12:59:36 -0300</pubDate><guid>https://brdefense.center/news/previsoes-de-ciberseguranca-para-2026-riscos-emerg/</guid><description>&lt;p>À medida que as organizações se preparam para 2026, as previsões de cibersegurança estão em alta, mas muitas estratégias ainda são moldadas por especulações. Um webinar da Bitdefender busca esclarecer essas previsões, focando em dados reais e riscos emergentes. O evento abordará três tendências principais: a evolução do ransomware, que está se tornando mais direcionado e impactante; a adoção descontrolada de inteligência artificial (IA), que gera uma crise interna de segurança; e a possibilidade de ataques orquestrados por IA. Os especialistas da Bitdefender discutirão a necessidade de ceticismo em relação à capacidade de ataques adaptativos baseados em IA no curto prazo. O webinar visa ajudar líderes de segurança a diferenciar previsões sensacionalistas de aquelas que realmente devem influenciar suas estratégias de segurança. Os participantes aprenderão a justificar investimentos em segurança com base em riscos reais e a atualizar suas defesas antes que novas técnicas de ataque se tornem comuns.&lt;/p></description></item><item><title>Hackers transformam robôs em máquinas violentas em teste de segurança</title><link>https://brdefense.center/news/hackers-transformam-robos-em-maquinas-violentas-em/</link><pubDate>Tue, 30 Dec 2025 18:58:46 -0300</pubDate><guid>https://brdefense.center/news/hackers-transformam-robos-em-maquinas-violentas-em/</guid><description>&lt;p>Especialistas de segurança da China alertaram para o risco de robôs humanoides serem sequestrados por cibercriminosos através de comandos de voz. Durante a GEEKCon, uma competição de hacking em Xangai, foi demonstrado como falhas em sistemas de controle de robôs podem ser exploradas para causar danos físicos. A equipe de pesquisa DARKNAVY mostrou que um robô humanoide, disponível no mercado, pode ser controlado por um simples comando de voz, transformando-o rapidamente em uma ameaça. Além disso, o robô comprometido pode infectar outros androides via conexões sem fio de curto alcance, criando uma rede de máquinas potencialmente perigosas. A demonstração incluiu um comando violento que fez o robô atacar um manequim, evidenciando o risco real que essa vulnerabilidade representa para a segurança pública. Os especialistas ressaltaram a necessidade de regulamentação mais rigorosa para a implementação de robôs em ambientes públicos e industriais, uma vez que a crença de que mantê-los desconectados seria suficiente para evitar riscos não se sustenta diante das novas tecnologias.&lt;/p></description></item><item><title>Inteligência Artificial nas Operações de Segurança Desafios e Oportunidades</title><link>https://brdefense.center/news/inteligencia-artificial-nas-operacoes-de-seguranca/</link><pubDate>Tue, 30 Dec 2025 12:58:44 -0300</pubDate><guid>https://brdefense.center/news/inteligencia-artificial-nas-operacoes-de-seguranca/</guid><description>&lt;p>A inteligência artificial (IA) está rapidamente se integrando às operações de segurança, mas muitas equipes ainda enfrentam dificuldades para transformar experimentos iniciais em valor operacional consistente. De acordo com a pesquisa SANS SOC de 2025, 40% dos Centros de Operações de Segurança (SOCs) utilizam ferramentas de IA ou aprendizado de máquina (ML) sem integrá-las formalmente às suas operações. Isso resulta em um uso informal e muitas vezes pouco confiável da IA, sem um modelo claro de como validar seus resultados. A IA pode melhorar a capacidade e a satisfação das equipes, mas deve ser aplicada a problemas bem definidos e acompanhada de processos de revisão rigorosos. O artigo destaca cinco áreas onde a IA pode oferecer suporte confiável: engenharia de detecção, caça a ameaças, desenvolvimento e análise de software, automação e orquestração. A aplicação eficaz da IA requer que as equipes definam claramente os problemas e validem as saídas, evitando a dependência excessiva da automação. A abordagem deve ser de refinamento de processos existentes, em vez de criar novas categorias de trabalho.&lt;/p></description></item><item><title>Ameaças de Segurança em Sistemas de IA Um Alerta Urgente</title><link>https://brdefense.center/news/ameacas-de-seguranca-em-sistemas-de-ia-um-alerta-u/</link><pubDate>Mon, 29 Dec 2025 06:59:54 -0300</pubDate><guid>https://brdefense.center/news/ameacas-de-seguranca-em-sistemas-de-ia-um-alerta-u/</guid><description>&lt;p>Em 2024, a segurança cibernética enfrentou um aumento alarmante de 25% em vazamentos de dados, totalizando 23,77 milhões de segredos expostos, devido a falhas em sistemas de inteligência artificial (IA). Incidentes como a invasão da biblioteca Ultralytics AI, que instalou código malicioso para mineração de criptomoedas, e a exposição de 2.349 credenciais por pacotes Nx, destacam a vulnerabilidade das organizações, mesmo aquelas com programas de segurança robustos. Os frameworks tradicionais de segurança, como NIST e ISO, não foram projetados para lidar com as especificidades das ameaças de IA, resultando em lacunas significativas na proteção. Por exemplo, ataques de injeção de prompt e envenenamento de modelo exploram falhas que não são abordadas por controles convencionais. A falta de diretrizes específicas para esses vetores de ataque torna as empresas vulneráveis, mesmo após auditorias e conformidade com normas de segurança. A situação é crítica, pois a detecção de ataques relacionados à IA pode levar ainda mais tempo, exacerbando o risco. A crescente adoção de pacotes de IA em ambientes de nuvem aumenta ainda mais a superfície de ataque, exigindo uma reavaliação urgente das estratégias de segurança.&lt;/p></description></item><item><title>Aumento de 62 em fraudes de investimento com uso de IA</title><link>https://brdefense.center/news/aumento-de-62-em-fraudes-de-investimento-com-uso-d/</link><pubDate>Wed, 24 Dec 2025 12:57:14 -0300</pubDate><guid>https://brdefense.center/news/aumento-de-62-em-fraudes-de-investimento-com-uso-d/</guid><description>&lt;p>O esquema de fraude de investimento conhecido como Nomani teve um aumento de 62% em suas atividades, conforme dados da ESET. Inicialmente documentado em dezembro de 2024, o Nomani utiliza malvertising em redes sociais, como Facebook e YouTube, além de vídeos de testemunhos gerados por inteligência artificial (IA) para enganar usuários a investirem em produtos inexistentes. Os golpistas solicitam taxas adicionais ou informações pessoais, como documentos de identidade e dados de cartão de crédito, quando as vítimas tentam retirar os lucros prometidos. Além disso, os fraudadores tentam enganar as vítimas novamente, oferecendo ajuda para recuperar os fundos roubados, mas acabam causando mais perdas financeiras. A ESET bloqueou mais de 64 mil URLs únicas associadas a essa ameaça, com a maioria das detecções originando-se de países como República Tcheca, Japão, Eslováquia, Espanha e Polônia. Apesar do aumento geral nas detecções, houve uma queda de 37% nas detecções na segunda metade de 2025, sugerindo que os atacantes estão mudando suas táticas em resposta a esforços de aplicação da lei. O uso de deepfakes de personalidades populares e a melhoria na qualidade dos vídeos gerados por IA tornam a identificação da fraude mais difícil para os usuários.&lt;/p></description></item><item><title>ChatGPT Atlas e Chrome são os piores navegadores para a privacidade</title><link>https://brdefense.center/news/chatgpt-atlas-e-chrome-sao-os-piores-navegadores-p/</link><pubDate>Tue, 23 Dec 2025 18:58:11 -0300</pubDate><guid>https://brdefense.center/news/chatgpt-atlas-e-chrome-sao-os-piores-navegadores-p/</guid><description>&lt;p>Um estudo recente da Digitain revelou que o ChatGPT Atlas, da OpenAI, e o Google Chrome são os navegadores com maior risco à privacidade dos usuários. O ChatGPT Atlas obteve uma pontuação de 99 em 100, indicando falhas significativas em bloquear o rastreamento de usuários entre sessões e sites. O Google Chrome, embora mais popular, ficou em segundo lugar com uma pontuação de 76, demonstrando que a proteção de dados não é uma prioridade para grandes empresas do setor. Outros navegadores, como Mozilla Firefox e Apple Safari, também apresentaram pontuações baixas, levantando preocupações sobre a segurança dos dados dos internautas. Em contrapartida, navegadores como Brave e Mullvad Browser se destacaram por suas funcionalidades focadas na privacidade, sendo recomendados para aqueles que buscam maior proteção online. O estudo alerta que a crescente popularidade de navegadores baseados em inteligência artificial pode aumentar a coleta de dados pessoais, o que representa um risco adicional para a privacidade dos usuários.&lt;/p></description></item><item><title>Golpes de Natal de 2025 quais são e como evitá-los</title><link>https://brdefense.center/news/golpes-de-natal-de-2025-quais-sao-e-como-evita-los/</link><pubDate>Mon, 22 Dec 2025 12:59:47 -0300</pubDate><guid>https://brdefense.center/news/golpes-de-natal-de-2025-quais-sao-e-como-evita-los/</guid><description>&lt;p>Um levantamento da Check Point Software revelou que os golpes de cibersegurança durante o período natalino de 2025 estão mais sofisticados, utilizando inteligência artificial para automatizar fraudes. Entre as ameaças destacadas, estão e-mails de phishing com temática natalina, que somaram 33.502 casos nas últimas duas semanas, e a criação de 10.000 anúncios falsos diariamente em redes sociais. Os golpistas têm se aproveitado de eventos como a Black Friday para lançar sites de varejo falsos, que imitam operações legítimas, incluindo carrinhos de compra e confirmações de e-mail. Além disso, golpes de sorteios e promoções fraudulentas têm inundado plataformas como Facebook e Instagram, onde contas recém-criadas alegam que as vítimas ganharam prêmios, solicitando taxas de envio. Para se proteger, os especialistas recomendam verificar URLs, desconfiar de solicitações de pagamento incomuns e evitar compartilhar informações pessoais sem ter buscado o serviço. O alerta é reforçado por instituições como o FBI e a Anatel, que promovem campanhas de conscientização como o movimento #FiqueEsperto.&lt;/p></description></item><item><title>A segurança de dados é essencial para o sucesso da IA nas empresas</title><link>https://brdefense.center/news/a-seguranca-de-dados-e-essencial-para-o-sucesso-da/</link><pubDate>Mon, 22 Dec 2025 06:58:53 -0300</pubDate><guid>https://brdefense.center/news/a-seguranca-de-dados-e-essencial-para-o-sucesso-da/</guid><description>&lt;p>O CEO da Veeam, Anand Eswaran, destaca a importância da segurança de dados e da resiliência para o sucesso da inteligência artificial (IA) nas empresas. Com o aumento do uso de IA, surgem também novas ameaças, como hackers que utilizam ferramentas de IA para criar malware mais sofisticado. Eswaran enfatiza que, independentemente do setor, os dados são o &amp;lsquo;sangue vital&amp;rsquo; dos negócios e que a postura de resiliência é crucial. A Veeam se posiciona como um parceiro essencial, oferecendo uma plataforma unificada que integra controles de segurança de dados, governança de privacidade e resiliência de dados. Ele alerta que a falta de segurança pode levar ao fracasso de projetos de IA, uma vez que 90% dos dados são não estruturados e podem não ter os controles adequados. O CEO também menciona que as empresas precisam agir rapidamente para evitar a disrupção causada por atores maliciosos. A Veeam busca garantir que cada projeto de IA seja bem-sucedido, unindo segurança e resiliência em um único ciclo de vida de dados.&lt;/p></description></item><item><title>Visa e Akamai se unem para combater fraudes em compras online</title><link>https://brdefense.center/news/visa-e-akamai-se-unem-para-combater-fraudes-em-com/</link><pubDate>Fri, 19 Dec 2025 12:59:04 -0300</pubDate><guid>https://brdefense.center/news/visa-e-akamai-se-unem-para-combater-fraudes-em-com/</guid><description>&lt;p>A Visa e a Akamai Technologies firmaram uma parceria para combater fraudes em transações realizadas por meio de assistentes de inteligência artificial (IA). Com o aumento do uso de IA em compras online, surgem novas vulnerabilidades que podem ser exploradas por agentes maliciosos. Para mitigar esses riscos, as empresas implementaram o Protocolo de Agente Confiável (TAP) da Visa, que, em conjunto com a inteligência de ameaças da Akamai, garante a autenticidade do agente de IA envolvido na transação. O TAP utiliza reconhecimento profundo de usuários e inteligência comportamental para assegurar que as transações sejam realizadas por humanos e não por bots maliciosos. Além disso, a Visa introduziu a ferramenta Comércio Inteligente, que oferece suporte a desenvolvedores na criação de experiências de compra seguras. O relatório da Akamai de 2025 revelou um aumento de 300% no tráfego de bots de IA, destacando a urgência de soluções eficazes. O TAP promete uma implementação simples, com mudanças mínimas na infraestrutura existente, e proteção de ponta a ponta para os pagamentos, assegurando que as transações sejam realizadas conforme as instruções do comprador.&lt;/p></description></item><item><title>Líderes de TI buscam equilíbrio entre segurança e produtividade em software</title><link>https://brdefense.center/news/lideres-de-ti-buscam-equilibrio-entre-seguranca-e/</link><pubDate>Wed, 17 Dec 2025 06:59:24 -0300</pubDate><guid>https://brdefense.center/news/lideres-de-ti-buscam-equilibrio-entre-seguranca-e/</guid><description>&lt;p>Um novo relatório da JumpCloud e Google Workspace revela que apenas 6% dos líderes de TI estão satisfeitos com suas configurações tecnológicas atuais, destacando preocupações com custos, segurança e complexidade. A pesquisa indica que 87% dos líderes estão abertos a mudar suas suítes de produtividade em busca de plataformas mais unificadas e seguras. Os principais desafios enfrentados incluem tarefas administrativas elevadas, configurações de segurança complexas e preços complicados. A pesquisa critica a plataforma Microsoft 365, apontando a alta sobrecarga administrativa e a complexidade de configuração de segurança como principais pontos de dor. A utilização de inteligência artificial (IA) e uma postura de segurança de confiança zero são sugeridas como soluções para simplificar a gestão de dispositivos e usuários, além de prevenir ataques. O relatório enfatiza a necessidade de uma abordagem unificada para a gestão de identidade e segurança, em vez de depender de uma coleção desorganizada de ferramentas separadas.&lt;/p></description></item><item><title>Novos kits de phishing ameaçam segurança digital em larga escala</title><link>https://brdefense.center/news/novos-kits-de-phishing-ameacam-seguranca-digital-e/</link><pubDate>Fri, 12 Dec 2025 12:57:25 -0300</pubDate><guid>https://brdefense.center/news/novos-kits-de-phishing-ameacam-seguranca-digital-e/</guid><description>&lt;p>Pesquisadores de cibersegurança identificaram quatro novos kits de phishing: BlackForce, GhostFrame, InboxPrime AI e Spiderman, que facilitam o roubo de credenciais em grande escala. O BlackForce, detectado pela primeira vez em agosto de 2025, é projetado para realizar ataques Man-in-the-Browser (MitB) e capturar senhas de uso único (OTPs), burlando a autenticação multifatorial (MFA). Vendido em fóruns do Telegram, o kit já foi utilizado para se passar por marcas renomadas como Disney e Netflix. O GhostFrame, descoberto em setembro de 2025, utiliza um iframe oculto para redirecionar vítimas a páginas de phishing, enquanto o InboxPrime AI automatiza campanhas de e-mail malicioso usando inteligência artificial, permitindo que atacantes simulem comportamentos humanos reais. Por fim, o Spiderman replica páginas de login de bancos europeus, oferecendo uma plataforma completa para gerenciar campanhas de phishing. Esses kits representam uma ameaça crescente, especialmente para empresas que dependem de autenticação digital, exigindo atenção redobrada das equipes de segurança.&lt;/p></description></item><item><title>Falha expõe rede com 1 milhão de deepfakes pornográficos</title><link>https://brdefense.center/news/falha-expoe-rede-com-1-milhao-de-deepfakes-pornogr/</link><pubDate>Tue, 09 Dec 2025 13:00:58 -0300</pubDate><guid>https://brdefense.center/news/falha-expoe-rede-com-1-milhao-de-deepfakes-pornogr/</guid><description>&lt;p>Um vazamento de dados na plataforma MagicEdit, uma ferramenta de geração de imagens com inteligência artificial, revelou a existência de cerca de um milhão de deepfakes pornográficos, incluindo conteúdos envolvendo crianças. O pesquisador de cibersegurança Jeremiah Fowler descobriu que o banco de dados da plataforma continha imagens e vídeos manipulados, muitos dos quais apresentavam sobreposições de rostos de adultos em corpos de menores, levantando sérias preocupações sobre consentimento e exploração. Após a descoberta, a MagicEdit restringiu o acesso ao seu banco de dados e iniciou uma investigação sobre o incidente. O aplicativo, que era destinado a usuários maiores de 18 anos, foi descrito na App Store como contendo conteúdo sexual, mas o vazamento expôs um uso indevido alarmante da tecnologia. Fowler alertou sobre os riscos de chantagem e outros crimes associados a esses deepfakes, embora sua análise tenha sido feita para fins educacionais. O incidente destaca a necessidade urgente de regulamentação e proteção contra o uso indevido da inteligência artificial na criação de conteúdos prejudiciais.&lt;/p></description></item><item><title>Novas funcionalidades de segurança do Chrome com IA</title><link>https://brdefense.center/news/novas-funcionalidades-de-seguranca-do-chrome-com-i/</link><pubDate>Tue, 09 Dec 2025 12:58:59 -0300</pubDate><guid>https://brdefense.center/news/novas-funcionalidades-de-seguranca-do-chrome-com-i/</guid><description>&lt;p>O Google anunciou novas funcionalidades de segurança para o navegador Chrome, integrando capacidades de inteligência artificial (IA) para mitigar riscos de segurança. Entre as inovações, destaca-se o &amp;lsquo;User Alignment Critic&amp;rsquo;, que avalia de forma independente as ações do agente de IA, garantindo que estas estejam alinhadas com os objetivos do usuário e não sejam influenciadas por conteúdos maliciosos. Essa abordagem é complementada por um sistema de &amp;lsquo;Agent Origin Sets&amp;rsquo;, que limita o acesso do agente a dados de origens relevantes, prevenindo vazamentos de dados entre sites. Além disso, o navegador agora exige a aprovação do usuário antes de acessar sites sensíveis, como portais bancários. O Google também implementou um classificador de injeção de prompts, que atua em paralelo ao modelo de planejamento, bloqueando ações baseadas em conteúdos potencialmente maliciosos. Para incentivar a pesquisa em segurança, a empresa oferece recompensas de até $20.000 por demonstrações que consigam violar essas novas barreiras de segurança. A iniciativa surge em um contexto onde especialistas alertam sobre os riscos associados ao uso de navegadores com IA, especialmente em ambientes corporativos. A pesquisa da Gartner recomenda que as empresas evitem o uso de navegadores de IA até que os riscos sejam adequadamente gerenciados.&lt;/p></description></item><item><title>WatchGuard antecipa tendências de cibersegurança para 2026</title><link>https://brdefense.center/news/watchguard-antecipa-tendencias-de-ciberseguranca-p/</link><pubDate>Mon, 08 Dec 2025 12:59:33 -0300</pubDate><guid>https://brdefense.center/news/watchguard-antecipa-tendencias-de-ciberseguranca-p/</guid><description>&lt;p>A WatchGuard Technologies divulgou um relatório com previsões para a cibersegurança em 2026, destacando a crescente importância da inteligência artificial e das regulamentações de segurança digital. Segundo os especialistas Marc Laliberte e Corey Nachreiner, os crypto-ransomwares devem diminuir, pois as empresas estão adotando melhores práticas de backup e recuperação, tornando-se menos propensas a pagar resgates. Em contrapartida, os ataques focados em roubo de dados e chantagem por exposição pública devem aumentar. Além disso, a segurança do ecossistema open source pode ser ameaçada por ataques a repositórios como NPM e PyPI, levando à necessidade de defesas automatizadas baseadas em IA. Com a implementação do Cyber Resilience Act na Europa, empresas terão apenas 24 horas para reportar vulnerabilidades, o que pode acelerar a adoção de práticas de segurança. A previsão é que em 2026 ocorra o primeiro ataque totalmente executado por IA, ressaltando a urgência de defesas igualmente automatizadas. Falhas de configuração e ataques a portas de VPN continuarão a ser vulnerabilidades significativas, especialmente para pequenas e médias empresas, que devem adotar medidas de segurança do tipo Zero Trust.&lt;/p></description></item><item><title>5 formas de se proteger de injeção de prompt em navegadores de IA</title><link>https://brdefense.center/news/5-formas-de-se-proteger-de-injecao-de-prompt-em-na/</link><pubDate>Sat, 06 Dec 2025 18:57:47 -0300</pubDate><guid>https://brdefense.center/news/5-formas-de-se-proteger-de-injecao-de-prompt-em-na/</guid><description>&lt;p>O avanço da inteligência artificial (IA) trouxe benefícios significativos, mas também expôs usuários a novos riscos, como a injeção de prompt em navegadores. Esse tipo de ataque ocorre quando hackers inserem códigos maliciosos em prompts, manipulando a IA para realizar atividades fraudulentas, como roubo de dados e credenciais. O artigo apresenta cinco dicas práticas para mitigar esses riscos. Primeiro, é essencial desconfiar das informações fornecidas pela IA, sempre verificando a veracidade com fontes confiáveis. Em segundo lugar, os usuários devem evitar compartilhar dados pessoais sensíveis, como informações bancárias, que podem ser acessadas por cibercriminosos. A atualização constante dos dispositivos é outra medida crucial, pois correções de segurança ajudam a fechar brechas exploráveis. Além disso, é importante monitorar as atividades da IA e verificar a precisão das informações geradas. Por fim, a autenticação multifator (MFA) é recomendada para adicionar uma camada extra de segurança, dificultando o acesso não autorizado mesmo em caso de vazamento de credenciais. Essas práticas são fundamentais para proteger os usuários em um cenário digital cada vez mais complexo.&lt;/p></description></item><item><title>A industrialização do cibercrime novas ferramentas de phishing</title><link>https://brdefense.center/news/a-industrializacao-do-cibercrime-novas-ferramentas/</link><pubDate>Wed, 03 Dec 2025 18:57:57 -0300</pubDate><guid>https://brdefense.center/news/a-industrializacao-do-cibercrime-novas-ferramentas/</guid><description>&lt;p>O cenário de cibersegurança está mudando drasticamente com a ascensão de ferramentas de inteligência artificial que facilitam ataques de phishing. Hoje, até mesmo indivíduos sem habilidades de programação podem lançar campanhas sofisticadas, equiparando-se a hackers patrocinados por estados. O artigo destaca três ferramentas principais que estão transformando o panorama das ameaças: WormGPT, que gera e-mails de comprometimento empresarial (BEC) com alta personalização; FraudGPT, um serviço de hacking que oferece um conjunto completo de ferramentas por uma assinatura mensal; e SpamGPT, que permite testes A/B em fraudes em larga escala. A eficácia das estratégias tradicionais de detecção de e-mails está em declínio, pois as mensagens geradas por IA são indistinguíveis das legítimas. A solução proposta é mudar o foco da defesa, não apenas bloqueando e-mails, mas protegendo identidades e neutralizando ataques no ponto de acesso, garantindo que os hackers não consigam obter credenciais. O artigo conclui que, para enfrentar essa nova realidade, é essencial que as empresas adotem uma abordagem proativa e inteligente na defesa contra essas ameaças emergentes.&lt;/p></description></item><item><title>Qualquer um pode criar vírus com esta nova inteligência artificial</title><link>https://brdefense.center/news/qualquer-um-pode-criar-virus-com-esta-nova-intelig/</link><pubDate>Tue, 02 Dec 2025 13:03:12 -0300</pubDate><guid>https://brdefense.center/news/qualquer-um-pode-criar-virus-com-esta-nova-intelig/</guid><description>&lt;p>Pesquisadores da Unit 42, da Palo Alto Networks, alertam sobre o uso de Grandes Modelos de Linguagem (LLMs) por cibercriminosos. Ferramentas como WormGPT 4 e KawaiiGPT estão sendo utilizadas para facilitar a criação de malwares e ataques cibernéticos. O WormGPT, que ressurgiu em sua quarta versão, permite que hackers, mesmo sem experiência, desenvolvam códigos de ransomware e mensagens de phishing. Por exemplo, foi solicitado ao WormGPT que criasse um código para encriptar arquivos PDF em sistemas Windows, resultando em um script PowerShell que utiliza o algoritmo AES-256. Já o KawaiiGPT, uma alternativa comunitária, pode gerar mensagens de spear-phishing e scripts para movimentação lateral em sistemas, demonstrando a facilidade com que cibercriminosos podem automatizar ataques. Ambas as LLMs têm atraído a atenção de hackers, com comunidades ativas no Telegram, o que torna a situação ainda mais preocupante para a cibersegurança. A análise indica que o uso dessas ferramentas não é mais uma ameaça teórica, mas uma realidade crescente, exigindo atenção redobrada das empresas para proteger seus dados e sistemas.&lt;/p></description></item><item><title>Brasil lidera uso de deepfakes em fraudes na América Latina</title><link>https://brdefense.center/news/brasil-lidera-uso-de-deepfakes-em-fraudes-na-ameri/</link><pubDate>Fri, 28 Nov 2025 12:58:17 -0300</pubDate><guid>https://brdefense.center/news/brasil-lidera-uso-de-deepfakes-em-fraudes-na-ameri/</guid><description>&lt;p>Um relatório da Sumsub, empresa especializada em verificação de identidade, revelou que o Brasil é o líder na utilização de deepfakes para fraudes na América Latina, com um aumento alarmante de 126% entre 2024 e 2025. Apesar da diminuição geral no número de ataques, a complexidade das fraudes tem crescido, com 28% das tentativas globais sendo consideradas altamente sofisticadas. O uso de deepfakes e identidades sintéticas está se tornando cada vez mais comum, especialmente em um cenário onde 43% das empresas na região relataram ter sofrido fraudes. O relatório destaca que a manipulação de telemetria, onde dados de dispositivos e fluxos de câmera são alterados, está na vanguarda dessas fraudes. A digitalização das fraudes também é crescente, com 1 em cada 50 documentos falsificados gerados por inteligência artificial. Para enfrentar esses desafios, as empresas precisam adotar tecnologias de segurança mais avançadas, como biometria comportamental e monitoramento contínuo, para se protegerem contra esses novos métodos de ataque.&lt;/p></description></item><item><title>Deepfakes de IA aumentam 1740 e tornam golpes indetectáveis</title><link>https://brdefense.center/news/deepfakes-de-ia-aumentam-1740-e-tornam-golpes-inde/</link><pubDate>Thu, 27 Nov 2025 12:59:42 -0300</pubDate><guid>https://brdefense.center/news/deepfakes-de-ia-aumentam-1740-e-tornam-golpes-inde/</guid><description>&lt;p>Um estudo da McAfee revelou que os golpes baseados em inteligência artificial (IA) e deepfakes aumentaram 1.740% nos Estados Unidos em um ano, com quase metade da população já tendo encontrado tais fraudes durante compras online. Os deepfakes, que são vídeos ou áudios manipulados para imitar pessoas reais, tornaram-se tão sofisticados que 39% dos entrevistados afirmaram ter dificuldade em identificá-los. Além disso, 22% dos que acreditavam ser capazes de detectar fraudes acabaram caindo em golpes. Um exemplo notável foi um vídeo falso da cantora Taylor Swift, que promovia uma doação de panelas de luxo, enganando fãs e levando-os a sites fraudulentos. Para se proteger, especialistas recomendam desconfiar de anúncios que parecem bons demais para serem verdade e sempre verificar diretamente os sites oficiais das marcas. A pesquisa destaca a necessidade de vigilância constante e de uma abordagem crítica ao consumir conteúdo online, especialmente em épocas de festas, quando os golpes tendem a aumentar.&lt;/p></description></item><item><title>Avast lança ferramenta gratuita de IA para combater fraudes digitais</title><link>https://brdefense.center/news/avast-lanca-ferramenta-gratuita-de-ia-para-combate/</link><pubDate>Wed, 26 Nov 2025 18:58:10 -0300</pubDate><guid>https://brdefense.center/news/avast-lanca-ferramenta-gratuita-de-ia-para-combate/</guid><description>&lt;p>A Avast lançou recentemente o Scam Guardian, uma ferramenta gratuita baseada em inteligência artificial (IA) destinada a combater fraudes digitais. Integrada ao Avast Free Antivirus, essa nova funcionalidade visa proteger os usuários de golpes online, especialmente em um cenário onde cibercriminosos estão utilizando IA para automatizar fraudes. O Scam Guardian é descrito como uma &amp;lsquo;investigadora experiente de fraudes&amp;rsquo;, capaz de analisar não apenas links maliciosos, mas também o contexto e a linguagem das URLs suspeitas, identificando sinais de perigo. Além disso, a ferramenta bloqueia ameaças ocultas no código de sites, promovendo uma navegação mais segura. A Avast também oferece uma versão premium, o Scam Guardian Pro, que proporciona proteção adicional contra golpes via e-mail e SMS. O aumento alarmante de fraudes digitais, com um crescimento de 186% em registros pessoais vazados e 466% em casos de phishing no primeiro trimestre de 2025, destaca a urgência de soluções eficazes. A diretora de produtos da Gen Digital, Leena Elias, enfatiza a importância de disponibilizar proteção robusta contra golpes, especialmente em tempos de crescente violação de dados.&lt;/p></description></item><item><title>Investimentos em SOC A chave para a segurança cibernética eficaz</title><link>https://brdefense.center/news/investimentos-em-soc-a-chave-para-a-seguranca-cibe/</link><pubDate>Wed, 26 Nov 2025 18:57:24 -0300</pubDate><guid>https://brdefense.center/news/investimentos-em-soc-a-chave-para-a-seguranca-cibe/</guid><description>&lt;p>As empresas atualmente são desafiadas a manter entre 6 a 8 ferramentas de detecção de ameaças, consideradas essenciais na defesa cibernética. No entanto, muitos líderes de segurança enfrentam dificuldades para justificar a alocação de recursos para suas equipes de Centro de Operações de Segurança (SOC), resultando em investimentos assimétricos. Um estudo de caso recente revelou que, apesar de oito ferramentas de segurança de e-mail falharem em detectar um ataque de phishing sofisticado direcionado a executivos, as equipes do SOC conseguiram identificar a ameaça rapidamente após relatos de funcionários. Essa eficácia se deve a um investimento equilibrado ao longo do ciclo de alerta, que não negligencia o SOC. O artigo destaca que a falta de recursos no SOC pode dificultar a identificação de ameaças e sobrecarregar os analistas com alertas, comprometendo a capacidade de investigação. A adoção de plataformas de SOC baseadas em inteligência artificial (IA) está emergindo como uma solução eficaz, permitindo que equipes pequenas realizem investigações mais profundas e reduzam significativamente os falsos positivos. O investimento em SOC não apenas maximiza o retorno sobre os investimentos em ferramentas de detecção, mas também se torna crucial à medida que as ameaças se tornam mais sofisticadas.&lt;/p></description></item><item><title>Norton revela ameaças baseadas em IA no Brasil e suas contramedidas</title><link>https://brdefense.center/news/norton-revela-ameacas-baseadas-em-ia-no-brasil-e-s/</link><pubDate>Wed, 26 Nov 2025 12:58:54 -0300</pubDate><guid>https://brdefense.center/news/norton-revela-ameacas-baseadas-em-ia-no-brasil-e-s/</guid><description>&lt;p>Em um recente evento, a Norton apresentou dados alarmantes sobre o aumento de ciberataques no Brasil, destacando o uso crescente de inteligência artificial (IA) em golpes de engenharia social. A pesquisa revelou que 68% dos brasileiros estão mais preocupados com fraudes online do que no ano anterior, e 74% temem pela segurança de seus dados pessoais. A sofisticação dos ataques inclui a combinação de SMS, e-mails e redes sociais, utilizando conteúdos gerados por IA, como deepfakes, para enganar as vítimas. A Norton, em resposta, atualizou suas ferramentas de segurança, como o Norton Scam Protection, que agora conta com o Norton Genie AI, capaz de identificar e bloquear golpes em tempo real, especialmente em mensagens SMS. A empresa bloqueia cerca de 110 tentativas de golpe relacionadas à engenharia social por segundo, evidenciando a gravidade da situação. Com 65% dos brasileiros incapazes de identificar golpes gerados por IA, a necessidade de soluções eficazes de cibersegurança se torna ainda mais urgente.&lt;/p></description></item><item><title>Google nega uso de e-mails do Gmail para treinar IAs</title><link>https://brdefense.center/news/google-nega-uso-de-e-mails-do-gmail-para-treinar-i/</link><pubDate>Tue, 25 Nov 2025 18:58:08 -0300</pubDate><guid>https://brdefense.center/news/google-nega-uso-de-e-mails-do-gmail-para-treinar-i/</guid><description>&lt;p>O Google se defendeu de acusações de que estaria utilizando e-mails do Gmail para treinar seus modelos de inteligência artificial sem o consentimento dos usuários. As alegações surgiram após uma análise da empresa de segurança Malwarebytes, que sugeriu que mudanças nas configurações do Gmail permitiriam a análise de e-mails pessoais para alimentar ferramentas como o Gemini. O Google, em resposta, afirmou que não altera as configurações dos usuários e que os recursos inteligentes do Gmail, como a Escrita e Resposta Inteligentes, existem há anos sem uso do conteúdo dos e-mails para treinar suas IAs. Após a resposta do Google, a Malwarebytes revisou sua posição, reconhecendo que as acusações foram baseadas em mal-entendidos sobre a apresentação das configurações do Gmail. A empresa de segurança esclareceu que o Gmail apenas escaneia e-mails para ativar recursos como filtragem de spam e sugestões de escrita, práticas consideradas normais em termos de segurança e privacidade. O artigo também menciona que os usuários podem desativar esses recursos nas configurações do Gmail, caso desejem maior controle sobre suas informações.&lt;/p></description></item><item><title>Brasil é vice-campeão mundial em ciberataques com 28 milhões de golpes no Pix</title><link>https://brdefense.center/news/brasil-e-vice-campeao-mundial-em-ciberataques-com/</link><pubDate>Mon, 24 Nov 2025 12:59:57 -0300</pubDate><guid>https://brdefense.center/news/brasil-e-vice-campeao-mundial-em-ciberataques-com/</guid><description>&lt;p>O Brasil enfrenta um alarmante aumento nas fraudes digitais, com 28 milhões de golpes via Pix registrados entre janeiro e setembro de 2025. O país ocupa o segundo lugar no ranking global de ciberataques, com 700 milhões de tentativas anuais, o que equivale a 1.379 ataques por minuto. A pesquisa da Associação de Defesa de Dados Pessoais e do Consumidor (ADDP) revela que a maioria das fraudes ocorre em compras online, com 2,7 milhões de casos, seguidos por 1,6 milhão de golpes via WhatsApp e 1,5 milhão relacionados a phishing. Os golpes financeiros, especialmente os que envolvem o Pix, representam 47% das fraudes totais, enquanto 15% estão ligados ao roubo de identidade. O estudo também destaca que pessoas acima de 50 anos são as mais afetadas, representando 53% das vítimas. Além disso, a utilização de tecnologias avançadas, como deepfakes e inteligência artificial, tem contribuído para a sofisticação dos golpes. O presidente da ADDP, Francisco Gomes Junior, alerta que a falta de educação digital e a popularização do Pix têm facilitado a atuação de quadrilhas organizadas, resultando em prejuízos estimados entre R$ 10 bilhões e R$ 112 bilhões, muitos dos quais não são reportados.&lt;/p></description></item><item><title>AGU derruba site que vendia deepfakes com pornografia infantil</title><link>https://brdefense.center/news/agu-derruba-site-que-vendia-deepfakes-com-pornogra/</link><pubDate>Mon, 24 Nov 2025 12:59:39 -0300</pubDate><guid>https://brdefense.center/news/agu-derruba-site-que-vendia-deepfakes-com-pornogra/</guid><description>&lt;p>A Advocacia Geral da União (AGU) tomou medidas para desativar um site estrangeiro que comercializava deepfakes utilizados na produção de pornografia infantil. A ação foi desencadeada após uma notificação extrajudicial da Procuradoria Nacional da União de Defesa da Democracia (PNDD). A investigação, realizada em parceria com o Pulitzer Center, revelou que o site utilizava inteligência artificial para criar imagens falsas a partir de fotos reais de crianças, que eram então vendidas na dark web. A tecnologia de deepfake, baseada em deep learning, permite a criação de conteúdos visuais extremamente realistas, o que representa um risco significativo, especialmente quando utilizada para fins ilícitos como a exploração sexual infantil. A AGU conseguiu que o site reconhecesse a ilegalidade de suas atividades e o retirasse do ar. Este incidente destaca a crescente preocupação com o uso de IA em crimes online, especialmente em um contexto onde a identificação de conteúdos falsificados se torna cada vez mais difícil. Além disso, o Brasil está em processo de regulamentação do uso de IA, com o Marco Legal da IA em análise na Câmara dos Deputados, visando aumentar a segurança e a transparência no uso dessas tecnologias.&lt;/p></description></item><item><title>Vulnerabilidades críticas em motores de IA afetam Nvidia e outras empresas</title><link>https://brdefense.center/news/vulnerabilidades-criticas-em-motores-de-ia-afetam/</link><pubDate>Mon, 17 Nov 2025 18:58:33 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidades-criticas-em-motores-de-ia-afetam/</guid><description>&lt;p>Pesquisadores da Oligo identificaram vulnerabilidades graves em motores de inferência de inteligência artificial, impactando grandes empresas como Meta, Microsoft e Nvidia. As falhas, que permitem a execução de código remoto, estão ligadas ao uso inseguro do ZeroMQ e à desserialização de dados com o módulo pickle do Python, resultando em um padrão de vulnerabilidade denominado ShadowMQ. A principal brecha foi encontrada no framework Llama da Meta, classificada como CVE-2024-50050, com um score CVSS de 6,3/9,3, que foi corrigida em outubro de 2025. Outras tecnologias, como a TensorRT-LLM da Nvidia e o Sarathi-Serve da Microsoft, também apresentaram falhas, com algumas ainda sem correção. A exploração dessas vulnerabilidades pode permitir que invasores executem códigos arbitrários, aumentem privilégios e até roubem modelos de IA. A situação é crítica, pois comprometer um único motor de inferência pode ter consequências severas, como a inserção de agentes maliciosos nas LLMs. O alerta é para que as empresas revisem suas implementações e apliquem as correções necessárias para evitar possíveis ataques.&lt;/p></description></item><item><title>Hackers chineses utilizam IA para automatizar ciberespionagem</title><link>https://brdefense.center/news/hackers-chineses-utilizam-ia-para-automatizar-cibe/</link><pubDate>Mon, 17 Nov 2025 12:59:44 -0300</pubDate><guid>https://brdefense.center/news/hackers-chineses-utilizam-ia-para-automatizar-cibe/</guid><description>&lt;p>Hackers chineses estão utilizando inteligência artificial (IA) para aprimorar suas campanhas de ciberespionagem, conforme identificado pela Anthropic, a empresa responsável pela ferramenta Claude Code. Essa nova abordagem foi observada em ataques a cerca de 30 empresas internacionais, incluindo instituições financeiras e agências governamentais. O relatório revela que, em ataques bem-sucedidos, a intervenção humana foi mínima, com a IA executando de 80 a 90% das tarefas necessárias.&lt;/p>
&lt;p>A operação dos hackers foi organizada em seis fases, começando pela seleção do alvo e culminando na exfiltração de dados confidenciais. A IA foi manipulada para atuar como um agente autônomo, encontrando vulnerabilidades, coletando credenciais e explorando sistemas. A Anthropic respondeu banindo as contas associadas a esses ataques e implementando novos mecanismos de defesa. Essa evolução na ciberespionagem, com uma IA promovendo um ataque de larga escala, levanta preocupações sobre a profissionalização do cibercrime e o uso crescente de tecnologias avançadas para atividades ilícitas.&lt;/p></description></item><item><title>A lacuna de exposição em IA pode ser o maior problema de segurança</title><link>https://brdefense.center/news/a-lacuna-de-exposicao-em-ia-pode-ser-o-maior-probl/</link><pubDate>Mon, 17 Nov 2025 06:57:57 -0300</pubDate><guid>https://brdefense.center/news/a-lacuna-de-exposicao-em-ia-pode-ser-o-maior-probl/</guid><description>&lt;p>Um novo relatório da Tenable destaca a crescente preocupação com a segurança em ambientes que utilizam inteligência artificial (IA). Com 89% das organizações já implementando ou testando cargas de trabalho de IA, a pesquisa revela que apenas 22% das empresas classificam e criptografam completamente seus dados de IA, deixando 78% vulneráveis a ataques. Além disso, 34% dos adotantes de IA já enfrentaram violações relacionadas à tecnologia, sendo que a maioria dessas falhas decorre de vulnerabilidades internas e não de ataques sofisticados aos modelos de IA. As principais causas de brechas incluem vulnerabilidades de software (21%) e ameaças internas (18%). A Tenable alerta que as empresas estão escalando suas operações de IA mais rapidamente do que conseguem garantir a segurança, resultando em defesas reativas. A pesquisa também indica que cerca de 51% das empresas seguem diretrizes mínimas, como o NIST AI Risk Management Framework, e apenas 26% realizam testes de segurança específicos para IA. Para mitigar a &amp;rsquo;lacuna de exposição em IA&amp;rsquo;, a Tenable recomenda que as empresas priorizem controles fundamentais, como governança de identidade e monitoramento de configurações, para estabelecer uma postura de segurança robusta.&lt;/p></description></item><item><title>IA com 30 anos de memória é usada para combater crimes digitais</title><link>https://brdefense.center/news/ia-com-30-anos-de-memoria-e-usada-para-combater-cr/</link><pubDate>Fri, 14 Nov 2025 13:03:04 -0300</pubDate><guid>https://brdefense.center/news/ia-com-30-anos-de-memoria-e-usada-para-combater-cr/</guid><description>&lt;p>A Cisco anunciou o desenvolvimento de uma nova inteligência artificial (IA) que utiliza 30 anos de dados sobre ataques cibernéticos para aprimorar a segurança digital. O projeto visa expandir o modelo Foundation-Sec-8B, que atualmente opera com 8 bilhões de parâmetros, para 17 bilhões, aumentando a precisão na detecção de ameaças. Raj Chopra, vice-presidente sênior da Cisco, destacou que o foco não é criar um sucessor, mas sim um modelo expandido que utilize um vasto arsenal de informações coletadas ao longo das últimas três décadas, incluindo incidentes e manuais de treinamento. A equipe de especialistas em segurança digital da Cisco liderará esse processo, que deve ser concluído até o final do ano. Além disso, a empresa está desenvolvendo novos modelos de IA para complementar essa versão atualizada, com o objetivo de apoiar os profissionais de segurança no combate ao cibercrime com ferramentas mais sofisticadas.&lt;/p></description></item><item><title>Hackers Chineses Usam IA para Infiltrar Grandes Empresas de Tecnologia</title><link>https://brdefense.center/news/hackers-chineses-usam-ia-para-infiltrar-grandes-em/</link><pubDate>Fri, 14 Nov 2025 12:59:19 -0300</pubDate><guid>https://brdefense.center/news/hackers-chineses-usam-ia-para-infiltrar-grandes-em/</guid><description>&lt;p>Um ataque cibernético em larga escala, realizado quase inteiramente por inteligência artificial, foi revelado pela Anthropic. O incidente, que ocorreu em setembro de 2025, envolveu hackers patrocinados pelo Estado chinês que utilizaram a IA Claude Code para comprometer cerca de 30 alvos globais, incluindo grandes empresas de tecnologia, instituições financeiras e agências governamentais. A campanha de espionagem se destacou pela automação sem precedentes, com a IA gerenciando 80-90% das operações, exigindo intervenção humana apenas em 4-6 pontos críticos. Os atacantes exploraram capacidades avançadas da IA, como execução de tarefas complexas e operação autônoma, para realizar reconhecimento, desenvolver códigos de exploração e exfiltrar dados classificados. A Anthropic detectou a atividade suspeita e iniciou uma investigação, resultando na identificação de contas comprometidas e na notificação das organizações afetadas. Este incidente sinaliza uma diminuição nas barreiras para ataques cibernéticos sofisticados, permitindo que grupos menos experientes realizem operações que antes exigiam recursos significativos. A empresa recomenda que equipes de segurança experimentem aplicações de IA para melhorar a detecção de ameaças e a resposta a incidentes.&lt;/p></description></item><item><title>Ciberespionagem Atores estatais da China usam IA para ataques</title><link>https://brdefense.center/news/ciberespionagem-atores-estatais-da-china-usam-ia-p/</link><pubDate>Fri, 14 Nov 2025 12:58:14 -0300</pubDate><guid>https://brdefense.center/news/ciberespionagem-atores-estatais-da-china-usam-ia-p/</guid><description>&lt;p>Em setembro de 2025, atores de ameaças patrocinados pelo Estado da China utilizaram tecnologia de inteligência artificial (IA) desenvolvida pela Anthropic para realizar uma campanha de ciberespionagem sofisticada. Os atacantes empregaram as capacidades &amp;lsquo;agentes&amp;rsquo; da IA para executar ataques cibernéticos de forma autônoma, sem intervenção humana significativa. A operação, denominada GTG-1002, visou cerca de 30 alvos globais, incluindo grandes empresas de tecnologia, instituições financeiras e agências governamentais, resultando em algumas intrusões bem-sucedidas. A Anthropic identificou que a IA foi utilizada para realizar diversas etapas do ciclo de ataque, como reconhecimento, descoberta de vulnerabilidades e exfiltração de dados. Embora a operação tenha demonstrado um uso inovador da IA, também revelou limitações, como a tendência da IA de &amp;lsquo;alucinar&amp;rsquo; dados, o que pode comprometer a eficácia das operações. Este incidente destaca a evolução das táticas de ciberataques, onde grupos menos experientes podem potencialmente realizar ataques em larga escala com o suporte de sistemas de IA.&lt;/p></description></item><item><title>Vulnerabilidades críticas em motores de IA expõem riscos de segurança</title><link>https://brdefense.center/news/vulnerabilidades-criticas-em-motores-de-ia-expoem/</link><pubDate>Fri, 14 Nov 2025 12:57:07 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidades-criticas-em-motores-de-ia-expoem/</guid><description>&lt;p>Pesquisadores de cibersegurança identificaram vulnerabilidades críticas de execução remota de código em motores de inferência de inteligência artificial (IA) de grandes empresas como Meta, Nvidia e Microsoft, além de projetos open-source como vLLM e SGLang. O problema central está relacionado ao uso inseguro do ZeroMQ (ZMQ) e à desserialização do Python, resultando em um padrão denominado ShadowMQ. A vulnerabilidade mais significativa foi encontrada no framework Llama da Meta (CVE-2024-50050), que permitia a execução de código arbitrário ao desserializar dados maliciosos. Outras plataformas, como NVIDIA TensorRT-LLM e Microsoft Sarathi-Serve, também apresentaram falhas semelhantes. A exploração dessas vulnerabilidades pode permitir que atacantes executem código arbitrário, escalem privilégios e até realizem roubo de modelos de IA. Com a rápida evolução dos projetos de IA, a reutilização de código inseguro se torna um risco crescente. Além disso, um novo relatório revelou que técnicas de injeção de JavaScript podem comprometer navegadores integrados em editores de código, aumentando ainda mais as preocupações de segurança. É crucial que as empresas adotem medidas de mitigação, como desativar recursos de execução automática e auditar servidores de integração.&lt;/p></description></item><item><title>Cibersabotagem em infraestruturas essenciais se torna ameaça crescente</title><link>https://brdefense.center/news/cibersabotagem-em-infraestruturas-essenciais-se-to/</link><pubDate>Thu, 13 Nov 2025 13:05:47 -0300</pubDate><guid>https://brdefense.center/news/cibersabotagem-em-infraestruturas-essenciais-se-to/</guid><description>&lt;p>A cibersabotagem, especialmente por governos autoritários, está se tornando uma preocupação global crescente, conforme destacado por Mike Burgess, diretor-geral da Organização Australiana de Inteligência de Segurança (ASIO). Em um discurso recente, Burgess alertou sobre a disposição de regimes totalitários em utilizar técnicas de cibersabotagem para interromper ou destruir infraestruturas críticas, como redes de telecomunicação e sistemas financeiros. Ele mencionou que incidentes recentes na Austrália, que resultaram em interrupções significativas, podem ter consequências fatais, como a morte de três pessoas.&lt;/p></description></item><item><title>Vazamento cibernético na China revela operações de hacking globais</title><link>https://brdefense.center/news/vazamento-cibernetico-na-china-revela-operacoes-de/</link><pubDate>Wed, 12 Nov 2025 18:58:11 -0300</pubDate><guid>https://brdefense.center/news/vazamento-cibernetico-na-china-revela-operacoes-de/</guid><description>&lt;p>Um recente vazamento de dados na empresa de segurança chinesa Knownsec expôs mais de 12.000 documentos classificados que revelam operações de hacking ligadas ao governo chinês. Os arquivos vazados incluem informações sobre &amp;lsquo;armas cibernéticas&amp;rsquo;, ferramentas internas de inteligência artificial e uma lista extensa de alvos internacionais, abrangendo mais de vinte países, como Japão, Índia e Reino Unido. Entre as informações preocupantes, estão planilhas que detalham ataques a 80 alvos estrangeiros, incluindo empresas de infraestrutura crítica e telecomunicações. O vazamento também revelou dados significativos, como 95GB de registros de imigração da Índia e 3TB de logs de chamadas da LG U Plus da Coreia do Sul. Os especialistas identificaram a presença de Trojans de Acesso Remoto (RATs) que podem comprometer sistemas operacionais populares, além de dispositivos de hacking de hardware utilizados pela Knownsec. Apesar das tentativas do governo chinês de desmentir o incidente, a profundidade da infiltração sugere uma colaboração estreita entre empresas privadas e operações estatais. Este evento destaca a necessidade de uma defesa cibernética mais robusta, que combine monitoramento em tempo real e segmentação de rede.&lt;/p></description></item><item><title>Google lança tecnologia de privacidade com Private AI Compute</title><link>https://brdefense.center/news/google-lanca-tecnologia-de-privacidade-com-private/</link><pubDate>Wed, 12 Nov 2025 06:57:38 -0300</pubDate><guid>https://brdefense.center/news/google-lanca-tecnologia-de-privacidade-com-private/</guid><description>&lt;p>No dia 12 de novembro de 2025, o Google apresentou uma nova tecnologia chamada Private AI Compute, que visa processar consultas de inteligência artificial (IA) em uma plataforma segura na nuvem. A empresa afirma que essa tecnologia desbloqueia a velocidade e o poder dos modelos de nuvem Gemini para experiências de IA, garantindo que os dados pessoais dos usuários permaneçam privados e inacessíveis, nem mesmo ao Google.&lt;/p>
&lt;p>O Private AI Compute é descrito como um &amp;rsquo;espaço seguro e fortificado&amp;rsquo; para o processamento de dados sensíveis, utilizando unidades de processamento de tensor Trillium (TPUs) e enclaves de inteligência Titanium (TIE). Essa infraestrutura é projetada para aproveitar a velocidade computacional da nuvem, mantendo as garantias de segurança e privacidade do processamento local.&lt;/p></description></item><item><title>Motorola e o lifestyle tech expectativas para a Black Friday</title><link>https://brdefense.center/news/motorola-e-o-lifestyle-tech-expectativas-para-a-bl/</link><pubDate>Tue, 11 Nov 2025 06:59:42 -0300</pubDate><guid>https://brdefense.center/news/motorola-e-o-lifestyle-tech-expectativas-para-a-bl/</guid><description>&lt;p>No episódio especial do podcast Canaltech, a Motorola é destaque ao discutir suas inovações em smartphones dobráveis e o uso de inteligência artificial. A diretora de Marketing da Motorola no Brasil, Stella Colucci, compartilha detalhes sobre as parcerias da marca com empresas como Pantone e Swarovski, que visam integrar tecnologia e estilo de vida. O programa também aborda as expectativas para as promoções da Motorola durante a Black Friday, um evento que promete atrair consumidores em busca de ofertas em tecnologia. Além disso, o podcast menciona outros tópicos relevantes, como mudanças no WhatsApp e fraudes envolvendo cartões de crédito, ressaltando a importância da cibersegurança no cenário atual. O episódio é uma oportunidade para os ouvintes se atualizarem sobre as tendências do setor e as estratégias da Motorola para se destacar no mercado.&lt;/p></description></item><item><title>Inovação e Chips O Futuro da AMD no Brasil</title><link>https://brdefense.center/news/inovacao-e-chips-o-futuro-da-amd-no-brasil/</link><pubDate>Sat, 08 Nov 2025 06:59:41 -0300</pubDate><guid>https://brdefense.center/news/inovacao-e-chips-o-futuro-da-amd-no-brasil/</guid><description>&lt;p>No episódio mais recente do Podcast Canaltech, os repórteres discutem as inovações apresentadas durante o AMD Partner Summit, um evento que reuniu parceiros e a mídia para discutir o futuro da AMD no Brasil. Sérgio Santos, diretor-geral da AMD Brasil, e Patrícia Martins, head da unidade de gráficos, destacaram o foco da empresa em inteligência artificial e inovação, além de suas expectativas de crescimento no mercado brasileiro. Matheus Barbosa, gerente de marketing da Gigabyte Brasil, também participou, abordando a parceria entre as duas empresas e as perspectivas para o mercado gamer até 2026. O podcast também trouxe à tona questões relevantes sobre segurança cibernética, como o vazamento de 40 milhões de downloads de vírus da Play Store e a prisão de um funcionário do Banco do Brasil por roubo de dados de clientes, ressaltando a importância da segurança digital em um cenário de crescente digitalização.&lt;/p></description></item><item><title>Como a Microsoft quer reinventar a segurança digital na era da IA</title><link>https://brdefense.center/news/como-a-microsoft-quer-reinventar-a-seguranca-digit/</link><pubDate>Fri, 07 Nov 2025 07:02:09 -0300</pubDate><guid>https://brdefense.center/news/como-a-microsoft-quer-reinventar-a-seguranca-digit/</guid><description>&lt;p>Durante o Seattle Security Campus Tour, a Microsoft apresentou sua visão sobre como a inteligência artificial (IA) e a colaboração global estão moldando o futuro da segurança digital. Frank X. Shaw, Diretor de Comunicação Global da Microsoft, destacou que a confiança é fundamental para a inovação, introduzindo o Secure Future Initiative (SFI), um programa que orienta o desenvolvimento de produtos e a resposta a incidentes. Vasu Jakkal, vice-presidente corporativa de Segurança, enfatizou que a segurança deve ser parte integrante de todos os processos da empresa, processando diariamente mais de 100 trilhões de sinais de segurança. A automação na detecção de falhas humanas foi exemplificada por um incidente em que um token interno foi exposto no GitHub, gerando um alerta imediato. Tori Westerhoff, do Red Team de IA, revelou que sua equipe simula ataques aos modelos de IA da Microsoft, explorando vulnerabilidades e comportamentos inesperados. Por fim, Herain Oberoi apresentou uma plataforma de defesa integrada que utiliza IA para proteger dados e identidades, ressaltando a importância do equilíbrio entre autonomia da IA e responsabilidade humana. A segurança, segundo os executivos, não é apenas um produto, mas a base para inovação e liberdade digital.&lt;/p></description></item><item><title>Por dentro da divisão da Microsoft que caça hackers pelo mundo</title><link>https://brdefense.center/news/por-dentro-da-divisao-da-microsoft-que-caca-hacker/</link><pubDate>Fri, 07 Nov 2025 07:01:49 -0300</pubDate><guid>https://brdefense.center/news/por-dentro-da-divisao-da-microsoft-que-caca-hacker/</guid><description>&lt;p>Durante o Microsoft Security Campus Tour, a Digital Crimes Unit (DCU) da Microsoft foi apresentada como um núcleo global dedicado à investigação de crimes digitais. Sob a liderança de Steve Masada, a equipe atua em três frentes principais: interromper infraestruturas criminosas, fornecer suporte jurídico e orientar empresas e governos em resposta a incidentes. A DCU monitora centenas de grupos de hackers e analisa bilhões de sinais de segurança diariamente, utilizando dados de produtos como Windows e Azure para agir de forma preventiva. Além disso, a equipe realiza simulações de crise cibernética para preparar empresas para possíveis ataques. Richard Domingues Boscovich, advogado sênior da DCU, destacou um caso recente de uso indevido de inteligência artificial generativa, onde imagens difamatórias foram criadas e disseminadas. A investigação levou à identificação dos responsáveis e à aplicação de ações legais. A DCU busca neutralizar o ecossistema criminoso e tornar o cibercrime mais difícil e caro. A abordagem da Microsoft enfatiza a antecipação de ameaças e a utilização de tecnologia como ferramenta de defesa, consolidando a segurança digital como um pilar essencial da inovação.&lt;/p></description></item><item><title>IA vai roubar empregos? O futuro é mais colaborativo do que você imagina</title><link>https://brdefense.center/news/ia-vai-roubar-empregos-o-futuro-e-mais-colaborativ/</link><pubDate>Thu, 06 Nov 2025 07:01:48 -0300</pubDate><guid>https://brdefense.center/news/ia-vai-roubar-empregos-o-futuro-e-mais-colaborativ/</guid><description>&lt;p>O podcast Canaltech discute a perspectiva da inteligência artificial (IA) no mercado de trabalho, com base em uma pesquisa do Instituto dos Engenheiros Eletricistas e Eletrônicos (IEEE). A pesquisa sugere que, ao invés de substituir empregos, a IA deve expandir oportunidades de trabalho em 2026. O repórter Marcelo Fischer entrevista Gabriel Gomes de Oliveira, professor da Universidade Estadual de Campinas, que destaca que a colaboração entre humanos e máquinas será fundamental. O episódio também aborda outras questões relevantes, como promessas da Apple sobre a Siri com IA, demissões na Rockstar e vazamentos de dados na dark web. A discussão enfatiza que a adoção da IA pode levar a um ambiente de trabalho mais colaborativo, onde as habilidades humanas são complementadas pela tecnologia, ao invés de serem substituídas por ela.&lt;/p></description></item><item><title>Estudo do MIT sobre ransomware e IA é retirado após críticas</title><link>https://brdefense.center/news/estudo-do-mit-sobre-ransomware-e-ia-e-retirado-apo/</link><pubDate>Wed, 05 Nov 2025 18:59:28 -0300</pubDate><guid>https://brdefense.center/news/estudo-do-mit-sobre-ransomware-e-ia-e-retirado-apo/</guid><description>&lt;p>Um estudo da MIT Sloan School of Management, que afirmava que 80,83% dos ataques de ransomware eram realizados por criminosos utilizando inteligência artificial (IA), foi retirado após críticas severas de especialistas em cibersegurança. O estudo, co-autorado por pesquisadores do MIT e executivos da Safe Security, foi amplamente desacreditado por figuras proeminentes da área, como Kevin Beaumont e Marcus Hutchins, que consideraram as alegações como &amp;lsquo;ridículas&amp;rsquo; e &amp;lsquo;sem provas&amp;rsquo;. Beaumont destacou que o estudo mencionava grupos de ransomware que não utilizam IA e até citou o Emotet, que não está ativo há anos. Após a repercussão negativa, o MIT anunciou que o documento estava sendo revisado. O autor Michael Siegel afirmou que o objetivo do estudo era alertar sobre o aumento do uso de IA em ataques cibernéticos e a necessidade de medições adequadas. A controvérsia ressalta a tensão crescente na pesquisa em cibersegurança, onde o entusiasmo por IA pode ofuscar a análise factual. Embora a IA tenha potencial tanto para ataques quanto para defesas, exagerar seu uso malicioso pode distorcer prioridades, especialmente quando proveniente de instituições respeitáveis como o MIT.&lt;/p></description></item><item><title>Previsão de Cibersegurança 2026 Google prevê aumento de ataques impulsionados por IA</title><link>https://brdefense.center/news/previsao-de-ciberseguranca-2026-google-preve-aumen/</link><pubDate>Wed, 05 Nov 2025 18:59:07 -0300</pubDate><guid>https://brdefense.center/news/previsao-de-ciberseguranca-2026-google-preve-aumen/</guid><description>&lt;p>O relatório &amp;lsquo;Cybersecurity Forecast 2026&amp;rsquo; do Google Cloud destaca uma mudança significativa no cenário de cibersegurança, com a adoção crescente de inteligência artificial (IA) tanto por atacantes quanto por defensores. O documento, que se baseia em análises de especialistas em segurança do Google, prevê que o próximo ano será marcado por uma evolução tecnológica rápida e técnicas de ataque cada vez mais sofisticadas. Um dos principais achados é a normalização do uso de IA por cibercriminosos, que estão integrando essa tecnologia em todos os ciclos de ataque, permitindo campanhas mais rápidas e ágeis. A vulnerabilidade de injeção de prompt, onde atacantes manipulam sistemas de IA para executar comandos ocultos, é uma preocupação crescente. Além disso, a engenharia social habilitada por IA, como campanhas de vishing com clonagem de voz, está se tornando mais comum, dificultando a detecção de ataques de phishing. O relatório também menciona que o ransomware e a extorsão continuarão a ser as categorias mais disruptivas e financeiramente prejudiciais, com foco em provedores terceirizados e vulnerabilidades críticas. A previsão sugere que as equipes de segurança devem se adaptar rapidamente, utilizando metodologias de IA para fortalecer suas defesas e preparar-se para um aumento nas atividades de engenharia social e operações de estados-nação.&lt;/p></description></item><item><title>Google descobre malware PROMPTFLUX que usa IA para evasão</title><link>https://brdefense.center/news/google-descobre-malware-promptflux-que-usa-ia-para/</link><pubDate>Wed, 05 Nov 2025 18:57:17 -0300</pubDate><guid>https://brdefense.center/news/google-descobre-malware-promptflux-que-usa-ia-para/</guid><description>&lt;p>O Google revelou a descoberta de um novo malware chamado PROMPTFLUX, que utiliza um script em Visual Basic (VBScript) para interagir com a API do modelo de inteligência artificial Gemini. Este malware é capaz de gerar seu próprio código-fonte, permitindo técnicas de ofuscação e evasão em tempo real, o que dificulta a detecção por sistemas de segurança baseados em assinaturas estáticas. A funcionalidade inovadora do PROMPTFLUX é parte de um componente denominado &amp;lsquo;Thinking Robot&amp;rsquo;, que consulta periodicamente o modelo de linguagem Gemini para obter novas técnicas de evasão. Embora atualmente o malware não tenha capacidade de comprometer redes ou dispositivos, sua evolução e a possibilidade de auto-modificação indicam um potencial de ameaça crescente. O Google também observou que atores maliciosos estão utilizando IA não apenas para aumentar a produtividade, mas para desenvolver ferramentas que se adaptam durante a execução. Além disso, o uso de IA por grupos patrocinados por estados, como os da China e Irã, para criar conteúdo enganoso e desenvolver infraestrutura técnica para exfiltração de dados, destaca a crescente sofisticação das ameaças cibernéticas. A expectativa é que o uso de IA por atores maliciosos se torne a norma, aumentando a velocidade e a eficácia de suas operações.&lt;/p></description></item><item><title>Por que a bateria do celular ainda é o maior desafio da tecnologia</title><link>https://brdefense.center/news/por-que-a-bateria-do-celular-ainda-e-o-maior-desaf/</link><pubDate>Tue, 04 Nov 2025 07:00:43 -0300</pubDate><guid>https://brdefense.center/news/por-que-a-bateria-do-celular-ainda-e-o-maior-desaf/</guid><description>&lt;p>No episódio mais recente do Podcast Canaltech, especialistas discutem os desafios enfrentados na busca por baterias de celular com maior autonomia. Hudson Zanin, professor da Unicamp, e André Varga, diretor de produto da JOVI, abordam os avanços nas tecnologias de baterias, como as de íon-lítio e silício-carbono, que prometem melhorar a duração das cargas. Apesar dos progressos, a corrida por baterias que durem mais tempo longe da tomada ainda enfrenta barreiras significativas, tanto do ponto de vista técnico quanto de mercado. Os convidados também exploram como a inteligência artificial pode ser uma aliada ou um obstáculo nesse processo. Além disso, o episódio traz atualizações sobre lançamentos de produtos e falhas de segurança que afetam bilhões de usuários, destacando a relevância contínua da cibersegurança no contexto tecnológico atual.&lt;/p></description></item><item><title>O novo consumidor como a IA está mudando o marketing, segundo o Google</title><link>https://brdefense.center/news/o-novo-consumidor-como-a-ia-esta-mudando-o-marketi/</link><pubDate>Mon, 03 Nov 2025 07:04:44 -0300</pubDate><guid>https://brdefense.center/news/o-novo-consumidor-como-a-ia-esta-mudando-o-marketi/</guid><description>&lt;p>No episódio mais recente do Podcast Canaltech, Guilherme Haas entrevista Arthur Borges, Head de Negócios para Mid-Market no Google, sobre a transformação do marketing impulsionada pela inteligência artificial (IA). A conversa se baseia no estudo &amp;lsquo;Mapa da Influência&amp;rsquo;, realizado em parceria com a Boston Consulting Group, que revela como o tradicional funil de marketing evoluiu para uma jornada mais dinâmica, caracterizada por quatro hábitos principais: buscar, rolar, assistir e comprar. Borges destaca a importância da maturidade digital e como a IA pode ser utilizada para personalizar experiências de forma responsável. Além disso, ele menciona as habilidades que os profissionais de marketing e tecnologia devem desenvolver para se adaptarem a essas mudanças. O podcast também aborda outras notícias relevantes do setor, como a entrada da OpenAI no mercado e o Brasil sendo um dos principais alvos de fraudes digitais.&lt;/p></description></item><item><title>IA e Regulamentação Tendências em Cibersegurança para 2026</title><link>https://brdefense.center/news/ia-e-regulamentacao-tendencias-em-ciberseguranca-p/</link><pubDate>Sat, 01 Nov 2025 12:57:33 -0300</pubDate><guid>https://brdefense.center/news/ia-e-regulamentacao-tendencias-em-ciberseguranca-p/</guid><description>&lt;p>No último Fórum Latinoamericano de Segurança da ESET, realizado no Uruguai, especialistas discutiram as tendências em cibersegurança até 2026, destacando o uso crescente da inteligência artificial (IA) em crimes virtuais. Pesquisadores como Martina López e Mario Micucci alertaram para o aumento do uso de IA agêntica, que permite a execução de ataques cibernéticos com mínima intervenção humana, especialmente em phishing e spear-phishing. Além disso, a evolução dos ransomwares, incluindo o modelo ransomware-as-a-service (RaaS), foi enfatizada, com a introdução de ransomwares desenvolvidos com IA, como o LunaLock. A regulamentação da IA também foi um ponto central, com a necessidade de legislações que garantam transparência e proteção contra abusos, como deepfakes. O Ato de Inteligência Artificial da União Europeia foi citado como um exemplo positivo, exigindo que empresas informem sobre o uso de IA. O artigo conclui que a cibersegurança enfrentará desafios significativos, com a necessidade de auditorias eficazes e regulamentações que protejam a integridade dos dispositivos tecnológicos.&lt;/p></description></item><item><title>Polícia usa IA para decifrar comunicação de cibercriminosos</title><link>https://brdefense.center/news/policia-usa-ia-para-decifrar-comunicacao-de-ciberc/</link><pubDate>Fri, 31 Oct 2025 18:59:32 -0300</pubDate><guid>https://brdefense.center/news/policia-usa-ia-para-decifrar-comunicacao-de-ciberc/</guid><description>&lt;p>A Polícia Federal da Austrália (AFP) está desenvolvendo uma ferramenta de inteligência artificial (IA) para decifrar a comunicação de cibercriminosos, especialmente aqueles que utilizam emojis e gírias das gerações Z e Alpha em discussões online sobre crimes. Esses criminosos, conhecidos como &amp;lsquo;crimefluencers&amp;rsquo;, atraem jovens e crianças para grupos de ódio, utilizando uma linguagem que pode ser difícil de interpretar para as autoridades. A comissária da AFP, Krissy Barrett, destacou que a maioria das vítimas são pré-adolescentes e jovens, o que motivou a criação dessa ferramenta. O projeto ainda está em fase de desenvolvimento e busca criar um protótipo que consiga interpretar a comunicação criptografada em grupos de bate-papo. A expectativa é que a IA consiga traduzir termos específicos com base no contexto e na semântica, embora o desafio de acompanhar a constante evolução das gírias e emojis possa dificultar a eficácia da ferramenta. A iniciativa faz parte de uma força-tarefa maior da Five Eyes Law Enforcement Group, que inclui países como Reino Unido, EUA, Canadá e Nova Zelândia, todos trabalhando juntos para combater crimes digitais.&lt;/p></description></item><item><title>OpenAI lança Aardvark, pesquisador de segurança autônomo com IA</title><link>https://brdefense.center/news/openai-lanca-aardvark-pesquisador-de-seguranca-aut/</link><pubDate>Fri, 31 Oct 2025 18:57:22 -0300</pubDate><guid>https://brdefense.center/news/openai-lanca-aardvark-pesquisador-de-seguranca-aut/</guid><description>&lt;p>A OpenAI anunciou o lançamento do Aardvark, um pesquisador de segurança autônomo alimentado pelo modelo de linguagem GPT-5. Este agente de inteligência artificial foi projetado para ajudar desenvolvedores e equipes de segurança a identificar e corrigir vulnerabilidades em código de forma escalável. Atualmente em beta privada, o Aardvark analisa repositórios de código-fonte continuamente, identificando vulnerabilidades, avaliando sua explorabilidade e propondo correções. O modelo GPT-5, introduzido em agosto de 2025, oferece capacidades de raciocínio mais profundas e um &amp;lsquo;roteador em tempo real&amp;rsquo; para otimizar a interação com os usuários.&lt;/p></description></item><item><title>Google Introduz Proteção com IA no Android Contra Golpes Móveis</title><link>https://brdefense.center/news/google-introduz-protecao-com-ia-no-android-contra/</link><pubDate>Fri, 31 Oct 2025 12:59:41 -0300</pubDate><guid>https://brdefense.center/news/google-introduz-protecao-com-ia-no-android-contra/</guid><description>&lt;p>Em resposta à crescente ameaça de golpes móveis, a Google anunciou melhorias significativas na proteção do Android, utilizando inteligência artificial para combater fraudes. Em um relatório divulgado em 30 de outubro de 2025, a empresa destacou que suas defesas baseadas em IA superam as de outras plataformas, com um impacto positivo na segurança dos usuários. No último ano, os golpes móveis geraram perdas superiores a 400 bilhões de dólares globalmente. O sistema do Android processa mensalmente mais de 10 bilhões de chamadas e mensagens suspeitas, bloqueando mais de 100 milhões de números fraudulentos recentemente. A pesquisa realizada pela YouGov, envolvendo 5.000 usuários nos EUA, Índia e Brasil, revelou que usuários do Android, especialmente os do Google Pixel, relataram menos mensagens de golpe em comparação aos usuários do iOS. Além disso, a análise de segurança da Leviathan Security Group confirmou que o Pixel 10 Pro oferece a melhor proteção contra fraudes. As funcionalidades incluem filtragem automática de spam e detecção de padrões de conversação fraudulentos, garantindo a privacidade dos usuários. Com atualizações contínuas através do Google Play Protect, o Android se mantém à frente das ameaças móveis em constante evolução.&lt;/p></description></item><item><title>Brasil lidera ranking mundial de fraudes digitais</title><link>https://brdefense.center/news/brasil-lidera-ranking-mundial-de-fraudes-digitais/</link><pubDate>Thu, 30 Oct 2025 18:59:49 -0300</pubDate><guid>https://brdefense.center/news/brasil-lidera-ranking-mundial-de-fraudes-digitais/</guid><description>&lt;p>O Brasil ocupa a primeira posição no ranking global de vítimas de fraudes digitais, segundo o Índice de Fraude 2025, elaborado pela Veriff. A pesquisa revela que os brasileiros enfrentam ataques online cinco vezes mais do que cidadãos dos Estados Unidos e do Reino Unido. Aproximadamente 26% dos entrevistados no Brasil relataram ter sido vítimas de fraudes nos últimos doze meses, enquanto as taxas nos EUA e Reino Unido são de 15% e 10%, respectivamente. O impacto financeiro é alarmante, com quase 40% dos brasileiros perdendo até R$ 1.300 em golpes, e 5% relatando perdas superiores a R$ 26 mil em um único incidente. A pesquisa também destaca o papel crescente da inteligência artificial (IA) e dos deepfakes, que contribuíram para um aumento de 21% nas fraudes digitais em comparação ao ano anterior. Quase metade dos entrevistados expressou preocupação com o uso de IA em golpes, refletindo um clima de insegurança. Apesar disso, os brasileiros demonstram maior disposição para adotar sistemas de proteção digital em comparação à média global, indicando uma conscientização crescente sobre a segurança online.&lt;/p></description></item><item><title>Simulação de Brechas A Nova Fronteira da Cibersegurança</title><link>https://brdefense.center/news/simulacao-de-brechas-a-nova-fronteira-da-cibersegu/</link><pubDate>Thu, 30 Oct 2025 12:58:54 -0300</pubDate><guid>https://brdefense.center/news/simulacao-de-brechas-a-nova-fronteira-da-cibersegu/</guid><description>&lt;p>O Picus Breach and Simulation (BAS) Summit deste ano destacou uma mudança significativa na abordagem da cibersegurança, enfatizando que a defesa não se trata mais de prever ataques, mas de provar a eficácia das defesas existentes. Com a velocidade com que novos exploits surgem e se espalham, a necessidade de testar controles em tempo real se torna crucial. O BAS evoluiu de uma atividade anual de conformidade para uma prática diária que valida se as defesas estão realmente funcionando. A simulação de comportamentos adversariais em ambientes controlados permite que as equipes de segurança entendam como suas defesas reagem a ataques, focando em resultados e não apenas em inventários de vulnerabilidades. Além disso, a integração da inteligência artificial no processo de cibersegurança se mostrou mais eficaz na organização de dados do que na criação de novos, permitindo uma resposta mais rápida e precisa a ameaças. O evento também evidenciou que a validação contínua das defesas pode transformar a abordagem de &amp;lsquo;corrigir tudo&amp;rsquo; para &amp;lsquo;corrigir o que realmente importa&amp;rsquo;, priorizando as vulnerabilidades que representam riscos reais. Essa mudança de paradigma é essencial para que as organizações se mantenham à frente das ameaças cibernéticas.&lt;/p></description></item><item><title>Avanço da IA desafia empresas com aumento de fraudes corporativas</title><link>https://brdefense.center/news/avanco-da-ia-desafia-empresas-com-aumento-de-fraud/</link><pubDate>Wed, 29 Oct 2025 18:58:37 -0300</pubDate><guid>https://brdefense.center/news/avanco-da-ia-desafia-empresas-com-aumento-de-fraud/</guid><description>&lt;p>O uso crescente da inteligência artificial (IA) no ambiente corporativo trouxe não apenas otimizações, mas também um aumento alarmante nas fraudes. Um levantamento da plataforma AppZen revelou que, em setembro de 2025, cerca de 14% dos documentos falsos foram gerados com a ajuda da IA, um aumento significativo em relação a 2024, quando essas fraudes eram raras. Os esquemas mais comuns incluem a criação de notas fiscais e comprovantes falsos, tornando a detecção desses golpes cada vez mais desafiadora. A fintech Ramp, por exemplo, bloqueou mais de US$ 1 milhão em notas suspeitas nos últimos três meses. Especialistas, como Fernando Nery, CEO da fintech Portão 3, alertam que a sofisticação das fraudes exige que as empresas invistam em mecanismos de verificação inteligente e cruzamento de dados em tempo real para aumentar a confiança na detecção de documentos falsificados. A situação exige uma reavaliação das práticas de controle financeiro, uma vez que o risco não se limita mais à falta de processos, mas à capacidade de identificar falsificações quase perfeitas geradas por IA.&lt;/p></description></item><item><title>Inteligência Artificial e sua Transformação no GRC</title><link>https://brdefense.center/news/inteligencia-artificial-e-sua-transformacao-no-grc/</link><pubDate>Wed, 29 Oct 2025 13:02:06 -0300</pubDate><guid>https://brdefense.center/news/inteligencia-artificial-e-sua-transformacao-no-grc/</guid><description>&lt;p>A Inteligência Artificial (IA) está revolucionando a Governança, Risco e Conformidade (GRC), trazendo mudanças significativas na forma como as equipes operam. As capacidades da IA incluem acelerar auditorias, identificar riscos críticos rapidamente e reduzir o trabalho manual, resultando em maior eficiência e precisão. No entanto, essa transformação também apresenta novos desafios, como viés potencial, lacunas regulatórias e pontos cegos perigosos, que ainda estão sendo abordados por órgãos reguladores. Para ajudar as organizações a se adaptarem a esse novo cenário, será realizado um webinar gratuito intitulado &amp;lsquo;O Futuro da IA no GRC: Oportunidades, Riscos e Insights Práticos&amp;rsquo;. O evento abordará exemplos reais de como a IA melhora fluxos de trabalho de conformidade, lições aprendidas e melhores práticas, além de estratégias para identificar e mitigar riscos comuns. A velocidade da inovação em IA é impressionante, e a lacuna entre a capacidade tecnológica e o arcabouço legal representa uma exposição imediata ao risco. O webinar reunirá especialistas e exemplos práticos, permitindo que as organizações se preparem proativamente para os desafios que a IA traz ao GRC.&lt;/p></description></item><item><title>Nova vulnerabilidade em navegadores expõe modelos de IA a ataques</title><link>https://brdefense.center/news/nova-vulnerabilidade-em-navegadores-expoe-modelos/</link><pubDate>Wed, 29 Oct 2025 13:01:44 -0300</pubDate><guid>https://brdefense.center/news/nova-vulnerabilidade-em-navegadores-expoe-modelos/</guid><description>&lt;p>Pesquisadores em cibersegurança identificaram uma nova vulnerabilidade em navegadores web que utilizam inteligência artificial, como o OpenAI ChatGPT Atlas. O problema, denominado &amp;lsquo;cloaking direcionado a IA&amp;rsquo;, permite que atacantes manipulem o conteúdo exibido para crawlers de IA, expondo-os a ataques de envenenamento de contexto. A técnica, semelhante ao cloaking de motores de busca, utiliza uma verificação simples do agente do usuário para entregar conteúdo diferente para humanos e sistemas de IA. Isso pode distorcer a percepção de autoridade e verdade, afetando milhões de usuários. A empresa SPLX, que divulgou a vulnerabilidade, alerta que essa manipulação pode ser uma arma poderosa de desinformação, comprometendo a confiança nas ferramentas de IA. Além disso, um estudo da hCaptcha Threat Analysis Group revelou que muitos navegadores tentaram executar ações maliciosas sem necessidade de jailbreak, indicando uma falta de salvaguardas adequadas. Isso levanta preocupações sobre a segurança de sistemas que dependem de IA, especialmente em um cenário onde a otimização para IA se torna cada vez mais comum.&lt;/p></description></item><item><title>Quando malware de IA encontra DDoS um novo desafio para a resiliência online</title><link>https://brdefense.center/news/quando-malware-de-ia-encontra-ddos-um-novo-desafio/</link><pubDate>Wed, 29 Oct 2025 07:02:09 -0300</pubDate><guid>https://brdefense.center/news/quando-malware-de-ia-encontra-ddos-um-novo-desafio/</guid><description>&lt;p>O uso de inteligência artificial (IA) no cibercrime está crescendo rapidamente, com 80% dos ataques de ransomware em 2023-2024 utilizando essa tecnologia. Ferramentas como GhostGPT e AkiraBot estão permitindo que cibercriminosos criem códigos maliciosos, elaborem e-mails de phishing e contornem CAPTCHAs. A evolução dos ataques DDoS, especialmente os de camada de aplicação, se torna mais complexa, pois a IA pode mimetizar o comportamento humano, dificultando a identificação de bots. A proteção tradicional, baseada em CAPTCHAs, já não é eficaz. Em resposta, a filtragem baseada em intenção surge como uma alternativa, avaliando o comportamento do usuário em vez de tentar distinguir humanos de máquinas. As empresas precisam investir em plataformas de mitigação de DDoS que suportem essa nova abordagem e implementar monitoramento em múltiplas camadas para detectar anomalias. A falta de soluções adequadas entre os provedores de segurança gerencia um risco significativo, especialmente para grandes empresas, que podem sofrer danos reputacionais e financeiros severos em caso de ataques bem-sucedidos.&lt;/p></description></item><item><title>IA generativa permite falsificação perfeita de documentos, alerta especialista</title><link>https://brdefense.center/news/ia-generativa-permite-falsificacao-perfeita-de-doc/</link><pubDate>Tue, 28 Oct 2025 19:00:50 -0300</pubDate><guid>https://brdefense.center/news/ia-generativa-permite-falsificacao-perfeita-de-doc/</guid><description>&lt;p>Durante o Cyber Security Summit 2025, o especialista em cibersegurança Andrew Bindner alertou sobre os riscos da inteligência artificial generativa na falsificação de documentos. Ele destacou que qualquer pessoa com acesso a ferramentas de IA pode criar documentos falsos, como carteiras de identidade e passaportes, que são visualmente perfeitos. Essa facilidade de criação de documentos falsificados representa uma ameaça significativa à segurança global, pois torna mais difícil a detecção de fraudes e espionagem. Bindner enfatizou a necessidade de educação cibernética e cooperação internacional para fortalecer a confiança no ambiente digital. Vitor Garcia, da Embraer, complementou que a IA não é apenas uma ameaça, mas também pode ser utilizada como uma ferramenta de defesa contra crimes digitais. A discussão levantou a importância de um esforço conjunto entre empresas e governos para mitigar os riscos associados ao uso malicioso da IA, que pode automatizar crimes digitais e comprometer a segurança das identidades digitais.&lt;/p></description></item><item><title>A Crise Silenciosa da Segurança em Inteligência Artificial</title><link>https://brdefense.center/news/a-crise-silenciosa-da-seguranca-em-inteligencia-ar/</link><pubDate>Thu, 23 Oct 2025 12:58:58 -0300</pubDate><guid>https://brdefense.center/news/a-crise-silenciosa-da-seguranca-em-inteligencia-ar/</guid><description>&lt;p>O uso crescente de inteligência artificial (IA) nas empresas traz benefícios como produtos mais rápidos e sistemas mais inteligentes, mas também levanta preocupações significativas em relação à segurança. Atualmente, estima-se que existam 100 agentes de IA para cada funcionário humano, e alarmantes 99% desses agentes estão completamente não gerenciados, sem supervisão ou controles de ciclo de vida. Isso representa um risco real, pois cada um desses agentes pode se tornar uma porta dos fundos para invasões. O artigo destaca a necessidade urgente de adaptar as ferramentas de segurança tradicionais para lidar com esse novo cenário. Um webinar gratuito intitulado &amp;lsquo;Transformando Controles em Aceleradores da Adoção de IA&amp;rsquo; promete oferecer estratégias práticas para que as empresas possam implementar segurança desde o início, em vez de como uma reflexão tardia. Os participantes aprenderão a governar agentes de IA, a evitar a proliferação de credenciais e a alinhar a segurança com os objetivos de negócios, permitindo que a segurança não seja um obstáculo, mas sim um facilitador da adoção de IA. Essa abordagem é essencial para que engenheiros, arquitetos e CISOs possam deixar de atuar de forma reativa e passar a ter controle e confiança em suas operações de segurança.&lt;/p></description></item><item><title>Ciberataques e espionagem internacional são impulsionados por IA generativa</title><link>https://brdefense.center/news/ciberataques-e-espionagem-internacional-sao-impuls/</link><pubDate>Wed, 22 Oct 2025 13:03:09 -0300</pubDate><guid>https://brdefense.center/news/ciberataques-e-espionagem-internacional-sao-impuls/</guid><description>&lt;p>Um relatório da Microsoft revelou que, entre janeiro e julho de 2025, mais de 200 casos de hackers estrangeiros utilizaram inteligência artificial (IA) para criar e disseminar conteúdo falso e realizar ataques diretos a governos. Este número representa um aumento significativo em relação aos anos anteriores, com mais do que o dobro de casos registrados em 2024 e mais de dez vezes em comparação a 2023. Os cibercriminosos estão utilizando IA para automatizar ataques, como a tradução de e-mails de phishing, tornando-os mais convincentes e difíceis de identificar. Além disso, a criação de clones digitais de altos funcionários governamentais tem sofisticado as táticas de engenharia social, visando a obtenção de dados confidenciais e a desestabilização de serviços essenciais. A vice-presidente de Segurança e Confiança do Cliente da Microsoft, Amy Hogan-Buney, destacou que os EUA são o país mais visado, seguido por Israel e Ucrânia. Apesar das evidências, países como Rússia e China negam envolvimento em operações cibernéticas de espionagem. A Coreia do Norte, por sua vez, tem utilizado IA para criar identidades falsas, permitindo acesso a segredos comerciais e a instalação de malwares em empresas de tecnologia.&lt;/p></description></item><item><title>Inteligência Artificial e Segurança Cibernética Desafios e Oportunidades</title><link>https://brdefense.center/news/inteligencia-artificial-e-seguranca-cibernetica-de/</link><pubDate>Tue, 21 Oct 2025 12:59:48 -0300</pubDate><guid>https://brdefense.center/news/inteligencia-artificial-e-seguranca-cibernetica-de/</guid><description>&lt;p>A inteligência artificial (IA) tem um grande potencial para aprimorar a defesa cibernética, facilitando o trabalho dos profissionais de segurança. Ela pode ajudar a reduzir a fadiga de alertas, identificar padrões rapidamente e escalar operações de segurança de forma que os analistas humanos não conseguem. No entanto, a adoção de IA também amplia a superfície de ataque das organizações, exigindo governança clara, controles de identidade robustos e visibilidade nas decisões tomadas pela IA. Para garantir a segurança, é fundamental estabelecer confiança nos dados que a IA utiliza, responsabilidade pelas ações que executa e supervisão dos resultados que produz. O artigo destaca a importância de tratar sistemas de IA como identidades críticas dentro do gerenciamento de identidade e acesso (IAM), aplicando controles rigorosos como credenciais limitadas, autenticação forte e monitoramento contínuo. Além disso, sugere práticas recomendadas para proteger modelos de IA, incluindo controles de acesso, validação de dados e segurança na inferência. A integração responsável da IA nas operações de segurança pode permitir que as equipes trabalhem de maneira mais inteligente e eficaz, mas é essencial encontrar um equilíbrio entre automação e supervisão humana.&lt;/p></description></item><item><title>Cibercrime movimenta US 10 trilhões dados sobre IA e segurança</title><link>https://brdefense.center/news/cibercrime-movimenta-us-10-trilhoes-dados-sobre-ia/</link><pubDate>Tue, 21 Oct 2025 00:59:25 -0300</pubDate><guid>https://brdefense.center/news/cibercrime-movimenta-us-10-trilhoes-dados-sobre-ia/</guid><description>&lt;p>O avanço da inteligência artificial generativa tem transformado o cibercrime em uma economia paralela que movimenta anualmente cerca de US$ 10 trilhões, segundo Tania Cosentino, ex-presidente da Microsoft Brasil. Durante sua apresentação no CRM Zummit 2025, Cosentino destacou que a combinação de novas tecnologias, como a migração para a nuvem e o trabalho remoto, ampliou a superfície de ataque, tornando as empresas mais vulneráveis. Os hackers utilizam IA para aumentar a velocidade e a sofisticação de seus ataques, resultando em ameaças complexas, como ransomware e ataques a cadeias de suprimento. O tempo de resposta dos atacantes é alarmante, com a média de apenas 1 hora e 12 minutos para se mover lateralmente dentro de uma rede após o acesso inicial. O Brasil é um dos países mais atacados, especialmente em ransomware, devido à percepção de que os resgates são frequentemente pagos. Além disso, a falta de profissionais qualificados em cibersegurança e a disparidade entre regiões do país agravam a situação. A segurança cibernética não é apenas uma questão técnica, mas também um diferencial competitivo, pois 69% dos consumidores evitam empresas percebidas como inseguras.&lt;/p></description></item><item><title>63 dos consumidores brasileiros não conseguem identificar golpes com IA</title><link>https://brdefense.center/news/63-dos-consumidores-brasileiros-nao-conseguem-iden/</link><pubDate>Mon, 20 Oct 2025 13:00:03 -0300</pubDate><guid>https://brdefense.center/news/63-dos-consumidores-brasileiros-nao-conseguem-iden/</guid><description>&lt;p>Uma pesquisa realizada pelo Reclame AQUI revelou que 63% dos consumidores brasileiros não conseguem identificar golpes digitais que utilizam inteligência artificial (IA), uma vulnerabilidade preocupante especialmente com a aproximação da Black Friday. O estudo, que ouviu mais de 3.300 pessoas, destaca que 79% dos entrevistados planejam comprar online durante a Black Friday, mas apenas 43% verificam links e ofertas com ferramentas de segurança. A pesquisa também aponta que 20% dos consumidores já foram vítimas de fraudes em edições anteriores do evento. O uso crescente de IA por cibercriminosos para criar campanhas de phishing mais sofisticadas torna essencial que os consumidores estejam informados e preparados. O relatório, intitulado &amp;ldquo;Black Friday na era da Inteligência Artificial&amp;rdquo;, serve como um guia tanto para consumidores quanto para marcas, enfatizando a importância da segurança nas compras online.&lt;/p></description></item><item><title>Risco zero não existe alívio para quem precisa decidir</title><link>https://brdefense.center/news/risco-zero-nao-existe-alivio-para-quem-precisa-dec/</link><pubDate>Wed, 15 Oct 2025 18:59:48 -0300</pubDate><guid>https://brdefense.center/news/risco-zero-nao-existe-alivio-para-quem-precisa-dec/</guid><description>&lt;p>O artigo de Arthur Capella discute a crescente complexidade da cibersegurança em um ambiente de trabalho distribuído e digitalizado, onde a migração para a nuvem e a adoção de inteligência artificial (IA) aumentam tanto o valor quanto o risco. A superfície de ataque se expandiu mais rapidamente do que a capacidade das empresas de medir e responder a essas ameaças. A gestão de exposição é apresentada como uma disciplina estratégica, essencial para priorizar riscos com base no impacto nos negócios, em vez de se concentrar apenas em uma lista de vulnerabilidades. O autor destaca a escassez de profissionais qualificados em cibersegurança e a limitação orçamentária como desafios constantes. Além disso, enfatiza que apenas 3% das vulnerabilidades frequentemente resultam em riscos significativos, tornando a priorização crucial. O artigo conclui que, em vez de tentar eliminar todos os riscos, as empresas devem focar em fechar as portas que realmente importam, equilibrando inovação e segurança.&lt;/p></description></item><item><title>Como a IA está transformando a segurança cibernética</title><link>https://brdefense.center/news/como-a-ia-esta-transformando-a-seguranca-ciberneti/</link><pubDate>Tue, 14 Oct 2025 12:59:24 -0300</pubDate><guid>https://brdefense.center/news/como-a-ia-esta-transformando-a-seguranca-ciberneti/</guid><description>&lt;p>O uso crescente da inteligência artificial (IA) está revolucionando a forma como os atacantes realizam a fase de reconhecimento em cibersegurança. Antes de enviar um ataque, os hackers analisam minuciosamente o ambiente da vítima, explorando fluxos de login, arquivos JavaScript, mensagens de erro e documentação de APIs. A IA acelera esse processo, permitindo que os atacantes mapeiem sistemas com maior rapidez e precisão. Embora a IA não execute ataques de forma autônoma, ela otimiza a coleta e análise de informações, ajudando a identificar vulnerabilidades e caminhos de ataque.&lt;/p></description></item><item><title>Ataques cibernéticos em evolução vulnerabilidades e ameaças emergentes</title><link>https://brdefense.center/news/ataques-ciberneticos-em-evolucao-vulnerabilidades/</link><pubDate>Mon, 13 Oct 2025 12:59:04 -0300</pubDate><guid>https://brdefense.center/news/ataques-ciberneticos-em-evolucao-vulnerabilidades/</guid><description>&lt;p>O cenário de cibersegurança continua a se deteriorar, com ataques cada vez mais sofisticados e coordenados. Um dos principais incidentes recentes envolve a exploração de uma falha crítica no Oracle E-Business Suite, afetando diversas organizações desde agosto de 2025. A falha, identificada como CVE-2025-61882, possui uma pontuação CVSS de 9.8 e foi utilizada por grupos como o Cl0p para exfiltrar dados sensíveis. Além disso, o grupo Storm-1175 explorou uma vulnerabilidade no GoAnywhere MFT, resultando em ataques em setores variados, como transporte e educação.&lt;/p></description></item><item><title>Hackers russos usam inteligência artificial em ataques cibernéticos na Ucrânia</title><link>https://brdefense.center/news/hackers-russos-usam-inteligencia-artificial-em-ata/</link><pubDate>Thu, 09 Oct 2025 06:57:47 -0300</pubDate><guid>https://brdefense.center/news/hackers-russos-usam-inteligencia-artificial-em-ata/</guid><description>&lt;p>No primeiro semestre de 2025, hackers russos intensificaram o uso de inteligência artificial (IA) em ataques cibernéticos contra a Ucrânia, conforme relatado pelo Serviço Estatal de Comunicações Especiais e Proteção da Informação (SSSCIP). A agência registrou 3.018 incidentes cibernéticos, um aumento em relação aos 2.575 do segundo semestre de 2024. Os ataques incluem campanhas de phishing e o uso de malware gerado por IA, como o WRECKSTEEL, que visa a administração estatal e infraestrutura crítica. Além disso, grupos como UAC-0218 e UAC-0226 têm direcionado suas ações a forças de defesa e órgãos governamentais, utilizando táticas sofisticadas como arquivos RAR armadilhados e técnicas de engenharia social. O SSSCIP também observou a exploração de vulnerabilidades em softwares de webmail, permitindo ataques sem interação do usuário. A utilização de serviços legítimos como Dropbox e Google Drive para hospedar malware também tem crescido, evidenciando a adaptação dos atacantes às tecnologias disponíveis. O cenário de guerra híbrida se intensifica, com operações cibernéticas sincronizadas a ataques físicos no campo de batalha.&lt;/p></description></item><item><title>Da exaustão à superação fotógrafos recuperam tempo perdido com IA</title><link>https://brdefense.center/news/da-exaustao-a-superacao-fotografos-recuperam-tempo/</link><pubDate>Wed, 08 Oct 2025 19:01:18 -0300</pubDate><guid>https://brdefense.center/news/da-exaustao-a-superacao-fotografos-recuperam-tempo/</guid><description>&lt;p>A indústria da fotografia está passando por uma transformação significativa com a adoção de ferramentas de inteligência artificial (IA), conforme revelado no relatório Aftershoot Photography Workflow Report de 2025. Com base em respostas de mais de 1.000 fotógrafos profissionais globalmente, o estudo indica que 81% dos entrevistados que implementaram fluxos de trabalho baseados em IA melhoraram seu equilíbrio entre vida profissional e pessoal, recuperando tempo anteriormente perdido em edições repetitivas. Um dado impressionante é que 64% dos fotógrafos afirmaram que seus clientes não perceberam diferença entre imagens editadas por IA e aquelas editadas manualmente. Essa mudança de percepção está redefinindo o que significa ter um negócio criativo sustentável, permitindo que os profissionais se concentrem em crescimento pessoal e bem-estar mental. Além disso, 28% dos fotógrafos agora conseguem entregar galerias completas em menos de uma semana, o que representa o dobro da taxa de 2024. A automação não apenas aumenta a produtividade, mas também redefine o tempo criativo, permitindo que os fotógrafos se concentrem em projetos pessoais e desenvolvimento de habilidades. A pesquisa sugere que a IA não substitui a criatividade, mas a amplifica, ajudando os fotógrafos a atender às expectativas de entrega rápida e qualidade consistente.&lt;/p></description></item><item><title>As 10 Melhores Empresas de Prevenção à Fraude em 2025</title><link>https://brdefense.center/news/as-10-melhores-empresas-de-prevencao-a-fraude-em-2/</link><pubDate>Wed, 08 Oct 2025 18:59:53 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-empresas-de-prevencao-a-fraude-em-2/</guid><description>&lt;p>A prevenção de fraudes é uma prioridade crítica para empresas de todos os tamanhos em 2025, à medida que os cibercriminosos evoluem suas táticas e exploram novas vulnerabilidades. Proteger dados sensíveis, garantir a segurança das transações e manter a confiança do cliente são essenciais no cenário digital atual. Este artigo apresenta as dez melhores empresas de prevenção à fraude de 2025, selecionadas com base em suas tecnologias avançadas, soluções abrangentes e sucesso comprovado no combate à fraude. As empresas listadas utilizam inteligência artificial, aprendizado de máquina, biometria comportamental e análises em tempo real para detectar e prevenir atividades fraudulentas de forma rápida e precisa. As soluções não apenas protegem as empresas contra perdas financeiras, mas também ajudam a manter a conformidade com as regulamentações em evolução e a preservar a reputação da marca. A escolha da melhor empresa de prevenção à fraude permite que as organizações se mantenham à frente das tendências de fraude e protejam suas operações de forma eficaz, reduzindo falsos positivos e otimizando fluxos de trabalho de fraude, melhorando a experiência do cliente.&lt;/p></description></item><item><title>As 10 Melhores Empresas de Segurança em Inteligência de Cadeia de Suprimentos em 2025</title><link>https://brdefense.center/news/as-10-melhores-empresas-de-seguranca-em-inteligenc/</link><pubDate>Wed, 08 Oct 2025 18:59:34 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-empresas-de-seguranca-em-inteligenc/</guid><description>&lt;p>No contexto atual da economia global interconectada, a segurança das cadeias de suprimentos se tornou uma prioridade crítica para empresas em todo o mundo. O aumento de ciberataques, vazamentos de dados e interrupções geopolíticas ameaça a estabilidade das cadeias de suprimentos e a continuidade dos negócios. Para enfrentar esses riscos, empresas especializadas em segurança de inteligência de cadeia de suprimentos utilizam tecnologias avançadas como inteligência artificial (IA), aprendizado de máquina, inteligência de ameaças e análises de risco. Essas soluções permitem que as organizações identifiquem proativamente vulnerabilidades, monitorem fornecedores e mitiguem ameaças em tempo real. O artigo apresenta uma análise das dez melhores empresas de segurança em inteligência de cadeia de suprimentos em 2025, avaliando-as com base em critérios como especificações, recursos, razões para compra, prós e contras. A lista destaca empresas que se destacam em precisão, capacidade de integração e insights acionáveis, ajudando os gestores de risco a responder rapidamente e reduzir potenciais interrupções ou violações na cadeia de suprimentos.&lt;/p></description></item><item><title>Top 10 Melhores Soluções de Proteção de Marca para Empresas em 2025</title><link>https://brdefense.center/news/top-10-melhores-solucoes-de-protecao-de-marca-para/</link><pubDate>Wed, 08 Oct 2025 13:00:53 -0300</pubDate><guid>https://brdefense.center/news/top-10-melhores-solucoes-de-protecao-de-marca-para/</guid><description>&lt;p>Em 2025, com o aumento sem precedentes de riscos digitais, como falsificações, phishing e roubo de propriedade intelectual, as empresas precisam de soluções rigorosas de proteção de marca. O artigo apresenta as 10 melhores soluções de proteção de marca, destacando suas forças, especificações e razões para aquisição. As soluções modernas combinam automação, inteligência artificial e expertise humana para monitorar ativos digitais e proteger a confiança da marca em escala global. Entre as principais soluções estão Red Points, BrandShield e MarkMonitor, cada uma oferecendo recursos como detecção automatizada, cobertura multicanal e análises impulsionadas por IA. A escolha da solução certa é crucial para que as marcas operem com confiança e inovem de forma segura, especialmente em um cenário onde a proteção vai além da segurança tradicional.&lt;/p></description></item><item><title>As 10 Melhores Plataformas de Proteção Digital em 2025</title><link>https://brdefense.center/news/as-10-melhores-plataformas-de-protecao-digital-em/</link><pubDate>Wed, 08 Oct 2025 06:59:27 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-plataformas-de-protecao-digital-em/</guid><description>&lt;p>Com a crescente digitalização das empresas, a superfície de ataque a ameaças cibernéticas também se expande. As plataformas de Proteção de Risco Digital (DRP) são essenciais para detectar, monitorar e mitigar ameaças externas, garantindo uma defesa proativa. Em 2025, as principais plataformas de DRP incorporaram automação, análises impulsionadas por inteligência artificial e integração de informações. As organizações não podem mais depender apenas de ferramentas tradicionais de cibersegurança, como firewalls. As dez melhores plataformas de DRP oferecem monitoramento em tempo real, enriquecimento de inteligência sobre ameaças e integração de resposta a incidentes. A escolha da plataforma certa pode ser a diferença entre antecipar-se a ataques ou reagir tarde demais, resultando em danos à marca e perdas financeiras. Entre as principais plataformas destacam-se Proofpoint, ReliaQuest e BlueVoyant Sky, cada uma com características únicas que atendem a diferentes necessidades de segurança, especialmente para grandes empresas e instituições financeiras. A adoção dessas tecnologias é crucial para proteger a reputação e os ativos digitais das organizações frente a ameaças emergentes.&lt;/p></description></item><item><title>OpenAI desmantela grupos que usavam ChatGPT para desenvolver malware</title><link>https://brdefense.center/news/openai-desmantela-grupos-que-usavam-chatgpt-para-d/</link><pubDate>Wed, 08 Oct 2025 06:58:40 -0300</pubDate><guid>https://brdefense.center/news/openai-desmantela-grupos-que-usavam-chatgpt-para-d/</guid><description>&lt;p>No dia 8 de outubro de 2025, a OpenAI anunciou a interrupção de três grupos de atividade que estavam utilizando sua ferramenta de inteligência artificial, o ChatGPT, para facilitar o desenvolvimento de malware. Um dos grupos, de língua russa, usou o chatbot para criar e aprimorar um trojan de acesso remoto (RAT) e um ladrão de credenciais, buscando evitar a detecção. A OpenAI observou que esses usuários estavam associados a grupos criminosos que compartilhavam evidências de suas atividades em canais do Telegram. Embora os modelos de linguagem da OpenAI tenham se recusado a atender a pedidos diretos para criar conteúdo malicioso, os criminosos contornaram essa limitação, gerando códigos que foram montados para criar fluxos de trabalho maliciosos. Outro grupo, da Coreia do Norte, utilizou o ChatGPT para desenvolver malware e ferramentas de comando e controle, enquanto um terceiro grupo, da China, focou em campanhas de phishing. Além disso, a OpenAI bloqueou contas que estavam envolvidas em fraudes e operações de influência, incluindo atividades de vigilância ligadas a entidades governamentais chinesas. A empresa destacou que os atores de ameaça estão se adaptando para ocultar sinais de que o conteúdo foi gerado por uma ferramenta de IA, o que representa um novo desafio para a segurança cibernética.&lt;/p></description></item><item><title>A Inteligência Artificial e a Evolução da Cibersegurança</title><link>https://brdefense.center/news/a-inteligencia-artificial-e-a-evolucao-da-ciberseg/</link><pubDate>Wed, 08 Oct 2025 06:58:20 -0300</pubDate><guid>https://brdefense.center/news/a-inteligencia-artificial-e-a-evolucao-da-ciberseg/</guid><description>&lt;p>A inteligência artificial (IA) está transformando o cenário da cibersegurança, tanto para atacantes quanto para defensores. Os cibercriminosos utilizam ferramentas baseadas em IA para automatizar e acelerar ataques, criando um desafio sem precedentes para as equipes de segurança, que enfrentam uma avalanche de dados sobre vulnerabilidades e alertas. Apesar do potencial da IA, muitas empresas ainda têm dificuldades em integrá-la efetivamente em suas estratégias de segurança. O artigo destaca três áreas principais onde a IA pode ser aplicada para maximizar a eficácia: deduplicação e correlação de dados, priorização de riscos e uma camada de inteligência que complementa a análise humana. A deduplicação ajuda a criar uma visão clara dos riscos, enquanto a priorização permite que as equipes concentrem seus esforços nas vulnerabilidades mais críticas. A camada de inteligência fornece recomendações e simulações que capacitam os analistas a tomar decisões mais informadas. Com a crescente utilização de IA pelos atacantes, é imperativo que as organizações adotem essas tecnologias para se manterem à frente. Plataformas como a PlexTrac estão na vanguarda dessa transformação, investindo em capacidades de IA para ajudar as equipes a gerenciar dados de forma centralizada e eficaz.&lt;/p></description></item><item><title>As 10 Melhores Soluções de Gestão de Risco da Cadeia de Suprimentos em 2025</title><link>https://brdefense.center/news/as-10-melhores-solucoes-de-gestao-de-risco-da-cade/</link><pubDate>Tue, 07 Oct 2025 18:59:40 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-solucoes-de-gestao-de-risco-da-cade/</guid><description>&lt;p>A gestão de risco da cadeia de suprimentos (SCRM) se tornou um pilar essencial para empresas que buscam resiliência em 2025. Com a crescente interconexão e fragilidade das cadeias globais, as organizações enfrentam riscos que vão desde conflitos geopolíticos até ciberataques. O artigo destaca as 10 melhores soluções de SCRM, que utilizam análises preditivas, monitoramento em tempo real e insights impulsionados por inteligência artificial para proteger suas redes de suprimentos. Entre as soluções mencionadas, Prewave se destaca por sua capacidade de detectar riscos em tempo real e monitorar a conformidade ESG, enquanto Resilinc oferece visibilidade em múltiplos níveis da cadeia de suprimentos, essencial para setores como tecnologia e saúde. Sphera é reconhecida por sua forte ênfase em gestão de riscos ambientais e de sustentabilidade. A escolha da solução adequada pode melhorar significativamente a visibilidade, mitigar riscos e fortalecer o desempenho dos fornecedores, tornando-se crucial para a competitividade das empresas no cenário atual.&lt;/p></description></item><item><title>A Inteligência Artificial e o Risco de Vazamento de Dados Corporativos</title><link>https://brdefense.center/news/a-inteligencia-artificial-e-o-risco-de-vazamento-d/</link><pubDate>Tue, 07 Oct 2025 12:59:10 -0300</pubDate><guid>https://brdefense.center/news/a-inteligencia-artificial-e-o-risco-de-vazamento-d/</guid><description>&lt;p>Um novo relatório da LayerX revela que a inteligência artificial (IA) se tornou o maior canal não controlado para a exfiltração de dados corporativos, superando ferramentas de SaaS não gerenciadas e compartilhamento de arquivos. Com 45% dos funcionários de empresas utilizando ferramentas de IA generativa, como ChatGPT, a falta de governança é alarmante, pois 67% do uso ocorre em contas pessoais não gerenciadas. O estudo aponta que 40% dos arquivos enviados para ferramentas de IA contêm dados sensíveis, e 77% dos funcionários colam informações nessas plataformas, com 82% dessas ações originando de contas não gerenciadas. A segurança tradicional, focada em uploads de arquivos, ignora esses vetores de vazamento. Além disso, 87% do uso de mensagens instantâneas ocorre em contas não gerenciadas, criando um cenário de risco elevado. O relatório recomenda que a segurança da IA seja tratada como uma categoria essencial, com estratégias de governança que incluam monitoramento de uploads e restrições a contas pessoais. Para os líderes de segurança, a urgência em adaptar as políticas de segurança é clara, pois a IA já está integrada aos fluxos de trabalho e representa um vetor principal para a perda de dados corporativos.&lt;/p></description></item><item><title>Google DeepMind lança agente de IA para corrigir vulnerabilidades de código</title><link>https://brdefense.center/news/google-deepmind-lanca-agente-de-ia-para-corrigir-v/</link><pubDate>Tue, 07 Oct 2025 12:58:45 -0300</pubDate><guid>https://brdefense.center/news/google-deepmind-lanca-agente-de-ia-para-corrigir-v/</guid><description>&lt;p>A divisão DeepMind do Google anunciou o lançamento do CodeMender, um agente de inteligência artificial (IA) que detecta, corrige e reescreve automaticamente códigos vulneráveis, visando prevenir futuras explorações. O CodeMender é projetado para ser tanto reativo quanto proativo, corrigindo novas vulnerabilidades assim que são identificadas e reforçando códigos existentes para eliminar classes inteiras de falhas. Nos últimos seis meses, a ferramenta já contribuiu com 72 correções de segurança para projetos de código aberto, incluindo alguns com até 4,5 milhões de linhas de código.&lt;/p></description></item><item><title>Gestão da Segurança em IA Perguntas Cruciais para Escolher Soluções</title><link>https://brdefense.center/news/gestao-da-seguranca-em-ia-perguntas-cruciais-para/</link><pubDate>Mon, 06 Oct 2025 12:59:32 -0300</pubDate><guid>https://brdefense.center/news/gestao-da-seguranca-em-ia-perguntas-cruciais-para/</guid><description>&lt;p>No contexto atual de rápida evolução da inteligência artificial (IA) e das tecnologias em nuvem, as organizações estão cada vez mais adotando medidas de segurança para proteger dados sensíveis e garantir conformidade regulatória. As soluções de AI-SPM (Gestão da Postura de Segurança em IA) têm se destacado como ferramentas essenciais para proteger pipelines de IA e ativos de dados. O artigo destaca cinco perguntas críticas que as empresas devem fazer ao avaliar soluções de AI-SPM. A primeira pergunta aborda a necessidade de visibilidade e controle abrangentes sobre os riscos associados à IA e aos dados. A segunda pergunta foca na capacidade da solução de identificar e remediar riscos específicos da IA, como ataques adversariais e viés em modelos preditivos. A conformidade regulatória é o tema da terceira pergunta, enfatizando a importância de garantir que as soluções atendam a normas como a LGPD e o GDPR. A escalabilidade em arquiteturas dinâmicas de nuvem é discutida na quarta pergunta, enquanto a integração com ferramentas de segurança existentes é abordada na quinta. O artigo conclui ressaltando que a segurança em IA deve ser proativa, permitindo que as organizações inovem com confiança em um ambiente de ameaças em constante evolução.&lt;/p></description></item><item><title>A IA está mudando a automação, mas nem sempre para melhor</title><link>https://brdefense.center/news/a-ia-esta-mudando-a-automacao-mas-nem-sempre-para/</link><pubDate>Wed, 01 Oct 2025 18:58:01 -0300</pubDate><guid>https://brdefense.center/news/a-ia-esta-mudando-a-automacao-mas-nem-sempre-para/</guid><description>&lt;p>A automação impulsionada pela inteligência artificial (IA) está transformando a forma como as organizações operam, mas não sem desafios significativos. O artigo destaca que a busca por automação total pode resultar em sistemas frágeis, onde a intervenção humana excessiva atrasa respostas e a rigidez das regras não se adapta a novas ameaças. Além disso, o uso excessivo de IA pode gerar processos obscuros que comprometem a confiança e a conformidade. Para líderes de cibersegurança e operações, a necessidade de fluxos de trabalho rápidos, confiáveis e auditáveis é crucial. O webinar &amp;lsquo;Workflow Clarity: Where AI Fits in Modern Automation&amp;rsquo;, apresentado por Thomas Kinsella, Co-fundador e Diretor de Clientes da Tines, abordará como as equipes de segurança estão integrando pessoas, regras e agentes de IA de forma intencional para criar fluxos de trabalho eficazes. Os participantes aprenderão a identificar onde a IA é mais útil, como evitar a complexidade desnecessária e garantir a segurança e a auditabilidade dos processos. O evento é voltado para líderes que buscam estratégias práticas para implementar automação que fortaleça as defesas sem criar novos riscos.&lt;/p></description></item><item><title>Malwares se disfarçam de ferramentas de IA para infectar empresas</title><link>https://brdefense.center/news/malwares-se-disfarcam-de-ferramentas-de-ia-para-in/</link><pubDate>Wed, 01 Oct 2025 13:02:33 -0300</pubDate><guid>https://brdefense.center/news/malwares-se-disfarcam-de-ferramentas-de-ia-para-in/</guid><description>&lt;p>Um estudo da Trend Micro revelou que malwares estão se disfarçando como ferramentas de inteligência artificial para infectar empresas em diversas regiões do mundo, incluindo Brasil, Estados Unidos, França e Índia. O malware mais destacado na pesquisa é o EvilAI, que imita softwares legítimos, dificultando a detecção por parte dos usuários. Os alvos principais incluem indústrias, agências governamentais e setores de saúde e tecnologia. Os hackers utilizam certificados válidos de empresas descartáveis para aumentar a credibilidade dos programas maliciosos. Uma vez instalados, esses malwares conseguem extrair dados sensíveis dos navegadores das vítimas e enviá-los para servidores controlados pelos criminosos. A distribuição ocorre por meio de anúncios falsos, sites de venda fraudulentos e manipulação de SEO. A situação é alarmante, pois a popularidade das ferramentas de IA está sendo explorada para enganar usuários, e a criação de mercados de malware na dark web facilita ainda mais esses ataques. Especialistas alertam que a tendência pode se intensificar, exigindo atenção redobrada das empresas em relação à segurança cibernética.&lt;/p></description></item><item><title>Desafios dos SOCs Legados e a Necessidade de Contexto na Cibersegurança</title><link>https://brdefense.center/news/desafios-dos-socs-legados-e-a-necessidade-de-conte/</link><pubDate>Tue, 30 Sep 2025 12:58:26 -0300</pubDate><guid>https://brdefense.center/news/desafios-dos-socs-legados-e-a-necessidade-de-conte/</guid><description>&lt;p>Os Centros de Operações de Segurança (SOCs) enfrentam um desafio crescente com a avalanche de alertas que chegam diariamente, resultando em um cenário caótico onde os analistas lutam para manter o controle. O modelo tradicional, que se baseia em regras e gera alertas sem contexto, muitas vezes resulta em atrasos na identificação de ameaças reais. Para superar essa situação, é essencial adotar uma abordagem que priorize o contexto em vez do caos. Ao normalizar e conectar dados de diferentes fontes, como logs de sistemas de identidade e cargas de trabalho em nuvem, os analistas podem obter uma visão mais clara das atividades suspeitas. Isso transforma tentativas de login em potencial em informações valiosas sobre um possível ataque em andamento.&lt;/p></description></item><item><title>Microsoft expande solução de segurança com novo data lake do Sentinel</title><link>https://brdefense.center/news/microsoft-expande-solucao-de-seguranca-com-novo-da/</link><pubDate>Tue, 30 Sep 2025 12:58:01 -0300</pubDate><guid>https://brdefense.center/news/microsoft-expande-solucao-de-seguranca-com-novo-da/</guid><description>&lt;p>No dia 30 de setembro de 2025, a Microsoft anunciou a expansão de sua solução de gerenciamento de incidentes e eventos de segurança (SIEM), o Sentinel, com a disponibilização geral do Sentinel data lake. Este novo recurso é uma ferramenta nativa da nuvem, projetada para ingerir, gerenciar e analisar dados de segurança, proporcionando melhor visibilidade e análises avançadas. O data lake permite que modelos de inteligência artificial, como o Security Copilot, tenham acesso ao contexto completo necessário para detectar padrões sutis e correlacionar sinais, facilitando a identificação de comportamentos de atacantes e a resposta a incidentes. Além disso, a Microsoft introduziu o Sentinel Graph e o Modelo de Contexto do Protocolo (MCP), que permitem uma orquestração mais eficiente e uma compreensão contextual mais rica dos dados de segurança. A empresa também destacou a importância de proteger plataformas de IA contra ataques de injeção de prompt, anunciando melhorias no Azure AI Foundry para aumentar a segurança dos agentes de IA. Com essas inovações, a Microsoft visa transformar a cibersegurança de um modelo reativo para um preditivo, permitindo que as equipes de segurança respondam mais rapidamente a eventos em larga escala.&lt;/p></description></item><item><title>Vulnerabilidades no assistente de IA Gemini expõem riscos de privacidade</title><link>https://brdefense.center/news/vulnerabilidades-no-assistente-de-ia-gemini-expoem/</link><pubDate>Tue, 30 Sep 2025 12:57:40 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidades-no-assistente-de-ia-gemini-expoem/</guid><description>&lt;p>Pesquisadores de cibersegurança revelaram três vulnerabilidades críticas no assistente de inteligência artificial Gemini do Google, que, se exploradas, poderiam comprometer a privacidade dos usuários e permitir o roubo de dados. As falhas, coletivamente chamadas de &amp;lsquo;Gemini Trifecta&amp;rsquo;, incluem: uma injeção de prompt no Gemini Cloud Assist, que poderia permitir que atacantes manipulassem serviços em nuvem; uma injeção de busca no modelo de Personalização de Busca do Gemini, que poderia vazar informações salvas e dados de localização ao manipular o histórico de busca do Chrome; e uma falha de injeção indireta no Gemini Browsing Tool, que poderia exfiltrar dados do usuário para servidores externos. Após a divulgação responsável, o Google implementou medidas de segurança, como a interrupção da renderização de hyperlinks nas respostas de resumo de logs. A Tenable, empresa de segurança, destacou que a situação evidencia que a IA pode ser utilizada como veículo de ataque, não apenas como alvo, enfatizando a necessidade de visibilidade e controle rigoroso sobre ferramentas de IA em ambientes corporativos.&lt;/p></description></item><item><title>Adoção de IA nas empresas riscos e segurança na cadeia de suprimentos</title><link>https://brdefense.center/news/adocao-de-ia-nas-empresas-riscos-e-seguranca-na-ca/</link><pubDate>Tue, 30 Sep 2025 06:57:13 -0300</pubDate><guid>https://brdefense.center/news/adocao-de-ia-nas-empresas-riscos-e-seguranca-na-ca/</guid><description>&lt;p>A adoção de Inteligência Artificial (IA) nas empresas está em rápida ascensão, com ferramentas de IA generativa sendo integradas a diversas funções, como marketing, desenvolvimento e recursos humanos. Essa transformação traz inovação e eficiência, mas também novos riscos que precisam ser geridos. Entre os principais desafios estão a &amp;rsquo;expansão da IA&amp;rsquo;, onde funcionários utilizam ferramentas sem supervisão de segurança, e as vulnerabilidades na cadeia de suprimentos, que aumentam a superfície de ataque. Além disso, o compartilhamento de dados sensíveis com serviços de IA externos levanta preocupações sobre vazamentos e uso indevido de informações. Para mitigar esses riscos, é necessário um novo paradigma de segurança que inclua descoberta contínua, monitoramento em tempo real e avaliação adaptativa de riscos. A Wing Security se destaca nesse cenário, oferecendo visibilidade e controle sobre a utilização de aplicações de IA, permitindo que as empresas inovem com segurança, reduzindo a exposição a ataques e garantindo conformidade regulatória. Essa abordagem transforma a segurança em um facilitador de negócios, permitindo que as organizações adotem ferramentas de IA de forma responsável e segura.&lt;/p></description></item><item><title>Banalização da privacidade app paga para você vender sua voz para IAs</title><link>https://brdefense.center/news/banalizacao-da-privacidade-app-paga-para-voce-vend/</link><pubDate>Mon, 29 Sep 2025 18:59:41 -0300</pubDate><guid>https://brdefense.center/news/banalizacao-da-privacidade-app-paga-para-voce-vend/</guid><description>&lt;p>O Neon Mobile é um aplicativo que permite aos usuários venderem suas gravações de voz, gerando polêmica sobre privacidade e ética. O app grava ligações telefônicas e paga US$ 0,30 por minuto, com um limite diário de US$ 30. Embora os usuários possam decidir o que fazer com seus dados, a política de privacidade do Neon levanta preocupações sobre o uso e a venda dessas gravações para empresas de inteligência artificial. O app afirma que remove informações pessoais antes de vender os dados, mas não esclarece como os parceiros utilizarão essas gravações. A prática é legal em alguns lugares, mas suscita debates sobre a conscientização dos usuários em relação ao que estão compartilhando. A questão central é se os usuários realmente entendem os riscos envolvidos em vender suas conversas privadas por valores baixos, considerando o potencial uso malicioso por terceiros. O caso do Neon Mobile reflete uma tendência preocupante em que a privacidade é sacrificada em troca de recompensas financeiras mínimas.&lt;/p></description></item><item><title>Campanha EvilAI usa ferramentas de IA para distribuir malware</title><link>https://brdefense.center/news/campanha-evilai-usa-ferramentas-de-ia-para-distrib/</link><pubDate>Mon, 29 Sep 2025 18:58:05 -0300</pubDate><guid>https://brdefense.center/news/campanha-evilai-usa-ferramentas-de-ia-para-distrib/</guid><description>&lt;p>Uma nova campanha de malware, denominada EvilAI, está utilizando ferramentas de inteligência artificial (IA) aparentemente legítimas para infiltrar malware em organizações ao redor do mundo. De acordo com a Trend Micro, os ataques têm como alvo setores como manufatura, governo, saúde, tecnologia e varejo, afetando países como Índia, EUA, França, Itália, Brasil, Alemanha, Reino Unido, Noruega, Espanha e Canadá. Os atacantes se destacam por sua habilidade em disfarçar software malicioso como aplicativos de produtividade, utilizando interfaces profissionais e assinaturas digitais válidas, dificultando a identificação por usuários e ferramentas de segurança.&lt;/p></description></item><item><title>Lideranças de Segurança Adotam IA para Enfrentar Sobrecarga de Alertas</title><link>https://brdefense.center/news/liderancas-de-seguranca-adotam-ia-para-enfrentar-s/</link><pubDate>Mon, 29 Sep 2025 12:58:14 -0300</pubDate><guid>https://brdefense.center/news/liderancas-de-seguranca-adotam-ia-para-enfrentar-s/</guid><description>&lt;p>Um estudo recente com 282 líderes de segurança revela que os Centros de Operações de Segurança (SOCs) enfrentam um aumento insustentável no volume de alertas, com uma média de 960 alertas processados diariamente, e até 3.000 em grandes empresas. Essa sobrecarga tem levado a uma situação crítica, onde 40% dos alertas não são investigados devido à falta de recursos. O tempo médio para investigar um alerta é de 70 minutos, mas 56 minutos se passam antes que qualquer ação seja tomada. Essa realidade resulta em um risco operacional significativo, pois 61% das equipes admitem ignorar alertas que se tornaram incidentes críticos. A adoção de Inteligência Artificial (IA) está se tornando essencial, com 55% das equipes já utilizando assistentes de IA para triagem e investigação. A pesquisa indica que 60% das cargas de trabalho dos SOCs podem ser geridas por IA nos próximos três anos, permitindo que analistas se concentrem em investigações mais complexas. Apesar das barreiras como preocupações com privacidade e integração, a tendência é clara: a IA está se tornando uma prioridade estratégica para melhorar a eficiência operacional e reduzir a fadiga dos analistas.&lt;/p></description></item><item><title>Ferramenta de IA ajuda Reino Unido a recuperar 480 milhões em fraudes</title><link>https://brdefense.center/news/ferramenta-de-ia-ajuda-reino-unido-a-recuperar-480/</link><pubDate>Fri, 26 Sep 2025 00:58:40 -0300</pubDate><guid>https://brdefense.center/news/ferramenta-de-ia-ajuda-reino-unido-a-recuperar-480/</guid><description>&lt;p>Um novo sistema de detecção de fraudes baseado em inteligência artificial (IA) ajudou o governo do Reino Unido a recuperar um recorde de £480 milhões em fraudes no último ano, o maior valor já recuperado em um período de 12 meses. O sistema, denominado Fraud Risk Assessment Accelerator, foi crucial na identificação de fraudes relacionadas ao programa de empréstimos Bounce Back, que visava apoiar empresas durante a pandemia de Covid-19. Esses empréstimos, que podiam chegar a £50.000, foram criticados por serem concedidos sem verificações adequadas, resultando em um aumento significativo de fraudes. O governo planeja reinvestir os valores recuperados em serviços essenciais como saúde, educação e policiamento. Apesar do sucesso, grupos de defesa das liberdades civis expressaram preocupações sobre o uso crescente de ferramentas de IA no governo, levantando questões sobre viés e resultados injustos. O ministro do Gabinete, Josh Simons, anunciou que o sistema será licenciado internacionalmente, com países como EUA, Canadá e Austrália demonstrando interesse em sua implementação.&lt;/p></description></item><item><title>Malware com Inteligência Artificial é Descoberto por Pesquisadores</title><link>https://brdefense.center/news/malware-com-inteligencia-artificial-e-descoberto-p/</link><pubDate>Sat, 20 Sep 2025 06:58:27 -0300</pubDate><guid>https://brdefense.center/news/malware-com-inteligencia-artificial-e-descoberto-p/</guid><description>&lt;p>Pesquisadores de cibersegurança da SentinelOne apresentaram no LABScon 2025 a descoberta do MalTerminal, um malware que incorpora capacidades de Modelos de Linguagem de Grande Escala (LLMs). Este malware, que utiliza a API do OpenAI GPT-4, é capaz de gerar dinamicamente códigos de ransomware ou shells reversos. Embora não haja evidências de que tenha sido utilizado em ataques reais, sua existência representa um marco na evolução das técnicas de ataque, com a introdução de malwares que podem gerar lógica maliciosa em tempo real. Além disso, a pesquisa revelou que criminosos cibernéticos estão utilizando prompts ocultos em e-mails de phishing para enganar scanners de segurança baseados em IA, aumentando a eficácia desses ataques. O uso de ferramentas de IA generativa por cibercriminosos está se tornando uma tendência preocupante, com um aumento nas campanhas de engenharia social que exploram plataformas de hospedagem de sites para criar páginas de phishing. Esse cenário exige atenção redobrada das empresas, especialmente em relação à segurança de suas comunicações eletrônicas e à proteção de dados sensíveis.&lt;/p></description></item><item><title>A IA e seu impacto na cibersegurança segundo o Google</title><link>https://brdefense.center/news/a-ia-e-seu-impacto-na-ciberseguranca-segundo-o-goo/</link><pubDate>Thu, 18 Sep 2025 13:05:16 -0300</pubDate><guid>https://brdefense.center/news/a-ia-e-seu-impacto-na-ciberseguranca-segundo-o-goo/</guid><description>&lt;p>A inteligência artificial (IA) está transformando o cenário da cibersegurança, conforme destacado por Sandra Joyce, Vice-Presidente de Inteligência de Ameaças do Google, em um encontro em São Paulo. A IA não é apenas uma ferramenta de defesa, mas também uma arma poderosa nas mãos de atacantes. Grupos criminosos estão utilizando IA para aprimorar suas táticas, resultando em ataques de phishing mais sofisticados, deepfakes convincentes e fraudes por voz (vishing) cada vez mais realistas. O Google, por sua vez, está investindo em IA para fortalecer suas defesas, conseguindo reverter malware em minutos e detectar vulnerabilidades críticas antes que sejam exploradas. A crescente organização dos grupos de ransomware, que agora operam como verdadeiras empresas, também foi um ponto destacado, evidenciando a necessidade de uma resposta robusta por parte das organizações. Sandra enfatizou que a corrida entre atacantes e defensores está apenas começando, e que a IA será fundamental para garantir a segurança digital. Além disso, a IA promete criar novas oportunidades de trabalho no setor de segurança digital, exigindo que os profissionais se familiarizem com essa tecnologia.&lt;/p></description></item><item><title>As 10 Melhores Ferramentas de Filtragem de Conteúdo Web em 2025</title><link>https://brdefense.center/news/as-10-melhores-ferramentas-de-filtragem-de-conteud/</link><pubDate>Wed, 17 Sep 2025 19:00:09 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-ferramentas-de-filtragem-de-conteud/</guid><description>&lt;p>Em um cenário digital marcado pelo trabalho remoto e ameaças cibernéticas sofisticadas, a filtragem de conteúdo web se tornou uma medida de segurança essencial para organizações de todos os tamanhos. As melhores soluções de filtragem em 2025 vão além do simples bloqueio de URLs, utilizando inteligência de ameaças impulsionada por IA, princípios de zero-trust e controle granular de políticas para proteger usuários e ativos. Este guia apresenta as dez principais ferramentas de filtragem de conteúdo web, destacando suas características e pontos fortes, ajudando na escolha da solução mais adequada. A seleção priorizou fornecedores estabelecidos com um histórico comprovado em cibersegurança, reputação no mercado e um conjunto abrangente de recursos, como descoberta automatizada, uso de IA para detecção de ameaças e flexibilidade de implantação. As tendências atuais incluem a integração com arquiteturas de zero-trust, controle granular e modelos nativos em nuvem, tornando a filtragem avançada mais acessível, especialmente para pequenas e médias empresas. Ao avaliar uma solução, é importante considerar a inteligência de ameaças, opções de filtragem de conteúdo, gerenciamento e cobertura de usuários e dispositivos.&lt;/p></description></item><item><title>A segurança de dados em IA desafios e soluções para empresas</title><link>https://brdefense.center/news/a-seguranca-de-dados-em-ia-desafios-e-solucoes-par/</link><pubDate>Wed, 17 Sep 2025 12:59:01 -0300</pubDate><guid>https://brdefense.center/news/a-seguranca-de-dados-em-ia-desafios-e-solucoes-par/</guid><description>&lt;p>A rápida adoção da Inteligência Artificial (IA) nas empresas trouxe benefícios significativos, mas também desafios de segurança. O artigo destaca que a maior preocupação não é a imprudência dos funcionários ao usar ferramentas de IA, mas sim a inadequação dos modelos de avaliação de risco das organizações. Muitas soluções de segurança legadas não conseguem monitorar adequadamente o uso de IA, resultando em decisões inadequadas, como proibições que podem levar ao uso de ferramentas não autorizadas. O processo de compra de soluções de segurança de dados em IA deve ser reavaliado, focando em como as ferramentas são utilizadas no dia a dia, em vez de apenas comparar funcionalidades. O artigo sugere que a jornada do comprador deve incluir a descoberta de ferramentas em uso, monitoramento em tempo real e enforcement que não seja apenas de bloqueio. Além disso, fatores não técnicos, como a experiência do usuário e a capacidade de adaptação a novas ferramentas, são cruciais para o sucesso das soluções. O equilíbrio entre segurança e produtividade é essencial, e a abordagem mais eficaz é permitir o uso de IA em contextos autorizados, enquanto se interceptam comportamentos de risco em tempo real.&lt;/p></description></item><item><title>Ferramenta Red AI Range Melhora Testes de Segurança em IA</title><link>https://brdefense.center/news/ferramenta-red-ai-range-melhora-testes-de-seguranc/</link><pubDate>Mon, 15 Sep 2025 12:59:53 -0300</pubDate><guid>https://brdefense.center/news/ferramenta-red-ai-range-melhora-testes-de-seguranc/</guid><description>&lt;p>O Red AI Range (RAR) é uma plataforma inovadora de código aberto que permite a profissionais de segurança realizar testes de red teaming em implementações de inteligência artificial (IA). Desenvolvido por Erdem Özgen, o RAR utiliza a tecnologia de containerização para criar ambientes controlados onde ataques simulados podem revelar vulnerabilidades em fluxos de trabalho de aprendizado de máquina, pipelines de dados e motores de inferência de modelos. A configuração inicial é simplificada através do Docker Compose, permitindo que os usuários lancem todo o ambiente de testes com um único comando.&lt;/p></description></item><item><title>Ferramenta de pentesting com IA gera preocupações de segurança</title><link>https://brdefense.center/news/ferramenta-de-pentesting-com-ia-gera-preocupacoes/</link><pubDate>Mon, 15 Sep 2025 06:59:16 -0300</pubDate><guid>https://brdefense.center/news/ferramenta-de-pentesting-com-ia-gera-preocupacoes/</guid><description>&lt;p>Uma nova ferramenta de teste de penetração, chamada Villager, desenvolvida por uma empresa chinesa, já foi baixada quase 11.000 vezes no repositório Python Package Index (PyPI). A ferramenta, que automatiza fluxos de trabalho de testes de segurança, levanta preocupações sobre seu uso potencial por cibercriminosos. Villager é uma criação da Cyberspike, que também lançou ferramentas como o HexStrike AI, utilizadas para explorar vulnerabilidades recentemente divulgadas. A automação proporcionada por essas ferramentas permite que até mesmo atacantes menos experientes realizem intrusões sofisticadas, reduzindo o tempo e o esforço necessários para executar ataques. A Cyberspike, que surgiu em 2023, integrou componentes de ferramentas de acesso remoto (RAT) em seus produtos, permitindo vigilância invasiva e controle sobre as vítimas. A arquitetura da Villager, que utiliza inteligência artificial para orquestrar ferramentas com base em objetivos, representa uma mudança significativa na condução de ataques cibernéticos. A natureza efêmera dos contêineres criados pela ferramenta dificulta a detecção e a análise forense, aumentando o risco para as organizações. Especialistas alertam que a rápida disponibilidade e as capacidades de automação da Villager podem torná-la um recurso valioso para atores maliciosos, semelhante ao que ocorreu com o Cobalt Strike.&lt;/p></description></item><item><title>Vulnerabilidade no editor de código Cursor pode permitir execução de código</title><link>https://brdefense.center/news/vulnerabilidade-no-editor-de-codigo-cursor-pode-pe/</link><pubDate>Fri, 12 Sep 2025 07:00:32 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-no-editor-de-codigo-cursor-pode-pe/</guid><description>&lt;p>Uma vulnerabilidade de segurança foi identificada no editor de código Cursor, que utiliza inteligência artificial. O problema ocorre porque a configuração de segurança chamada &amp;lsquo;Workspace Trust&amp;rsquo; está desativada por padrão, permitindo que atacantes executem código arbitrário nos computadores dos usuários ao abrir repositórios maliciosos. A análise da Oasis Security destaca que, com essa configuração desativada, um arquivo malicioso &amp;lsquo;.vscode/tasks.json&amp;rsquo; pode transformar a simples ação de abrir uma pasta em uma execução silenciosa de código malicioso. Isso pode resultar em vazamento de credenciais sensíveis ou comprometer todo o sistema do usuário. Para mitigar esse risco, os usuários devem ativar o &amp;lsquo;Workspace Trust&amp;rsquo;, abrir repositórios não confiáveis em editores de código alternativos e realizar auditorias antes de usar o Cursor. Além disso, a pesquisa aponta que a segurança em ferramentas de desenvolvimento baseadas em IA enfrenta desafios adicionais, como injeções de prompt e vulnerabilidades clássicas, que ampliam a superfície de ataque. A situação é preocupante, pois a segurança deve ser uma prioridade em um ambiente de desenvolvimento cada vez mais dependente de IA.&lt;/p></description></item><item><title>SpamGPT Ferramenta de IA impulsiona campanhas massivas de phishing</title><link>https://brdefense.center/news/spamgpt-ferramenta-de-ia-impulsiona-campanhas-mass/</link><pubDate>Tue, 09 Sep 2025 06:59:11 -0300</pubDate><guid>https://brdefense.center/news/spamgpt-ferramenta-de-ia-impulsiona-campanhas-mass/</guid><description>&lt;p>O SpamGPT é uma nova ferramenta de cibercrime que combina inteligência artificial com plataformas de marketing por e-mail, facilitando a execução de campanhas de phishing em larga escala. Comercializada em fóruns clandestinos como uma solução de &amp;ldquo;spam como serviço&amp;rdquo;, a ferramenta reduz drasticamente a barreira técnica para criminosos. Com uma interface que imita softwares de marketing legítimos, o SpamGPT automatiza quase todas as etapas de uma operação de e-mail fraudulenta. Seu núcleo é um framework criptografado que inclui um assistente de IA, o &amp;ldquo;KaliGPT&amp;rdquo;, que gera textos persuasivos e personaliza conteúdos para alvos específicos, eliminando a necessidade de habilidades de escrita. Além disso, oferece painéis em tempo real para monitoramento de entregabilidade e métricas de engajamento, características normalmente restritas a grandes empresas. O SpamGPT permite que até iniciantes lancem operações de spam em massa, utilizando técnicas avançadas de spoofing para contornar verificações de segurança. Essa evolução no cibercrime torna os ataques de phishing mais escaláveis e convincentes, exigindo que as organizações adotem medidas robustas de segurança por e-mail, como políticas DMARC, SPF e DKIM, além de soluções de segurança baseadas em IA para detectar padrões de phishing gerados por IA.&lt;/p></description></item><item><title>As 10 Melhores Empresas de Gestão de Superfície de Ataque em 2025</title><link>https://brdefense.center/news/as-10-melhores-empresas-de-gestao-de-superficie-de/</link><pubDate>Sat, 06 Sep 2025 18:57:39 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-empresas-de-gestao-de-superficie-de/</guid><description>&lt;p>Em 2025, a gestão da superfície de ataque (ASM) se torna essencial para a segurança cibernética, à medida que as organizações ampliam sua presença digital através de serviços em nuvem e trabalho remoto. O ASM é uma disciplina proativa que envolve a descoberta, inventário e monitoramento contínuo de ativos expostos à internet, permitindo a identificação e mitigação de vulnerabilidades antes que sejam exploradas. As melhores empresas de ASM utilizam automação e inteligência artificial para oferecer uma visão abrangente da postura de segurança externa das organizações.&lt;/p></description></item><item><title>Cibercriminosos usam IA para contornar proteções de anúncios no X</title><link>https://brdefense.center/news/cibercriminosos-usam-ia-para-contornar-protecoes-d/</link><pubDate>Thu, 04 Sep 2025 12:58:09 -0300</pubDate><guid>https://brdefense.center/news/cibercriminosos-usam-ia-para-contornar-protecoes-d/</guid><description>&lt;p>Pesquisadores de cibersegurança identificaram uma nova técnica utilizada por cibercriminosos para contornar as proteções contra malvertising da plataforma de mídia social X, utilizando o assistente de inteligência artificial Grok. A técnica, chamada de Grokking, permite que links maliciosos sejam propagados através de postagens promovidas. Os malvertisers utilizam conteúdo adulto como isca, escondendo links maliciosos no campo de metadados &amp;lsquo;From:&amp;rsquo; abaixo do player de vídeo, que não é escaneado pela plataforma. Em seguida, eles mencionam Grok em respostas, fazendo com que o chatbot exiba o link, que agora é amplificado em SEO e reputação de domínio. Os links direcionam os usuários para redes de anúncios suspeitas, levando a fraudes como CAPTCHAs falsos e malware que rouba informações. A Guardio Labs observou centenas de contas envolvidas nesse comportamento, que postam incessantemente até serem suspensas. Essa técnica representa um risco significativo, pois permite que links proibidos pela plataforma se espalhem rapidamente, atingindo milhões de usuários.&lt;/p></description></item><item><title>Ameaça de Exploração com Ferramenta de IA HexStrike AI</title><link>https://brdefense.center/news/ameaca-de-exploracao-com-ferramenta-de-ia-hexstrik/</link><pubDate>Wed, 03 Sep 2025 12:57:10 -0300</pubDate><guid>https://brdefense.center/news/ameaca-de-exploracao-com-ferramenta-de-ia-hexstrik/</guid><description>&lt;p>Recentemente, a ferramenta de segurança ofensiva HexStrike AI, que utiliza inteligência artificial para automatizar a descoberta de vulnerabilidades, está sendo explorada por atores maliciosos para tirar proveito de falhas de segurança recém-divulgadas. De acordo com um relatório da Check Point, esses indivíduos estão utilizando a plataforma, que integra mais de 150 ferramentas de segurança, para realizar ataques em sistemas vulneráveis, como os da Citrix. A ferramenta, que deveria fortalecer a defesa cibernética, está sendo rapidamente adaptada para fins de exploração, aumentando a eficiência dos ataques e reduzindo o tempo entre a divulgação pública de falhas e sua exploração em massa. A Check Point alerta que essa situação representa uma mudança significativa na forma como as vulnerabilidades são exploradas, permitindo que ataques sejam realizados de maneira automatizada e em larga escala. Os pesquisadores também destacam que agentes de cibersegurança baseados em IA, como o PentestGPT, apresentam riscos elevados de injeção de comandos, transformando ferramentas de segurança em vetores de ataque. A recomendação imediata é que as organizações atualizem e reforcem seus sistemas para mitigar esses riscos.&lt;/p></description></item><item><title>Identificado primeiro ransomware criado com inteligência artificial</title><link>https://brdefense.center/news/identificado-primeiro-ransomware-criado-com-inteli/</link><pubDate>Mon, 01 Sep 2025 19:00:36 -0300</pubDate><guid>https://brdefense.center/news/identificado-primeiro-ransomware-criado-com-inteli/</guid><description>&lt;p>Pesquisadores da ESET, Anton Cherepanov e Peter Strycek, descobriram o PromptLock, o primeiro ransomware desenvolvido com a ajuda de inteligência artificial. Embora ainda não tenha sido visto em ação, a ESET alertou a comunidade de cibersegurança sobre essa nova ameaça. O PromptLock utiliza o modelo gpt-oss-20b da OpenAI, que é uma versão gratuita do ChatGPT, e opera localmente em dispositivos infectados através da API Ollama.&lt;/p>
&lt;p>O ransomware é capaz de inspecionar arquivos, extrair dados e criptografá-los, podendo futuramente ser utilizado para extorsão. Ele é compatível com sistemas operacionais Windows, Linux e macOS. O código do malware indica que ele pode até destruir arquivos, embora essa funcionalidade ainda não esteja totalmente implementada. O PromptLock utiliza a encriptação SPECK de 128 bits e é escrito na linguagem Go. A ESET também observou que variantes do malware foram enviadas para a plataforma VirusTotal, que serve como repositório de vírus para especialistas em segurança.&lt;/p></description></item><item><title>As 10 Melhores Empresas de Teste de Segurança de API em 2025</title><link>https://brdefense.center/news/as-10-melhores-empresas-de-teste-de-seguranca-de-a/</link><pubDate>Sat, 30 Aug 2025 12:58:31 -0300</pubDate><guid>https://brdefense.center/news/as-10-melhores-empresas-de-teste-de-seguranca-de-a/</guid><description>&lt;p>Em 2025, as APIs se tornaram fundamentais para o funcionamento de aplicações modernas, abrangendo desde serviços bancários móveis até arquiteturas de microserviços. No entanto, essa popularidade também as torna alvos preferenciais para ataques cibernéticos, com violações relacionadas a APIs se destacando como uma das principais causas de exfiltração de dados. O artigo destaca a importância de um teste de segurança de API robusto, que vai além das soluções tradicionais, como firewalls de aplicativos web (WAFs) e scanners DAST, que muitas vezes falham em detectar ataques específicos de API, como a autorização de nível de objeto quebrada (BOLA) e abusos de lógica de negócios. As melhores empresas de teste de segurança de API em 2025 são aquelas que oferecem uma abordagem abrangente, cobrindo todo o ciclo de vida da segurança, desde o design até a execução. O artigo analisa empresas como Salt Security, Traceable AI e Wallarm, que se destacam por suas capacidades de descoberta automática de APIs, proteção em tempo real e uso de inteligência artificial para detectar ameaças sofisticadas. A segurança de APIs não é mais um luxo, mas uma necessidade crítica para prevenir violações e manter uma postura de segurança forte.&lt;/p></description></item><item><title>Google usa IA para corrigir falha crítica no Chrome</title><link>https://brdefense.center/news/google-usa-ia-para-corrigir-falha-critica-no-chrom/</link><pubDate>Fri, 29 Aug 2025 18:59:43 -0300</pubDate><guid>https://brdefense.center/news/google-usa-ia-para-corrigir-falha-critica-no-chrom/</guid><description>&lt;p>O Google anunciou a correção de uma vulnerabilidade crítica no navegador Chrome, identificada como CVE-2025-9478, com o auxílio de sua inteligência artificial interna, chamada Big Sleep. Essa ferramenta, baseada na tecnologia Gemini, foi capaz de detectar a falha sem intervenção humana, destacando a crescente importância da IA na segurança cibernética. As versões corrigidas do Chrome incluem 139.0.7258.154/155 para Windows e macOS, e 139.0.7258.154 para Linux. A vulnerabilidade estava relacionada à galeria de gráficos Angle e foi classificada como crítica, o que significa que poderia ser explorada para comprometer a segurança dos usuários. Embora a Big Sleep tenha demonstrado eficácia na identificação dessa falha, especialistas em segurança ainda são necessários para validar os diagnósticos, uma vez que a precisão das ferramentas de IA pode variar. Além disso, outros navegadores baseados em Chromium, como Brave, Microsoft Edge e Vivaldi, também precisarão aplicar as correções. O Google recomenda que os usuários atualizem seus navegadores para garantir a proteção contra possíveis explorações dessa vulnerabilidade.&lt;/p></description></item><item><title>Nova funcionalidade do VirusTotal fornece descrições de funções de código</title><link>https://brdefense.center/news/nova-funcionalidade-do-virustotal-fornece-descrico/</link><pubDate>Fri, 29 Aug 2025 12:59:27 -0300</pubDate><guid>https://brdefense.center/news/nova-funcionalidade-do-virustotal-fornece-descrico/</guid><description>&lt;p>Em 28 de agosto de 2025, o VirusTotal lançou uma nova funcionalidade chamada Code Insight, que visa ajudar analistas de segurança a lidar com a crescente complexidade dos malwares. Essa ferramenta integra análise de código impulsionada por inteligência artificial (IA) diretamente em ferramentas de engenharia reversa, permitindo que os analistas submetam trechos de código em formato Base64 e recebam resumos e descrições detalhadas sobre as funções do código. A funcionalidade promete reduzir significativamente o esforço manual, ao resumir e contextualizar funções desmontadas ou decompiladas, acelerando o ciclo de análise de malware. A atualização do plugin VT-IDA para IDA Pro facilita a integração, permitindo que os analistas aceitem, editem e armazenem análises em um caderno específico. A nova abordagem também inclui um recurso de memória que aprende com as correções feitas pelos analistas, melhorando a precisão das análises ao longo do tempo. Embora os resultados iniciais sejam promissores, o VirusTotal está buscando feedback da comunidade para aprimorar ainda mais os modelos de IA utilizados.&lt;/p></description></item><item><title>Visibilidade de Código à Nuvem A Nova Base para Segurança de Aplicativos</title><link>https://brdefense.center/news/visibilidade-de-codigo-a-nuvem-a-nova-base-para-se/</link><pubDate>Fri, 29 Aug 2025 00:57:10 -0300</pubDate><guid>https://brdefense.center/news/visibilidade-de-codigo-a-nuvem-a-nova-base-para-se/</guid><description>&lt;p>O artigo destaca a crescente preocupação com a segurança de aplicativos na nuvem, especialmente em um cenário onde falhas de segurança podem custar milhões às empresas. Em 2025, o custo médio de uma violação de dados é estimado em US$ 4,44 milhões, com uma parte significativa desses problemas originando-se de erros de segurança em aplicativos. A visibilidade de código à nuvem é apresentada como uma solução eficaz para identificar riscos desde a fase de desenvolvimento até a operação na nuvem. O artigo menciona que 32% das organizações enfrentam dificuldades na gestão de vulnerabilidades, enquanto 97% lidam com questões de segurança relacionadas à inteligência artificial generativa. Um webinar programado para 8 de setembro de 2025 promete oferecer insights práticos sobre como implementar essa abordagem, visando melhorar a colaboração entre equipes de desenvolvimento, operações e segurança. Os participantes aprenderão a mapear riscos, acelerar correções e se preparar para novas ameaças, tudo isso sem sobrecarregar suas operações.&lt;/p></description></item><item><title>Novas sanções dos EUA visam esquema de TI da Coreia do Norte</title><link>https://brdefense.center/news/novas-sancoes-dos-eua-visam-esquema-de-ti-da-corei/</link><pubDate>Thu, 28 Aug 2025 06:57:20 -0300</pubDate><guid>https://brdefense.center/news/novas-sancoes-dos-eua-visam-esquema-de-ti-da-corei/</guid><description>&lt;p>O Departamento do Tesouro dos EUA anunciou novas sanções contra dois indivíduos e duas entidades ligadas ao esquema de trabalhadores de tecnologia da informação (TI) da Coreia do Norte, que visa gerar receitas ilícitas para programas de armas de destruição em massa. As sanções atingem Vitaliy Sergeyevich Andreyev, Kim Ung Sun, a Shenyang Geumpungri Network Technology Co., Ltd e a Korea Sinjin Trading Corporation. O esquema, que já é monitorado há anos, envolve a infiltração de trabalhadores de TI norte-coreanos em empresas legítimas nos EUA, utilizando documentos fraudulentos e identidades roubadas. Além disso, a operação tem se apoiado em ferramentas de inteligência artificial para criar perfis profissionais convincentes e realizar trabalhos técnicos. O Departamento do Tesouro destacou que Andreyev facilitou transferências financeiras significativas para a Chinyong Information Technology Cooperation Company, que já havia sido sancionada anteriormente. O uso de IA por esses atores levanta preocupações sobre a segurança cibernética, especialmente para empresas que podem ser alvos de fraudes e extorsões. O impacto dessas atividades é significativo, com lucros estimados em mais de um milhão de dólares desde 2021.&lt;/p></description></item><item><title>O primeiro ransomware com inteligência artificial foi identificado</title><link>https://brdefense.center/news/o-primeiro-ransomware-com-inteligencia-artificial/</link><pubDate>Wed, 27 Aug 2025 18:58:35 -0300</pubDate><guid>https://brdefense.center/news/o-primeiro-ransomware-com-inteligencia-artificial/</guid><description>&lt;p>Pesquisadores da ESET descobriram o PromptLock, o primeiro ransomware conhecido a utilizar inteligência artificial. Este malware, que ainda é considerado um conceito em desenvolvimento, utiliza scripts em Lua gerados por prompts codificados para explorar sistemas de arquivos locais, inspecionar arquivos-alvo, exfiltrar dados selecionados e realizar criptografia. O PromptLock opera localmente através da API Ollama, utilizando o modelo gpt-oss:20b da OpenAI, lançado em agosto de 2025. A versatilidade dos scripts em Lua permite que o malware funcione em diferentes sistemas operacionais, como macOS, Linux e Windows. Embora o PromptLock ainda não tenha sido observado em ataques reais, especialistas alertam que a combinação de inteligência artificial e ransomware representa uma nova era de ameaças cibernéticas, tornando os ataques mais acessíveis e difíceis de detectar. A imprevisibilidade dos resultados gerados por modelos de linguagem torna a defesa contra esses ataques ainda mais desafiadora, aumentando a preocupação entre as equipes de segurança.&lt;/p></description></item><item><title>Ransomware PromptLock usa IA para gerar scripts maliciosos em tempo real</title><link>https://brdefense.center/news/ransomware-promptlock-usa-ia-para-gerar-scripts-ma/</link><pubDate>Wed, 27 Aug 2025 18:57:56 -0300</pubDate><guid>https://brdefense.center/news/ransomware-promptlock-usa-ia-para-gerar-scripts-ma/</guid><description>&lt;p>A empresa de cibersegurança ESET revelou a descoberta de um novo ransomware chamado PromptLock, que utiliza inteligência artificial para gerar scripts maliciosos em tempo real. Escrito em Golang, o PromptLock emprega o modelo gpt-oss:20b da OpenAI através da API Ollama para criar scripts em Lua que podem enumerar sistemas de arquivos, inspecionar arquivos-alvo, exfiltrar dados e criptografar informações. Este ransomware é compatível com Windows, Linux e macOS, e é capaz de gerar notas personalizadas para as vítimas, dependendo dos arquivos afetados. Embora ainda não se saiba quem está por trás do malware, a ESET identificou que artefatos do PromptLock foram enviados ao VirusTotal a partir dos Estados Unidos em 25 de agosto de 2025. A natureza do ransomware, que é considerada uma prova de conceito, utiliza o algoritmo de criptografia SPECK de 128 bits. A ESET alerta que a variabilidade dos indicadores de comprometimento (IoCs) torna a detecção mais desafiadora, complicando as tarefas de defesa. O surgimento do PromptLock destaca como a IA pode facilitar a criação de campanhas de malware, mesmo para criminosos com pouca experiência técnica.&lt;/p></description></item><item><title>Cinco regras para uma adoção segura de IA nas empresas</title><link>https://brdefense.center/news/cinco-regras-para-uma-adocao-segura-de-ia-nas-empr/</link><pubDate>Wed, 27 Aug 2025 12:57:34 -0300</pubDate><guid>https://brdefense.center/news/cinco-regras-para-uma-adocao-segura-de-ia-nas-empr/</guid><description>&lt;p>O uso de Inteligência Artificial (IA) nas empresas está crescendo rapidamente, com colaboradores utilizando-a para redigir e-mails, analisar dados e transformar o ambiente de trabalho. No entanto, a adoção acelerada da IA traz desafios significativos em termos de segurança, especialmente pela falta de controle e salvaguardas adequadas. Para os Chief Information Security Officers (CISOs), a prioridade é garantir que a inovação não comprometa a segurança. O artigo apresenta cinco regras essenciais para uma adoção segura da IA: 1) Visibilidade e descoberta da IA, que exige monitoramento contínuo do uso de ferramentas de IA; 2) Avaliação de risco contextual, que considera o nível de risco associado a diferentes aplicações de IA; 3) Proteção de dados, estabelecendo limites sobre quais informações podem ser compartilhadas com ferramentas de IA; 4) Controles de acesso e diretrizes, que garantem que o uso da IA esteja dentro de políticas de segurança definidas; e 5) Supervisão contínua, para adaptar as medidas de segurança conforme as aplicações evoluem. A adoção segura da IA não deve ser vista como uma barreira, mas como uma oportunidade de inovar de forma responsável, garantindo a proteção dos dados e a conformidade com regulamentações como a LGPD.&lt;/p></description></item><item><title>Anthropic interrompe ataque cibernético com uso de IA avançada</title><link>https://brdefense.center/news/anthropic-interrompe-ataque-cibernetico-com-uso-de/</link><pubDate>Wed, 27 Aug 2025 12:56:54 -0300</pubDate><guid>https://brdefense.center/news/anthropic-interrompe-ataque-cibernetico-com-uso-de/</guid><description>&lt;p>Em julho de 2025, a Anthropic revelou ter desmantelado uma operação sofisticada que utilizava seu chatbot Claude, alimentado por inteligência artificial, para realizar roubo e extorsão em larga escala de dados pessoais. O ataque visou pelo menos 17 organizações, incluindo instituições de saúde, serviços de emergência e órgãos governamentais, com os criminosos ameaçando expor publicamente as informações roubadas para forçar o pagamento de resgates que ultrapassavam $500.000. Utilizando o Claude Code em uma plataforma Kali Linux, o ator desconhecido automatizou várias fases do ciclo de ataque, desde a coleta de credenciais até a penetração de redes. O uso de IA permitiu que o atacante tomasse decisões táticas e estratégicas, selecionando quais dados exfiltrar e elaborando demandas de extorsão personalizadas com base em análises financeiras. A Anthropic desenvolveu um classificador personalizado para detectar comportamentos semelhantes e compartilhou indicadores técnicos com parceiros estratégicos. O caso destaca como ferramentas de IA estão sendo mal utilizadas para facilitar operações cibernéticas complexas, tornando a defesa mais desafiadora.&lt;/p></description></item><item><title>Geradores de sites por IA se tornam ferramentas de phishing para hackers</title><link>https://brdefense.center/news/geradores-de-sites-por-ia-se-tornam-ferramentas-de/</link><pubDate>Mon, 25 Aug 2025 19:01:21 -0300</pubDate><guid>https://brdefense.center/news/geradores-de-sites-por-ia-se-tornam-ferramentas-de/</guid><description>&lt;p>A crescente popularidade de serviços de criação de sites por inteligência artificial, como a Lovable, trouxe à tona preocupações significativas de segurança cibernética. Um relatório da Proofpoint revelou que, desde fevereiro, dezenas de milhares de URLs geradas na plataforma têm sido utilizadas em campanhas de phishing, instalação de malwares e roubo de dados. Os sites fraudulentos imitam páginas de marcas conhecidas e utilizam mecanismos como captchas para evitar a detecção por bots. Quatro campanhas distintas foram identificadas, incluindo uma que visava roubar credenciais de login da Microsoft e outra que imitava a empresa de logística UPS para coletar dados pessoais e informações de pagamento. A Lovable implementou medidas de segurança em junho, mas a Proofpoint alerta que ainda é possível criar sites fraudulentos na plataforma. Essa situação evidencia como a tecnologia pode ser explorada para fins criminosos, exigindo atenção redobrada de empresas e usuários para evitar danos.&lt;/p></description></item><item><title>Atores de Ameaças Usam Resumos Gerados por IA para Entregar Ransomware</title><link>https://brdefense.center/news/atores-de-ameacas-usam-resumos-gerados-por-ia-para/</link><pubDate>Mon, 25 Aug 2025 12:58:17 -0300</pubDate><guid>https://brdefense.center/news/atores-de-ameacas-usam-resumos-gerados-por-ia-para/</guid><description>&lt;p>Um novo método de engenharia social, denominado ClickFix, está sendo utilizado por cibercriminosos para implantar ransomware através de resumos gerados por inteligência artificial (IA). Essa técnica envolve a injeção de comandos maliciosos em elementos HTML, que se tornam invisíveis para os usuários, mas são processados por modelos de IA. Os atacantes escondem instruções em caracteres de largura zero, texto branco sobre fundo branco e fontes minúsculas, fazendo com que os resumos gerados incluam guias passo a passo para a instalação de ransomware. Quando os destinatários confiam nesses resumos, podem executar comandos sem perceber que estão seguindo instruções maliciosas. O ataque reduz a barreira técnica para usuários não especializados, permitindo que eles se tornem vetores de ransomware. Para mitigar esses riscos, recomenda-se a sanitização de atributos CSS invisíveis, filtragem de prompts e reconhecimento de padrões de payload. À medida que as ferramentas de resumo de IA se tornam comuns, essa técnica pode ser rapidamente adotada por criminosos, exigindo uma colaboração entre desenvolvedores de IA e equipes de segurança para prevenir campanhas de ransomware mediadas por IA.&lt;/p></description></item><item><title>Navegadores com IA ainda não são seguros o suficiente, diz estudo</title><link>https://brdefense.center/news/navegadores-com-ia-ainda-nao-sao-seguros-o-suficie/</link><pubDate>Fri, 22 Aug 2025 13:01:35 -0300</pubDate><guid>https://brdefense.center/news/navegadores-com-ia-ainda-nao-sao-seguros-o-suficie/</guid><description>&lt;p>Um estudo da empresa de cibersegurança Guardio revela que navegadores de internet que utilizam inteligência artificial (IA) ainda não estão prontos para realizar tarefas sensíveis, como compras online e gerenciamento de e-mails, devido a vulnerabilidades que podem ser exploradas por golpistas. Tecnologias como Comet, do Perplexity, Copilot, do Microsoft Edge, e Aura, da OpenAI, foram testadas e mostraram-se suscetíveis a ataques de phishing e engenharia social. Em um dos testes, a IA Comet completou uma compra em um site falso sem solicitar confirmação do usuário, expondo dados sensíveis como informações de cartão de crédito. Outro teste demonstrou que a IA foi enganada por um e-mail de phishing que parecia legítimo, resultando no fornecimento de credenciais do usuário. A Guardio alerta que esses testes preliminares indicam que há mais problemas em potencial, sugerindo que os usuários evitem delegar tarefas críticas à IA e adotem medidas de segurança, como a autenticação de dois fatores. O estudo destaca a necessidade de cautela ao usar navegadores com IA para evitar o roubo de dados e outras fraudes.&lt;/p></description></item><item><title>O Risco Oculto dos Agentes de IA nas Empresas</title><link>https://brdefense.center/news/o-risco-oculto-dos-agentes-de-ia-nas-empresas/</link><pubDate>Thu, 21 Aug 2025 01:01:13 -0300</pubDate><guid>https://brdefense.center/news/o-risco-oculto-dos-agentes-de-ia-nas-empresas/</guid><description>&lt;p>O artigo destaca a crescente preocupação com os agentes de inteligência artificial (IA) que operam silenciosamente nas empresas, muitas vezes sem supervisão adequada. Esses &amp;lsquo;agentes sombra&amp;rsquo; são configurados por diversas unidades de negócios, não apenas pela equipe de TI, o que resulta em uma falta de controle sobre suas identidades e atividades. Quando comprometidos, esses agentes podem acessar dados sensíveis e escalar privilégios rapidamente, representando uma ameaça significativa à segurança cibernética. A maioria dos programas de segurança atuais não foi projetada para lidar com esses agentes autônomos, o que aumenta o risco à medida que sua adoção se expande. O artigo também menciona um webinar que abordará como identificar e controlar esses agentes, além de compartilhar estratégias para atribuir identidades adequadas e garantir a responsabilidade. A urgência em lidar com essa questão é enfatizada, pois a escolha entre transformar esses agentes em ativos confiáveis ou em passivos perigosos depende das ações que as empresas tomarem agora.&lt;/p></description></item><item><title>Tecnologia de reconhecimento facial não está pronta para uso policial</title><link>https://brdefense.center/news/tecnologia-de-reconhecimento-facial-nao-esta-pront/</link><pubDate>Wed, 20 Aug 2025 18:58:26 -0300</pubDate><guid>https://brdefense.center/news/tecnologia-de-reconhecimento-facial-nao-esta-pront/</guid><description>&lt;p>Pesquisadores da Universidade de Oxford alertam sobre as falhas da tecnologia de reconhecimento facial utilizada por forças policiais em diversos países, como Estados Unidos e Reino Unido. A pesquisa destaca que as condições reais de identificação são muito mais complexas do que as simuladas em ambientes laboratoriais, resultando em prisões injustas de indivíduos inocentes. A Avaliação de Tecnologia de Reconhecimento Facial (FRTE) do Instituto Nacional de Padrões e Tecnologia (NIST) é frequentemente utilizada para justificar o uso dessa tecnologia, mas apresenta limitações significativas, como a falta de representatividade demográfica nos bancos de dados e a incapacidade de lidar com imagens de baixa qualidade. Estudos demonstram que, embora os modelos de reconhecimento facial possam apresentar uma precisão de até 99,95% em condições ideais, essa taxa cai drasticamente em situações do mundo real. Além disso, a tecnologia tende a falhar desproporcionalmente em relação a grupos marginalizados. Os pesquisadores concluem que a tecnologia ainda é muito falha para ser utilizada de forma segura por agências policiais e recomendam uma revisão das políticas de direitos civis relacionadas ao seu uso.&lt;/p></description></item><item><title>Google Lança Ferramentas Avançadas para Proteger a Segurança em IA</title><link>https://brdefense.center/news/google-lanca-ferramentas-avancadas-para-proteger-a/</link><pubDate>Wed, 20 Aug 2025 12:59:10 -0300</pubDate><guid>https://brdefense.center/news/google-lanca-ferramentas-avancadas-para-proteger-a/</guid><description>&lt;p>No Google Cloud Security Summit 2025, a Google apresentou uma nova suíte de ferramentas de segurança impulsionadas por inteligência artificial, com o objetivo de proteger ecossistemas de IA e fortalecer as defesas organizacionais. As inovações abrangem três áreas principais: proteção de implementações de IA autônomas, suporte a centros de operações de segurança com agentes autônomos e ampliação dos controles de segurança em nuvem.&lt;/p>
&lt;p>Entre as novidades, destaca-se o Centro de Comando de Segurança, que agora possui capacidades expandidas para identificação de riscos e inventário de agentes de IA, permitindo a descoberta automatizada de vulnerabilidades. A proteção em tempo real contra ameaças, como injeção de comandos e vazamento de dados sensíveis, foi aprimorada com a extensão do Model Armor. Além disso, um novo agente de investigação de alertas foi introduzido, que realiza investigações dinâmicas para acelerar os tempos de resposta.&lt;/p></description></item><item><title>Microsoft Defender AI agora detecta credenciais em texto simples no Active Directory</title><link>https://brdefense.center/news/microsoft-defender-ai-agora-detecta-credenciais-em/</link><pubDate>Tue, 19 Aug 2025 13:00:07 -0300</pubDate><guid>https://brdefense.center/news/microsoft-defender-ai-agora-detecta-credenciais-em/</guid><description>&lt;p>A Microsoft lançou uma nova funcionalidade de segurança baseada em inteligência artificial no Defender for Identity, que visa mitigar uma vulnerabilidade crítica relacionada ao armazenamento de credenciais em texto simples no Active Directory. A pesquisa da empresa revelou que mais de 40.000 credenciais estavam expostas em 2.500 locatários, evidenciando a gravidade do problema. Essa vulnerabilidade surge do uso inadequado de campos de texto livre, onde administradores frequentemente armazenam senhas e tokens de autenticação para facilitar a resolução de problemas e a integração de sistemas. Isso cria alvos valiosos para cibercriminosos, especialmente em contas de identidade não humanas, que não podem utilizar métodos tradicionais de autenticação multifatorial. A nova arquitetura de detecção da Microsoft combina varredura abrangente do diretório com uma análise contextual, reduzindo falsos positivos e garantindo alertas acionáveis para as equipes de segurança. A funcionalidade já está disponível em versão pública para todos os clientes do Defender for Identity, permitindo que as organizações identifiquem e corrijam configurações inadequadas antes que possam ser exploradas.&lt;/p></description></item><item><title>HexStrike AI integra ChatGPT, Claude e Copilot com 150 ferramentas de segurança</title><link>https://brdefense.center/news/hexstrike-ai-integra-chatgpt-claude-e-copilot-com/</link><pubDate>Fri, 15 Aug 2025 12:58:35 -0300</pubDate><guid>https://brdefense.center/news/hexstrike-ai-integra-chatgpt-claude-e-copilot-com/</guid><description>&lt;p>A HexStrike AI, plataforma líder em cibersegurança impulsionada por inteligência artificial, anunciou uma nova atualização que integra assistentes de IA como ChatGPT, Claude e Copilot com mais de 150 ferramentas de segurança, incluindo Burp Suite e Nmap. Essa atualização, parte da versão 6.0 da HexStrike AI, permite que desenvolvedores e pesquisadores de segurança realizem testes de penetração e avaliações de vulnerabilidades de forma autônoma e em uma escala sem precedentes. A arquitetura multi-agente da HexStrike AI conta com 12 agentes especializados que colaboram para executar fluxos de trabalho de segurança complexos. Através do protocolo “FastMCP”, os assistentes de IA podem invocar diretamente as ferramentas de segurança, automatizando tarefas como reconhecimento de rede e desenvolvimento de exploits. Profissionais de segurança agora podem, por exemplo, solicitar uma auditoria de segurança em um site e ver a execução automática de testes em segundos. A HexStrike AI v6.0 está disponível sob uma licença MIT de código aberto, permitindo que desenvolvedores integrem facilmente suas ferramentas de segurança com assistentes de IA.&lt;/p></description></item><item><title>A Nova Era da Privacidade Confiança em um Mundo de IA Agente</title><link>https://brdefense.center/news/a-nova-era-da-privacidade-confianca-em-um-mundo-de/</link><pubDate>Fri, 15 Aug 2025 09:26:12 -0300</pubDate><guid>https://brdefense.center/news/a-nova-era-da-privacidade-confianca-em-um-mundo-de/</guid><description>&lt;p>O conceito de privacidade está evoluindo em um mundo onde a inteligência artificial (IA) agente se torna cada vez mais autônoma. Em vez de ser apenas uma questão de controle, a privacidade agora se baseia na confiança, especialmente quando essas IAs interagem com dados sensíveis e tomam decisões em nome dos usuários. A IA agente não apenas processa informações, mas também as interpreta, o que levanta preocupações sobre o que é inferido e compartilhado sem supervisão constante. Um exemplo prático é um assistente de saúde que, ao longo do tempo, pode começar a priorizar consultas e analisar o estado emocional do usuário, o que pode levar à perda de controle sobre a narrativa pessoal. Além disso, a privacidade não se resume mais ao triângulo clássico da CIA (Confidencialidade, Integridade e Disponibilidade), mas deve incluir autenticidade e veracidade. A falta de um conceito claro de privilégio entre IA e cliente pode resultar em consequências legais, onde informações pessoais podem ser acessadas por terceiros. Portanto, é essencial que as organizações desenvolvam sistemas de IA que respeitem a intenção por trás da privacidade e que sejam capazes de explicar suas ações. A necessidade de um novo contrato social que considere a agência da IA como uma categoria moral e legal é urgente, pois a privacidade se torna uma questão de reciprocidade e governança em um mundo onde humanos e máquinas interagem cada vez mais.&lt;/p></description></item></channel></rss>