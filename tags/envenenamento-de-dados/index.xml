<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Envenenamento De Dados on BR Defense Center</title><link>https://brdefense.center/tags/envenenamento-de-dados/</link><description>Recent content in Envenenamento De Dados on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Fri, 24 Oct 2025 19:00:25 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/envenenamento-de-dados/index.xml" rel="self" type="application/rss+xml"/><item><title>Hackers envenenam IA como ChatGPT com facilidade</title><link>https://brdefense.center/news/hackers-envenenam-ia-como-chatgpt-com-facilidade/</link><pubDate>Fri, 24 Oct 2025 19:00:25 -0300</pubDate><guid>https://brdefense.center/news/hackers-envenenam-ia-como-chatgpt-com-facilidade/</guid><description>&lt;p>O aumento do uso de Inteligência Artificial (IA), especialmente em chatbots como ChatGPT, Gemini e Claude, trouxe à tona novas vulnerabilidades. Pesquisadores do Instituto Alan Turing e da Anthropic identificaram uma técnica chamada &amp;rsquo;envenenamento de IA&amp;rsquo;, onde apenas 250 arquivos maliciosos podem comprometer o aprendizado de um modelo de dados. Isso resulta em chatbots que aprendem informações erradas, levando a respostas incorretas ou até maliciosas. Existem dois tipos principais de envenenamento: o direto, que altera respostas específicas, e o indireto, que prejudica a performance geral. Um exemplo de ataque direto é o backdoor, onde o modelo é manipulado para responder de forma errada ao detectar um código específico. Já o ataque indireto envolve a criação de informações falsas que o modelo replica como verdade. Em março de 2023, a OpenAI suspendeu temporariamente o ChatGPT devido a um bug que expôs dados de usuários. Além disso, artistas têm utilizado dados envenenados como uma forma de proteger suas obras contra sistemas de IA que as utilizam sem autorização. Essa situação levanta preocupações sobre a segurança e a integridade dos sistemas de IA, especialmente em um cenário onde a desinformação pode ter consequências graves.&lt;/p></description></item></channel></rss>