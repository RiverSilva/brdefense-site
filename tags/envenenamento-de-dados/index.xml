<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Envenenamento De Dados on BR Defense Center</title><link>https://brdefense.center/tags/envenenamento-de-dados/</link><description>Recent content in Envenenamento De Dados on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Thu, 11 Dec 2025 18:59:17 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/envenenamento-de-dados/index.xml" rel="self" type="application/rss+xml"/><item><title>Golpistas envenenam buscas de IA com números de suporte falsos</title><link>https://brdefense.center/news/golpistas-envenenam-buscas-de-ia-com-numeros-de-su/</link><pubDate>Thu, 11 Dec 2025 18:59:17 -0300</pubDate><guid>https://brdefense.center/news/golpistas-envenenam-buscas-de-ia-com-numeros-de-su/</guid><description>&lt;p>Cibercriminosos estão utilizando técnicas de envenenamento de IA para promover números falsos de suporte ao cliente em fontes públicas acessadas por chatbots. Um estudo da Aurascape revelou que essa manipulação, chamada de &amp;ldquo;envenenamento de números de telefone de LLM&amp;rdquo;, afeta modelos de linguagem como a Visão Geral da Google e o navegador Comet da Perplexity. A técnica, que se assemelha à otimização de motores de busca (SEO), visa garantir que sites fraudulentos sejam utilizados como fontes de informação por assistentes de IA. Isso é feito ao comprometer sites legítimos, como os de instituições governamentais e universidades, e ao abusar de plataformas que permitem conteúdo gerado por usuários, como YouTube e Yelp. Os pesquisadores destacam que a dificuldade em distinguir entre informações legítimas e fraudulentas pode levar a usuários a entrarem em contato com call centers falsos, como demonstrado em casos envolvendo as companhias aéreas Emirates e British Airlines. A recomendação é que os usuários verifiquem a veracidade das informações e evitem compartilhar dados sensíveis com assistentes de IA, que ainda não foram amplamente testados em termos de segurança.&lt;/p></description></item><item><title>Hackers envenenam IA como ChatGPT com facilidade</title><link>https://brdefense.center/news/hackers-envenenam-ia-como-chatgpt-com-facilidade/</link><pubDate>Fri, 24 Oct 2025 19:00:25 -0300</pubDate><guid>https://brdefense.center/news/hackers-envenenam-ia-como-chatgpt-com-facilidade/</guid><description>&lt;p>O aumento do uso de Inteligência Artificial (IA), especialmente em chatbots como ChatGPT, Gemini e Claude, trouxe à tona novas vulnerabilidades. Pesquisadores do Instituto Alan Turing e da Anthropic identificaram uma técnica chamada &amp;rsquo;envenenamento de IA&amp;rsquo;, onde apenas 250 arquivos maliciosos podem comprometer o aprendizado de um modelo de dados. Isso resulta em chatbots que aprendem informações erradas, levando a respostas incorretas ou até maliciosas. Existem dois tipos principais de envenenamento: o direto, que altera respostas específicas, e o indireto, que prejudica a performance geral. Um exemplo de ataque direto é o backdoor, onde o modelo é manipulado para responder de forma errada ao detectar um código específico. Já o ataque indireto envolve a criação de informações falsas que o modelo replica como verdade. Em março de 2023, a OpenAI suspendeu temporariamente o ChatGPT devido a um bug que expôs dados de usuários. Além disso, artistas têm utilizado dados envenenados como uma forma de proteger suas obras contra sistemas de IA que as utilizam sem autorização. Essa situação levanta preocupações sobre a segurança e a integridade dos sistemas de IA, especialmente em um cenário onde a desinformação pode ter consequências graves.&lt;/p></description></item></channel></rss>