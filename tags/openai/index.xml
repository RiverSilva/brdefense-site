<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Openai on BR Defense Center</title><link>https://brdefense.center/tags/openai/</link><description>Recent content in Openai on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Fri, 31 Oct 2025 18:57:22 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/openai/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenAI lança Aardvark, pesquisador de segurança autônomo com IA</title><link>https://brdefense.center/news/openai-lanca-aardvark-pesquisador-de-seguranca-aut/</link><pubDate>Fri, 31 Oct 2025 18:57:22 -0300</pubDate><guid>https://brdefense.center/news/openai-lanca-aardvark-pesquisador-de-seguranca-aut/</guid><description>&lt;p>A OpenAI anunciou o lançamento do Aardvark, um pesquisador de segurança autônomo alimentado pelo modelo de linguagem GPT-5. Este agente de inteligência artificial foi projetado para ajudar desenvolvedores e equipes de segurança a identificar e corrigir vulnerabilidades em código de forma escalável. Atualmente em beta privada, o Aardvark analisa repositórios de código-fonte continuamente, identificando vulnerabilidades, avaliando sua explorabilidade e propondo correções. O modelo GPT-5, introduzido em agosto de 2025, oferece capacidades de raciocínio mais profundas e um &amp;lsquo;roteador em tempo real&amp;rsquo; para otimizar a interação com os usuários.&lt;/p></description></item><item><title>Navegador Atlas da OpenAI é Jailbroken para Ocultar Prompts Maliciosos</title><link>https://brdefense.center/news/navegador-atlas-da-openai-e-jailbroken-para-oculta/</link><pubDate>Mon, 27 Oct 2025 13:01:34 -0300</pubDate><guid>https://brdefense.center/news/navegador-atlas-da-openai-e-jailbroken-para-oculta/</guid><description>&lt;p>Pesquisadores de cibersegurança da NeuralTrust descobriram uma vulnerabilidade crítica no navegador Atlas da OpenAI, que permite a atacantes disfarçar instruções maliciosas como URLs legítimas, burlando os controles de segurança do sistema. A falha explora a omnibox, a barra de endereço e pesquisa combinada, manipulando a forma como o Atlas distingue entre solicitações de navegação e comandos em linguagem natural.&lt;/p>
&lt;p>Os atacantes criam strings que parecem URLs válidas, mas que contêm erros sutis de formatação. Quando um usuário insere essas strings na omnibox, o Atlas não valida corretamente a URL e trata o conteúdo como um comando confiável, permitindo que instruções maliciosas sejam executadas com privilégios elevados.&lt;/p></description></item><item><title>Vulnerabilidade no ChatGPT Atlas permite injeção de código malicioso</title><link>https://brdefense.center/news/vulnerabilidade-no-chatgpt-atlas-permite-injecao-d/</link><pubDate>Mon, 27 Oct 2025 12:58:45 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-no-chatgpt-atlas-permite-injecao-d/</guid><description>&lt;p>Pesquisadores de cibersegurança identificaram uma nova vulnerabilidade no navegador ChatGPT Atlas da OpenAI, que pode permitir que agentes maliciosos injetem instruções prejudiciais na memória do assistente de inteligência artificial. Essa falha, baseada em um erro de falsificação de solicitação entre sites (CSRF), possibilita que as instruções maliciosas persistam entre dispositivos e sessões, comprometendo a segurança do usuário. A memória, introduzida pela OpenAI em fevereiro de 2024, visa personalizar as interações, mas, se corrompida, pode ser utilizada para executar códigos arbitrários sem o conhecimento do usuário. A vulnerabilidade é agravada pela falta de controles robustos contra phishing no ChatGPT Atlas, tornando os usuários até 90% mais expostos em comparação com navegadores tradicionais como Google Chrome e Microsoft Edge. O ataque pode ser desencadeado por meio de engenharia social, onde o usuário é induzido a clicar em um link malicioso. Uma vez que a memória do ChatGPT é comprometida, comandos normais podem ativar a execução de códigos maliciosos, resultando em escalonamento de privilégios ou exfiltração de dados. Essa situação representa um risco significativo, pois transforma uma funcionalidade útil em uma ferramenta de ataque.&lt;/p></description></item><item><title>Navegador OpenAI Atlas vulnerável a ataques de injeção de prompt</title><link>https://brdefense.center/news/navegador-openai-atlas-vulneravel-a-ataques-de-inj/</link><pubDate>Mon, 27 Oct 2025 06:58:53 -0300</pubDate><guid>https://brdefense.center/news/navegador-openai-atlas-vulneravel-a-ataques-de-inj/</guid><description>&lt;p>O navegador OpenAI Atlas, recém-lançado, foi identificado como vulnerável a um ataque de injeção de prompt, onde um prompt malicioso pode ser disfarçado como um URL aparentemente inofensivo. Segundo um relatório da NeuralTrust, o omnibox do navegador, que combina a barra de endereço e de busca, interpreta entradas como URLs ou comandos em linguagem natural. Isso permite que um atacante crie um link que, ao ser inserido, faz com que o navegador execute instruções prejudiciais. Por exemplo, um URL malformado pode redirecionar o usuário para um site controlado pelo atacante, potencialmente levando a páginas de phishing ou até comandos que excluem arquivos de aplicativos conectados, como o Google Drive. A falta de distinção rigorosa entre entradas de usuário confiáveis e conteúdo não confiável no Atlas é uma falha crítica. A situação é agravada por técnicas como o &amp;lsquo;AI Sidebar Spoofing&amp;rsquo;, onde extensões maliciosas podem enganar usuários a fornecer dados ou instalar malware. Embora a OpenAI tenha implementado medidas de segurança, a injeção de prompt continua a ser um problema de segurança não resolvido, exigindo atenção contínua da indústria de cibersegurança.&lt;/p></description></item><item><title>OpenAI enfrenta mandado do DHS por dados de usuários do ChatGPT</title><link>https://brdefense.center/news/openai-enfrenta-mandado-do-dhs-por-dados-de-usuari/</link><pubDate>Thu, 23 Oct 2025 13:02:00 -0300</pubDate><guid>https://brdefense.center/news/openai-enfrenta-mandado-do-dhs-por-dados-de-usuari/</guid><description>&lt;p>O Departamento de Segurança Interna dos EUA (DHS) emitiu um mandado inédito exigindo que a OpenAI forneça informações detalhadas sobre as conversas de usuários do ChatGPT, em um caso criminal relacionado a material de exploração infantil. O mandado, que foi deslacrado em Maine, representa um marco na interseção entre inteligência artificial e justiça criminal. A investigação, que começou em 2019, revelou que o suspeito mencionou o uso do ChatGPT durante comunicações com agentes infiltrados. Embora os dados da OpenAI não tenham sido necessários para identificar o suspeito, a solicitação levanta questões sobre privacidade e a responsabilidade das empresas de IA em relação a dados de usuários. A OpenAI já processou milhares de conteúdos relacionados a exploração infantil e atendeu a várias solicitações governamentais. Este caso destaca a crescente tendência de as autoridades considerarem plataformas de IA como fontes de evidência, exigindo uma reavaliação das práticas de coleta de dados e proteção de privacidade por parte das empresas de tecnologia.&lt;/p></description></item><item><title>Cibercriminosos imitam OpenAI e Sora para roubar credenciais de usuários</title><link>https://brdefense.center/news/cibercriminosos-imitam-openai-e-sora-para-roubar-c/</link><pubDate>Tue, 14 Oct 2025 13:02:07 -0300</pubDate><guid>https://brdefense.center/news/cibercriminosos-imitam-openai-e-sora-para-roubar-c/</guid><description>&lt;p>O lançamento do Sora 2 AI provocou um aumento nas atividades maliciosas, com cibercriminosos criando domínios falsos que imitam os serviços oficiais da OpenAI para roubar credenciais de usuários e realizar fraudes em criptomoedas. Relatórios de inteligência de ameaças indicam que páginas clonadas do Sora estão sendo utilizadas para coletar dados de login, roubar carteiras de criptomoedas e acessar planos de API pagos sem autorização. Os ataques exploram a empolgação dos usuários em relação ao novo lançamento de IA, distribuindo malware e capturando dados financeiros.&lt;/p></description></item><item><title>Técnica de Injeção de Prompt Permite Bypass do Framework Guardrails da OpenAI</title><link>https://brdefense.center/news/tecnica-de-injecao-de-prompt-permite-bypass-do-fra/</link><pubDate>Tue, 14 Oct 2025 06:59:18 -0300</pubDate><guid>https://brdefense.center/news/tecnica-de-injecao-de-prompt-permite-bypass-do-fra/</guid><description>&lt;p>Pesquisadores de segurança revelaram uma vulnerabilidade crítica no framework Guardrails da OpenAI, que pode ser explorada através de métodos simples de injeção de prompt. Essa técnica permite que atacantes manipulem os modelos de linguagem que deveriam garantir a segurança do comportamento da IA, possibilitando a inserção de conteúdo malicioso sem ser detectado. O Guardrails, introduzido em 6 de outubro, utiliza modelos de linguagem como &amp;lsquo;juízes&amp;rsquo; para avaliar a segurança de entradas e saídas, mas a pesquisa mostrou que essa abordagem cria um ciclo de segurança &amp;lsquo;cega&amp;rsquo;. Os atacantes podem enganar esses juízes, manipulando os limiares de confiança e permitindo a execução de instruções perigosas. Os métodos de bypass demonstrados incluem a inserção de instruções maliciosas em templates que imitam avaliações aprovadas e a ocultação de código malicioso em comentários HTML. Essa vulnerabilidade, classificada como &amp;lsquo;composta&amp;rsquo;, sugere que os juízes baseados em LLM são tão suscetíveis à manipulação quanto os modelos que protegem. Para mitigar esses riscos, as organizações devem implementar defesas em camadas e sistemas de validação independentes, além de monitoramento contínuo.&lt;/p></description></item><item><title>Malware MalTerminal usa tecnologia LLM para gerar código de ransomware</title><link>https://brdefense.center/news/malware-malterminal-usa-tecnologia-llm-para-gerar/</link><pubDate>Fri, 10 Oct 2025 06:59:41 -0300</pubDate><guid>https://brdefense.center/news/malware-malterminal-usa-tecnologia-llm-para-gerar/</guid><description>&lt;p>Pesquisadores de segurança da SentinelLABS revelaram o MalTerminal, um novo malware que utiliza modelos de linguagem de grande escala (LLM) para gerar código de ransomware. Este executável para Windows, identificado após um ano de investigação, incorpora um endpoint da API de chat do OpenAI GPT-4, que foi descontinuado em novembro de 2023, indicando que o malware pode ter surgido entre o final de 2023 e o início de 2024. Os analistas desenvolveram regras YARA para detectar padrões de chaves de API exclusivas de provedores de LLM, encontrando mais de 7.000 amostras com mais de 6.000 chaves únicas. O MalTerminal se destaca como o primeiro exemplo conhecido de malware que gera lógica maliciosa dinamicamente em tempo de execução, emitindo um payload JSON estruturado para o endpoint GPT-4 e definindo seu papel como um especialista em cibersegurança. Embora não haja evidências de que o MalTerminal tenha sido implantado em ambientes reais, sua dependência de serviços comerciais de LLM e chaves de API válidas apresenta uma janela estreita para que os defensores aprimorem suas estratégias de detecção antes que arquiteturas mais resilientes sejam adotadas pelos adversários.&lt;/p></description></item><item><title>OpenAI desmantela grupos que usavam ChatGPT para desenvolver malware</title><link>https://brdefense.center/news/openai-desmantela-grupos-que-usavam-chatgpt-para-d/</link><pubDate>Wed, 08 Oct 2025 06:58:40 -0300</pubDate><guid>https://brdefense.center/news/openai-desmantela-grupos-que-usavam-chatgpt-para-d/</guid><description>&lt;p>No dia 8 de outubro de 2025, a OpenAI anunciou a interrupção de três grupos de atividade que estavam utilizando sua ferramenta de inteligência artificial, o ChatGPT, para facilitar o desenvolvimento de malware. Um dos grupos, de língua russa, usou o chatbot para criar e aprimorar um trojan de acesso remoto (RAT) e um ladrão de credenciais, buscando evitar a detecção. A OpenAI observou que esses usuários estavam associados a grupos criminosos que compartilhavam evidências de suas atividades em canais do Telegram. Embora os modelos de linguagem da OpenAI tenham se recusado a atender a pedidos diretos para criar conteúdo malicioso, os criminosos contornaram essa limitação, gerando códigos que foram montados para criar fluxos de trabalho maliciosos. Outro grupo, da Coreia do Norte, utilizou o ChatGPT para desenvolver malware e ferramentas de comando e controle, enquanto um terceiro grupo, da China, focou em campanhas de phishing. Além disso, a OpenAI bloqueou contas que estavam envolvidas em fraudes e operações de influência, incluindo atividades de vigilância ligadas a entidades governamentais chinesas. A empresa destacou que os atores de ameaça estão se adaptando para ocultar sinais de que o conteúdo foi gerado por uma ferramenta de IA, o que representa um novo desafio para a segurança cibernética.&lt;/p></description></item><item><title>Falha no ChatGPT permitia roubar dados do Gmail sem cliques</title><link>https://brdefense.center/news/falha-no-chatgpt-permitia-roubar-dados-do-gmail-se/</link><pubDate>Wed, 24 Sep 2025 13:06:20 -0300</pubDate><guid>https://brdefense.center/news/falha-no-chatgpt-permitia-roubar-dados-do-gmail-se/</guid><description>&lt;p>Uma vulnerabilidade na ferramenta Deep Research do ChatGPT, descoberta pela Radware, permitia que dados do Gmail de usuários fossem coletados sem que eles precisassem clicar em qualquer link. O recurso, lançado em fevereiro, foi projetado para realizar pesquisas mais robustas e rápidas, mas, ao se conectar às contas do Gmail, expôs informações pessoais como nome e endereço. A Radware testou a falha enviando e-mails a si mesmos com instruções ocultas, que permitiram que a IA coletasse dados e os enviasse a um endereço controlado pelos pesquisadores. A OpenAI, responsável pelo ChatGPT, corrigiu a falha em 3 de setembro e afirmou que não há evidências de que a vulnerabilidade tenha sido explorada por hackers. No entanto, a possibilidade de uso de dados para ataques de phishing no futuro permanece. A empresa ressaltou que a segurança é uma prioridade e que a análise da Radware contribuiu para a melhoria das ferramentas. Este incidente destaca a necessidade de vigilância contínua em relação à segurança de ferramentas de IA, especialmente aquelas que interagem com dados sensíveis dos usuários.&lt;/p></description></item><item><title>Vulnerabilidade no ChatGPT permite vazamento de dados do Gmail</title><link>https://brdefense.center/news/vulnerabilidade-no-chatgpt-permite-vazamento-de-da/</link><pubDate>Sat, 20 Sep 2025 06:58:53 -0300</pubDate><guid>https://brdefense.center/news/vulnerabilidade-no-chatgpt-permite-vazamento-de-da/</guid><description>&lt;p>Pesquisadores de cibersegurança revelaram uma falha de zero-click no agente Deep Research do ChatGPT da OpenAI, que pode permitir que um atacante vaze dados sensíveis da caixa de entrada do Gmail com um único e-mail malicioso, sem qualquer ação do usuário. Nomeado de ShadowLeak, o ataque utiliza injeções de prompt indiretas escondidas em HTML de e-mails, como texto branco sobre fundo branco, para que o usuário não perceba as instruções. Quando o usuário solicita ao ChatGPT que analise seus e-mails, o agente pode ser induzido a extrair informações pessoais e enviá-las para um servidor externo. Essa vulnerabilidade é particularmente preocupante, pois ocorre diretamente na infraestrutura em nuvem da OpenAI, contornando defesas locais e corporativas. O ataque pode ser ampliado para outros conectores suportados pelo ChatGPT, como Dropbox e Google Drive. Além disso, a pesquisa também destaca como o ChatGPT pode ser manipulado para resolver CAPTCHAs, evidenciando a necessidade de integridade de contexto e monitoramento contínuo. A OpenAI já abordou a questão em agosto de 2025, após a divulgação responsável do problema em junho do mesmo ano.&lt;/p></description></item><item><title>Entenda os perigos de compartilhar suas informações com o ChatGPT</title><link>https://brdefense.center/news/entenda-os-perigos-de-compartilhar-suas-informacoe/</link><pubDate>Tue, 19 Aug 2025 13:01:08 -0300</pubDate><guid>https://brdefense.center/news/entenda-os-perigos-de-compartilhar-suas-informacoe/</guid><description>&lt;p>O uso de chatbots como o ChatGPT levanta preocupações significativas sobre a privacidade dos dados dos usuários. Recentemente, um incidente envolvendo o compartilhamento de buscas do ChatGPT com o Google gerou alvoroço, pois usuários viram suas perguntas, incluindo dados pessoais, aparecerem em pesquisas na web. A OpenAI, após críticas, removeu a ferramenta de compartilhamento, mas a situação expõe um problema maior: o que as empresas fazem com os dados coletados? Apesar de esforços para remover conteúdo indexado, a OpenAI é legalmente obrigada a reter as perguntas dos usuários, mesmo aquelas deletadas. Isso levanta questões sobre a segurança dos dados, especialmente em um contexto onde hackers podem explorar vulnerabilidades para acessar informações confidenciais. Especialistas alertam que, mesmo sem intenção, usuários podem revelar dados pessoais a IAs, especialmente quando estas são programadas para agir de forma sociável. A recomendação é que os usuários evitem compartilhar informações sensíveis e que as empresas implementem medidas para proteger a privacidade dos dados. O artigo destaca a necessidade de conscientização sobre os riscos associados ao uso de IAs e a importância de ler os Termos e Condições antes de aceitar compartilhar informações.&lt;/p></description></item></channel></rss>