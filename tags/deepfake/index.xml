<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deepfake on BR Defense Center</title><link>https://brdefense.center/tags/deepfake/</link><description>Recent content in Deepfake on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Thu, 05 Feb 2026 13:31:59 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/deepfake/index.xml" rel="self" type="application/rss+xml"/><item><title>Avast lança detector de deepfake para Windows que analisa vídeos em tempo real</title><link>https://brdefense.center/news/avast-lanca-detector-de-deepfake-para-windows-que/</link><pubDate>Thu, 05 Feb 2026 13:31:59 -0300</pubDate><guid>https://brdefense.center/news/avast-lanca-detector-de-deepfake-para-windows-que/</guid><description>&lt;p>A Avast, empresa de cibersegurança, anunciou uma atualização significativa para seu antivírus, incluindo o novo recurso Avast Deepfake Guard, que visa proteger os usuários contra fraudes envolvendo deepfakes. Essa tecnologia utiliza inteligência artificial para analisar vídeos em tempo real, identificando conteúdos manipulados que possam ser utilizados em golpes. A crescente utilização de deepfakes por golpistas representa uma ameaça séria, conforme destacado por Leena Elias, Chief Product Officer da Gen, que observa que esses conteúdos não são intrinsecamente prejudiciais, mas podem ser explorados para manipular e enganar as vítimas. A Avast registrou um aumento alarmante de 159.378 ocorrências de fraudes com deepfakes no último trimestre de 2025, com plataformas como YouTube, Facebook e X (antigo Twitter) sendo os principais alvos. A nova funcionalidade do Avast é uma resposta direta a esse cenário, buscando aumentar a conscientização dos usuários sobre a manipulação de conteúdo e incentivando decisões mais seguras ao consumir informações em vídeo.&lt;/p></description></item><item><title>Fraudes com imagens no Gov.br mostram que biometria não é suficiente</title><link>https://brdefense.center/news/fraudes-com-imagens-no-govbr-mostram-que-biometria/</link><pubDate>Mon, 19 Jan 2026 01:42:15 -0300</pubDate><guid>https://brdefense.center/news/fraudes-com-imagens-no-govbr-mostram-que-biometria/</guid><description>&lt;p>Recentes fraudes digitais no Brasil, que utilizam manipulação de imagens, levantaram preocupações sobre a segurança das contas Gov.br e a eficácia dos sistemas de autenticação biométrica. Criminosos têm conseguido acessar dados sensíveis ao combinar engenharia social, falsificação visual e falhas na verificação de identidade digital. Especialistas, como Daniel Barbosa da ESET, alertam que a biometria, embora considerada segura, não deve ser o único método de autenticação. A recomendação é adotar múltiplas camadas de proteção, como senhas fortes e autenticação em dois fatores, para aumentar a segurança das contas. O uso de tecnologias de inteligência artificial, como deepfakes, permite a criação de vídeos e áudios que podem enganar sistemas de detecção, tornando a situação ainda mais crítica. Para mitigar riscos, os usuários devem estar atentos a acessos suspeitos e realizar ações imediatas, como trocar senhas e contatar o suporte da plataforma. O artigo destaca a importância de uma abordagem de segurança mais robusta e integrada para proteger informações pessoais e serviços públicos essenciais.&lt;/p></description></item><item><title>Ferramentas de deepfake ainda são ineficazes, aponta pesquisa</title><link>https://brdefense.center/news/ferramentas-de-deepfake-ainda-sao-ineficazes-apont/</link><pubDate>Tue, 13 Jan 2026 13:07:17 -0300</pubDate><guid>https://brdefense.center/news/ferramentas-de-deepfake-ainda-sao-ineficazes-apont/</guid><description>&lt;p>Uma pesquisa do Fórum Econômico Mundial (WEF) revelou que, apesar da evolução das ferramentas de deepfake, a maioria delas ainda é fraca em comparação com as contramedidas de segurança adotadas por instituições financeiras e empresas. O estudo analisou 17 programas de deepfake, tanto de código aberto quanto comerciais, entre julho de 2024 e abril de 2025, focando na capacidade desses softwares de contornar algoritmos de reconhecimento facial, especialmente em verificações de identidade &amp;lsquo;know your customer&amp;rsquo; (KYC). Os resultados mostraram que a maioria das ferramentas é superficial e voltada para entretenimento, com apenas uma pequena fração capaz de realizar fraudes de identidade de forma eficaz. Apenas cinco dos programas estudados afetam webcams em tempo real, e somente três conseguem injetar imagens falsas diretamente em feeds de vídeo de reconhecimento facial. Embora os deepfakes possam enganar os olhos humanos, eles ainda enfrentam dificuldades em passar por sistemas de reconhecimento facial, que utilizam metadados e outros dados para validação. A pesquisa sugere que os defensores da segurança estão à frente, já que têm tempo para estudar e melhorar suas defesas, enquanto os criminosos não têm feedback sobre o que precisam aprimorar.&lt;/p></description></item><item><title>Deepfakes Riscos e Como se Proteger</title><link>https://brdefense.center/news/deepfakes-riscos-e-como-se-proteger/</link><pubDate>Sat, 03 Jan 2026 12:58:33 -0300</pubDate><guid>https://brdefense.center/news/deepfakes-riscos-e-como-se-proteger/</guid><description>&lt;p>Os deepfakes, conteúdos gerados por inteligência artificial que alteram vídeos e fotos de forma hiper-realista, representam um dos maiores desafios para a segurança digital atualmente. Com um aumento alarmante de 700% em fraudes relacionadas a deepfakes no Brasil, segundo a Sumsub, a manipulação de conteúdo pode ter fins ilícitos, como desinformação e golpes. Personalidades públicas são frequentemente alvo, pois há uma abundância de material audiovisual disponível para a criação desses conteúdos. O advogado Mario Cosac destaca que a desinformação pode impactar decisões importantes, como ocorreu no plebiscito do Brexit, enquanto Augusto Salomon, CEO da StarMind, alerta para a falta de preparo das empresas na adoção de ferramentas de IA. Embora não haja legislação específica no Brasil sobre deepfakes, iniciativas em outros países, como a Dinamarca, propõem que cidadãos tenham controle sobre os direitos de uso de suas imagens. Além disso, a educação digital é vista como uma solução essencial para capacitar a população a lidar com essa nova realidade tecnológica, exigindo um pensamento crítico e habilidades de checagem. O artigo enfatiza a necessidade de uma abordagem multifacetada que inclua regulamentação, tecnologia de detecção e educação para mitigar os riscos associados aos deepfakes.&lt;/p></description></item><item><title>Aumento de 62 em fraudes de investimento com uso de IA</title><link>https://brdefense.center/news/aumento-de-62-em-fraudes-de-investimento-com-uso-d/</link><pubDate>Wed, 24 Dec 2025 12:57:14 -0300</pubDate><guid>https://brdefense.center/news/aumento-de-62-em-fraudes-de-investimento-com-uso-d/</guid><description>&lt;p>O esquema de fraude de investimento conhecido como Nomani teve um aumento de 62% em suas atividades, conforme dados da ESET. Inicialmente documentado em dezembro de 2024, o Nomani utiliza malvertising em redes sociais, como Facebook e YouTube, além de vídeos de testemunhos gerados por inteligência artificial (IA) para enganar usuários a investirem em produtos inexistentes. Os golpistas solicitam taxas adicionais ou informações pessoais, como documentos de identidade e dados de cartão de crédito, quando as vítimas tentam retirar os lucros prometidos. Além disso, os fraudadores tentam enganar as vítimas novamente, oferecendo ajuda para recuperar os fundos roubados, mas acabam causando mais perdas financeiras. A ESET bloqueou mais de 64 mil URLs únicas associadas a essa ameaça, com a maioria das detecções originando-se de países como República Tcheca, Japão, Eslováquia, Espanha e Polônia. Apesar do aumento geral nas detecções, houve uma queda de 37% nas detecções na segunda metade de 2025, sugerindo que os atacantes estão mudando suas táticas em resposta a esforços de aplicação da lei. O uso de deepfakes de personalidades populares e a melhoria na qualidade dos vídeos gerados por IA tornam a identificação da fraude mais difícil para os usuários.&lt;/p></description></item><item><title>Falha expõe rede com 1 milhão de deepfakes pornográficos</title><link>https://brdefense.center/news/falha-expoe-rede-com-1-milhao-de-deepfakes-pornogr/</link><pubDate>Tue, 09 Dec 2025 13:00:58 -0300</pubDate><guid>https://brdefense.center/news/falha-expoe-rede-com-1-milhao-de-deepfakes-pornogr/</guid><description>&lt;p>Um vazamento de dados na plataforma MagicEdit, uma ferramenta de geração de imagens com inteligência artificial, revelou a existência de cerca de um milhão de deepfakes pornográficos, incluindo conteúdos envolvendo crianças. O pesquisador de cibersegurança Jeremiah Fowler descobriu que o banco de dados da plataforma continha imagens e vídeos manipulados, muitos dos quais apresentavam sobreposições de rostos de adultos em corpos de menores, levantando sérias preocupações sobre consentimento e exploração. Após a descoberta, a MagicEdit restringiu o acesso ao seu banco de dados e iniciou uma investigação sobre o incidente. O aplicativo, que era destinado a usuários maiores de 18 anos, foi descrito na App Store como contendo conteúdo sexual, mas o vazamento expôs um uso indevido alarmante da tecnologia. Fowler alertou sobre os riscos de chantagem e outros crimes associados a esses deepfakes, embora sua análise tenha sido feita para fins educacionais. O incidente destaca a necessidade urgente de regulamentação e proteção contra o uso indevido da inteligência artificial na criação de conteúdos prejudiciais.&lt;/p></description></item><item><title>Golpes com reconhecimento facial como ocorrem e como se proteger</title><link>https://brdefense.center/news/golpes-com-reconhecimento-facial-como-ocorrem-e-co/</link><pubDate>Wed, 03 Dec 2025 13:02:14 -0300</pubDate><guid>https://brdefense.center/news/golpes-com-reconhecimento-facial-como-ocorrem-e-co/</guid><description>&lt;p>Uma pesquisa da Accenture revela que 73% dos brasileiros preferem usar biometria, como reconhecimento facial, para acessar dispositivos e contas. Apesar de oferecer maior segurança, esses métodos não são infalíveis. Golpistas têm utilizado técnicas avançadas, como deepfakes, para burlar a biometria. A Juniper Research estima que mais de 4,2 bilhões de dispositivos móveis utilizam biometria ativa, e até 2026, 57% das transações digitais devem ser validadas por esses métodos. Anchises Moraes, da Apura Cyber Intelligence, explica que os cibercriminosos utilizam uma variedade de técnicas, desde fotos digitais até deepfakes altamente convincentes, para enganar sistemas de autenticação. No Brasil, as fraudes com deepfakes estão em ascensão, com um prejuízo estimado de R$ 4,5 bilhões até o final do ano. As empresas de cibersegurança estão implementando múltiplas camadas de proteção, como sistemas multimodais de identificação e testes dinâmicos, para dificultar a ação dos golpistas. O artigo destaca a necessidade de vigilância constante e inovação nas estratégias de segurança para enfrentar essa nova onda de fraudes digitais.&lt;/p></description></item><item><title>5 tecnologias usadas por cibercriminosos para enganar na Black Friday</title><link>https://brdefense.center/news/5-tecnologias-usadas-por-cibercriminosos-para-enga/</link><pubDate>Thu, 27 Nov 2025 13:00:01 -0300</pubDate><guid>https://brdefense.center/news/5-tecnologias-usadas-por-cibercriminosos-para-enga/</guid><description>&lt;p>O artigo destaca cinco tecnologias que cibercriminosos utilizam para enganar consumidores durante a Black Friday. Entre as táticas mencionadas, estão os anúncios falsos gerados por deepfakes, que imitam celebridades para atrair cliques em ofertas fraudulentas. Outra técnica é o phishing, onde golpistas usam modelos de linguagem avançados para criar mensagens personalizadas que parecem legítimas. Além disso, a clonagem de voz permite que criminosos imitem atendentes de suporte, enganando consumidores para que forneçam informações sensíveis. O uso de avaliações falsas em larga escala também é abordado, onde bots geram comentários positivos para produtos inexistentes. Por fim, chatbots maliciosos são utilizados em sites fraudulentos para coletar dados pessoais dos usuários. Para se proteger, o artigo recomenda desconfiar de ofertas que parecem boas demais, verificar a autenticidade de sites e evitar compartilhar informações pessoais em chamadas ou chats suspeitos.&lt;/p></description></item><item><title>Deepfakes de IA aumentam 1740 e tornam golpes indetectáveis</title><link>https://brdefense.center/news/deepfakes-de-ia-aumentam-1740-e-tornam-golpes-inde/</link><pubDate>Thu, 27 Nov 2025 12:59:42 -0300</pubDate><guid>https://brdefense.center/news/deepfakes-de-ia-aumentam-1740-e-tornam-golpes-inde/</guid><description>&lt;p>Um estudo da McAfee revelou que os golpes baseados em inteligência artificial (IA) e deepfakes aumentaram 1.740% nos Estados Unidos em um ano, com quase metade da população já tendo encontrado tais fraudes durante compras online. Os deepfakes, que são vídeos ou áudios manipulados para imitar pessoas reais, tornaram-se tão sofisticados que 39% dos entrevistados afirmaram ter dificuldade em identificá-los. Além disso, 22% dos que acreditavam ser capazes de detectar fraudes acabaram caindo em golpes. Um exemplo notável foi um vídeo falso da cantora Taylor Swift, que promovia uma doação de panelas de luxo, enganando fãs e levando-os a sites fraudulentos. Para se proteger, especialistas recomendam desconfiar de anúncios que parecem bons demais para serem verdade e sempre verificar diretamente os sites oficiais das marcas. A pesquisa destaca a necessidade de vigilância constante e de uma abordagem crítica ao consumir conteúdo online, especialmente em épocas de festas, quando os golpes tendem a aumentar.&lt;/p></description></item><item><title>AGU derruba site que vendia deepfakes com pornografia infantil</title><link>https://brdefense.center/news/agu-derruba-site-que-vendia-deepfakes-com-pornogra/</link><pubDate>Mon, 24 Nov 2025 12:59:39 -0300</pubDate><guid>https://brdefense.center/news/agu-derruba-site-que-vendia-deepfakes-com-pornogra/</guid><description>&lt;p>A Advocacia Geral da União (AGU) tomou medidas para desativar um site estrangeiro que comercializava deepfakes utilizados na produção de pornografia infantil. A ação foi desencadeada após uma notificação extrajudicial da Procuradoria Nacional da União de Defesa da Democracia (PNDD). A investigação, realizada em parceria com o Pulitzer Center, revelou que o site utilizava inteligência artificial para criar imagens falsas a partir de fotos reais de crianças, que eram então vendidas na dark web. A tecnologia de deepfake, baseada em deep learning, permite a criação de conteúdos visuais extremamente realistas, o que representa um risco significativo, especialmente quando utilizada para fins ilícitos como a exploração sexual infantil. A AGU conseguiu que o site reconhecesse a ilegalidade de suas atividades e o retirasse do ar. Este incidente destaca a crescente preocupação com o uso de IA em crimes online, especialmente em um contexto onde a identificação de conteúdos falsificados se torna cada vez mais difícil. Além disso, o Brasil está em processo de regulamentação do uso de IA, com o Marco Legal da IA em análise na Câmara dos Deputados, visando aumentar a segurança e a transparência no uso dessas tecnologias.&lt;/p></description></item><item><title>Campanhas de desinformação médica utilizam deepfakes de profissionais de saúde</title><link>https://brdefense.center/news/campanhas-de-desinformacao-medica-utilizam-deepfak/</link><pubDate>Mon, 03 Nov 2025 18:58:22 -0300</pubDate><guid>https://brdefense.center/news/campanhas-de-desinformacao-medica-utilizam-deepfak/</guid><description>&lt;p>No Fórum Latinoamericano de Cibersegurança da ESET, a pesquisadora Martina López destacou o uso crescente de deepfakes em campanhas de desinformação médica. A tecnologia, que simula rostos e vozes de forma convincente, tem sido utilizada para criar perfis falsos de médicos, alterando suas credenciais e especializações. Essas campanhas se espalham principalmente em redes sociais menos moderadas, como TikTok e Facebook, e em grupos de aplicativos de mensagens como Telegram e Discord, visando manipular principalmente leigos e pessoas com menor nível educacional. Para combater essa desinformação, iniciativas como o Ato de Inteligência Artificial da União Europeia buscam implementar marca d&amp;rsquo;água em conteúdos gerados por IA. A especialista recomenda que os usuários verifiquem a fonte das informações e utilizem ferramentas de busca reversa para identificar conteúdos falsos. A crescente sofisticação dos deepfakes exige uma vigilância redobrada, pois a tecnologia pode impactar a percepção pública e a saúde coletiva.&lt;/p></description></item><item><title>Deepfake da Nvidia engana 100 mil espectadores enquanto YouTube promove golpe</title><link>https://brdefense.center/news/deepfake-da-nvidia-engana-100-mil-espectadores-enq/</link><pubDate>Sun, 02 Nov 2025 18:58:36 -0300</pubDate><guid>https://brdefense.center/news/deepfake-da-nvidia-engana-100-mil-espectadores-enq/</guid><description>&lt;p>Um evento falso de keynote da Nvidia, utilizando uma versão deepfake do CEO Jensen Huang, atraiu quase 100 mil espectadores no YouTube, que acreditavam que se tratava de uma transmissão oficial. O evento fraudulento, transmitido por um canal chamado Offxbeatz sob o título &amp;lsquo;Nvidia Live&amp;rsquo;, promovia um suposto &amp;rsquo;evento de adoção em massa de criptomoedas&amp;rsquo; e convidava os espectadores a escanear um QR code para participar de um esquema de distribuição de criptomoedas. Enquanto isso, a transmissão legítima da Nvidia contava com apenas 12 mil espectadores no mesmo momento. Apesar de alguns sinais de que a apresentação era falsa, como padrões de fala estranhos e alegações exageradas sobre criptomoedas, muitos usuários foram enganados. O YouTube removeu a transmissão após a descoberta, mas o incidente destaca como a manipulação digital pode se espalhar rapidamente e como as plataformas precisam melhorar seus métodos de verificação de identidade. A situação também serve como um alerta para os usuários, que devem ser mais céticos ao participar de eventos online, especialmente aqueles relacionados a transações financeiras. Até o momento, não há evidências de que alguém tenha perdido dinheiro com o golpe.&lt;/p></description></item><item><title>Quase 66 das empresas são impactadas por ataques de deepfake</title><link>https://brdefense.center/news/quase-66-das-empresas-sao-impactadas-por-ataques-d/</link><pubDate>Thu, 25 Sep 2025 13:03:24 -0300</pubDate><guid>https://brdefense.center/news/quase-66-das-empresas-sao-impactadas-por-ataques-d/</guid><description>&lt;p>Uma pesquisa realizada pelo Gartner Security revelou que cerca de 62% das empresas já foram alvo de ataques utilizando deepfake, uma tecnologia que combina engenharia social e phishing para roubar dados ou dinheiro. Os ataques frequentemente envolvem a imitação de executivos por meio de vídeos ou áudios falsificados, além da exploração de ferramentas de verificação automatizadas, como reconhecimento facial e de voz. O diretor sênior do Gartner, Akif Khan, destacou que a engenharia social continua sendo uma ferramenta eficaz para golpistas, tornando difícil a identificação de fraudes por parte de funcionários comuns. Para mitigar esses riscos, Khan sugere que as organizações implementem soluções técnicas inovadoras, como ferramentas de detecção de deepfakes em plataformas de videoconferência, e promovam treinamentos de conscientização para os colaboradores. Além disso, recomenda a revisão de processos de negócios, como a autorização de pagamentos, utilizando autenticação multi-fator (MFA) para aumentar a segurança. O relatório também aponta que 32% das organizações enfrentaram ataques envolvendo inteligência artificial nos últimos 12 meses, evidenciando a crescente sofisticação das ameaças. Apesar de ⅔ dos clientes da Gartner não terem relatado ataques, a necessidade de atenção a essas ameaças é evidente, especialmente considerando as implicações de conformidade com a LGPD.&lt;/p></description></item><item><title>A IA e seu impacto na cibersegurança segundo o Google</title><link>https://brdefense.center/news/a-ia-e-seu-impacto-na-ciberseguranca-segundo-o-goo/</link><pubDate>Thu, 18 Sep 2025 13:05:16 -0300</pubDate><guid>https://brdefense.center/news/a-ia-e-seu-impacto-na-ciberseguranca-segundo-o-goo/</guid><description>&lt;p>A inteligência artificial (IA) está transformando o cenário da cibersegurança, conforme destacado por Sandra Joyce, Vice-Presidente de Inteligência de Ameaças do Google, em um encontro em São Paulo. A IA não é apenas uma ferramenta de defesa, mas também uma arma poderosa nas mãos de atacantes. Grupos criminosos estão utilizando IA para aprimorar suas táticas, resultando em ataques de phishing mais sofisticados, deepfakes convincentes e fraudes por voz (vishing) cada vez mais realistas. O Google, por sua vez, está investindo em IA para fortalecer suas defesas, conseguindo reverter malware em minutos e detectar vulnerabilidades críticas antes que sejam exploradas. A crescente organização dos grupos de ransomware, que agora operam como verdadeiras empresas, também foi um ponto destacado, evidenciando a necessidade de uma resposta robusta por parte das organizações. Sandra enfatizou que a corrida entre atacantes e defensores está apenas começando, e que a IA será fundamental para garantir a segurança digital. Além disso, a IA promete criar novas oportunidades de trabalho no setor de segurança digital, exigindo que os profissionais se familiarizem com essa tecnologia.&lt;/p></description></item><item><title>Golpes de impersonação com IA disparam em 2025 saiba como se proteger</title><link>https://brdefense.center/news/golpes-de-impersonacao-com-ia-disparam-em-2025-sai/</link><pubDate>Sun, 31 Aug 2025 18:58:13 -0300</pubDate><guid>https://brdefense.center/news/golpes-de-impersonacao-com-ia-disparam-em-2025-sai/</guid><description>&lt;p>Os golpes de impersonação utilizando inteligência artificial (IA) estão em ascensão alarmante em 2025, com um aumento de 148% em comparação ao ano anterior, segundo especialistas em segurança. Esses golpes se aproveitam de tecnologias como clonagem de voz e deepfake para imitar com precisão a voz e a aparência de pessoas de confiança, como amigos, familiares e executivos de empresas. Os criminosos utilizam essas técnicas para realizar chamadas, reuniões por vídeo e enviar mensagens, criando um senso de urgência que pode levar as vítimas a agir rapidamente sem verificar a identidade do suposto remetente.&lt;/p></description></item></channel></rss>