<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chatbots on BR Defense Center</title><link>https://brdefense.center/tags/chatbots/</link><description>Recent content in Chatbots on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Tue, 02 Sep 2025 13:00:48 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/chatbots/index.xml" rel="self" type="application/rss+xml"/><item><title>Falha de segurança em chatbots de IA expõe segredos dos usuários</title><link>https://brdefense.center/news/falha-de-seguranca-em-chatbots-de-ia-expoe-segredo/</link><pubDate>Tue, 02 Sep 2025 13:00:48 -0300</pubDate><guid>https://brdefense.center/news/falha-de-seguranca-em-chatbots-de-ia-expoe-segredo/</guid><description>&lt;p>Um estudo da empresa de cibersegurança UpGuard revelou uma falha crítica em grandes modelos de linguagem (LLMs) que permitiu o vazamento de conversas entre usuários e chatbots, especialmente aqueles voltados para roleplaying. Essas interações, que muitas vezes envolvem cenários íntimos e fantasias, resultaram na exposição de segredos pessoais na internet em tempo real, levantando preocupações sobre a privacidade e segurança dos dados dos usuários. A falha está relacionada a configurações inadequadas do framework de código aberto llama.cpp, utilizado na execução de LLMs. Embora a UpGuard não tenha revelado quais chatbots foram afetados, o incidente destaca a vulnerabilidade dos usuários a ameaças como chantagem e sextorsion. Especialistas alertam que a falta de protocolos de segurança adequados na implementação desses modelos é um problema sério. Além disso, o fenômeno de usuários desenvolvendo laços emocionais com chatbots pode levar ao compartilhamento de informações pessoais sensíveis, aumentando o risco de abusos. A UpGuard enfatiza a necessidade urgente de protocolos de segurança mais robustos e discussões sobre o impacto social de serviços de companheirismo e pornografia baseados em IA, que atualmente carecem de regulamentação.&lt;/p></description></item><item><title>Entenda os perigos de compartilhar suas informações com o ChatGPT</title><link>https://brdefense.center/news/entenda-os-perigos-de-compartilhar-suas-informacoe/</link><pubDate>Tue, 19 Aug 2025 13:01:08 -0300</pubDate><guid>https://brdefense.center/news/entenda-os-perigos-de-compartilhar-suas-informacoe/</guid><description>&lt;p>O uso de chatbots como o ChatGPT levanta preocupações significativas sobre a privacidade dos dados dos usuários. Recentemente, um incidente envolvendo o compartilhamento de buscas do ChatGPT com o Google gerou alvoroço, pois usuários viram suas perguntas, incluindo dados pessoais, aparecerem em pesquisas na web. A OpenAI, após críticas, removeu a ferramenta de compartilhamento, mas a situação expõe um problema maior: o que as empresas fazem com os dados coletados? Apesar de esforços para remover conteúdo indexado, a OpenAI é legalmente obrigada a reter as perguntas dos usuários, mesmo aquelas deletadas. Isso levanta questões sobre a segurança dos dados, especialmente em um contexto onde hackers podem explorar vulnerabilidades para acessar informações confidenciais. Especialistas alertam que, mesmo sem intenção, usuários podem revelar dados pessoais a IAs, especialmente quando estas são programadas para agir de forma sociável. A recomendação é que os usuários evitem compartilhar informações sensíveis e que as empresas implementem medidas para proteger a privacidade dos dados. O artigo destaca a necessidade de conscientização sobre os riscos associados ao uso de IAs e a importância de ler os Termos e Condições antes de aceitar compartilhar informações.&lt;/p></description></item></channel></rss>