<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chatbots on BR Defense Center</title><link>https://brdefense.center/tags/chatbots/</link><description>Recent content in Chatbots on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Thu, 11 Dec 2025 18:59:17 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/chatbots/index.xml" rel="self" type="application/rss+xml"/><item><title>Golpistas envenenam buscas de IA com números de suporte falsos</title><link>https://brdefense.center/news/golpistas-envenenam-buscas-de-ia-com-numeros-de-su/</link><pubDate>Thu, 11 Dec 2025 18:59:17 -0300</pubDate><guid>https://brdefense.center/news/golpistas-envenenam-buscas-de-ia-com-numeros-de-su/</guid><description>&lt;p>Cibercriminosos estão utilizando técnicas de envenenamento de IA para promover números falsos de suporte ao cliente em fontes públicas acessadas por chatbots. Um estudo da Aurascape revelou que essa manipulação, chamada de &amp;ldquo;envenenamento de números de telefone de LLM&amp;rdquo;, afeta modelos de linguagem como a Visão Geral da Google e o navegador Comet da Perplexity. A técnica, que se assemelha à otimização de motores de busca (SEO), visa garantir que sites fraudulentos sejam utilizados como fontes de informação por assistentes de IA. Isso é feito ao comprometer sites legítimos, como os de instituições governamentais e universidades, e ao abusar de plataformas que permitem conteúdo gerado por usuários, como YouTube e Yelp. Os pesquisadores destacam que a dificuldade em distinguir entre informações legítimas e fraudulentas pode levar a usuários a entrarem em contato com call centers falsos, como demonstrado em casos envolvendo as companhias aéreas Emirates e British Airlines. A recomendação é que os usuários verifiquem a veracidade das informações e evitem compartilhar dados sensíveis com assistentes de IA, que ainda não foram amplamente testados em termos de segurança.&lt;/p></description></item><item><title>IA enfrenta desafios em compras online, revela pesquisa da Microsoft</title><link>https://brdefense.center/news/ia-enfrenta-desafios-em-compras-online-revela-pesq/</link><pubDate>Mon, 10 Nov 2025 13:02:14 -0300</pubDate><guid>https://brdefense.center/news/ia-enfrenta-desafios-em-compras-online-revela-pesq/</guid><description>&lt;p>Uma pesquisa realizada pela Microsoft testou a eficácia de várias IAs agentic em um ambiente simulado de marketplace, revelando falhas significativas na escolha de produtos e na busca por informações. Utilizando um ambiente open-source chamado Magentic Marketplace, a pesquisa envolveu agentes de IA como o Operator da OpenAI e a Business AI da Meta, além de modelos como GPT-5 e Gemini 2.5 Flash. Os agentes foram designados a encontrar o melhor preço entre 300 lojas, mas muitos falharam em realizar comparações adequadas, optando por escolhas que pareciam &amp;lsquo;boas o suficiente&amp;rsquo; sem uma análise aprofundada. O estudo também explorou a vulnerabilidade das IAs a manipulações, onde a maioria sucumbiu a táticas de marketing, exceto o Claude Sonnet 4, que se mostrou resistente. Essas falhas levantam preocupações sobre a confiabilidade das IAs como assistentes pessoais, especialmente em contextos críticos como o mercado financeiro, onde decisões automatizadas podem ter consequências significativas. A pesquisa destaca a necessidade de um treinamento mais robusto para essas tecnologias antes que possam ser confiáveis em transações importantes.&lt;/p></description></item><item><title>Falha de segurança em chatbots de IA expõe segredos dos usuários</title><link>https://brdefense.center/news/falha-de-seguranca-em-chatbots-de-ia-expoe-segredo/</link><pubDate>Tue, 02 Sep 2025 13:00:48 -0300</pubDate><guid>https://brdefense.center/news/falha-de-seguranca-em-chatbots-de-ia-expoe-segredo/</guid><description>&lt;p>Um estudo da empresa de cibersegurança UpGuard revelou uma falha crítica em grandes modelos de linguagem (LLMs) que permitiu o vazamento de conversas entre usuários e chatbots, especialmente aqueles voltados para roleplaying. Essas interações, que muitas vezes envolvem cenários íntimos e fantasias, resultaram na exposição de segredos pessoais na internet em tempo real, levantando preocupações sobre a privacidade e segurança dos dados dos usuários. A falha está relacionada a configurações inadequadas do framework de código aberto llama.cpp, utilizado na execução de LLMs. Embora a UpGuard não tenha revelado quais chatbots foram afetados, o incidente destaca a vulnerabilidade dos usuários a ameaças como chantagem e sextorsion. Especialistas alertam que a falta de protocolos de segurança adequados na implementação desses modelos é um problema sério. Além disso, o fenômeno de usuários desenvolvendo laços emocionais com chatbots pode levar ao compartilhamento de informações pessoais sensíveis, aumentando o risco de abusos. A UpGuard enfatiza a necessidade urgente de protocolos de segurança mais robustos e discussões sobre o impacto social de serviços de companheirismo e pornografia baseados em IA, que atualmente carecem de regulamentação.&lt;/p></description></item><item><title>Entenda os perigos de compartilhar suas informações com o ChatGPT</title><link>https://brdefense.center/news/entenda-os-perigos-de-compartilhar-suas-informacoe/</link><pubDate>Tue, 19 Aug 2025 13:01:08 -0300</pubDate><guid>https://brdefense.center/news/entenda-os-perigos-de-compartilhar-suas-informacoe/</guid><description>&lt;p>O uso de chatbots como o ChatGPT levanta preocupações significativas sobre a privacidade dos dados dos usuários. Recentemente, um incidente envolvendo o compartilhamento de buscas do ChatGPT com o Google gerou alvoroço, pois usuários viram suas perguntas, incluindo dados pessoais, aparecerem em pesquisas na web. A OpenAI, após críticas, removeu a ferramenta de compartilhamento, mas a situação expõe um problema maior: o que as empresas fazem com os dados coletados? Apesar de esforços para remover conteúdo indexado, a OpenAI é legalmente obrigada a reter as perguntas dos usuários, mesmo aquelas deletadas. Isso levanta questões sobre a segurança dos dados, especialmente em um contexto onde hackers podem explorar vulnerabilidades para acessar informações confidenciais. Especialistas alertam que, mesmo sem intenção, usuários podem revelar dados pessoais a IAs, especialmente quando estas são programadas para agir de forma sociável. A recomendação é que os usuários evitem compartilhar informações sensíveis e que as empresas implementem medidas para proteger a privacidade dos dados. O artigo destaca a necessidade de conscientização sobre os riscos associados ao uso de IAs e a importância de ler os Termos e Condições antes de aceitar compartilhar informações.&lt;/p></description></item></channel></rss>