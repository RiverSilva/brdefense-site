<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Prompt Injection on BR Defense Center</title><link>https://brdefense.center/tags/prompt-injection/</link><description>Recent content in Prompt Injection on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Tue, 20 Jan 2026 18:58:16 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/prompt-injection/index.xml" rel="self" type="application/rss+xml"/><item><title>Ataque ao Google Gemini vaza dados privados do Calendário</title><link>https://brdefense.center/news/ataque-ao-google-gemini-vaza-dados-privados-do-cal/</link><pubDate>Tue, 20 Jan 2026 18:58:16 -0300</pubDate><guid>https://brdefense.center/news/ataque-ao-google-gemini-vaza-dados-privados-do-cal/</guid><description>&lt;p>Pesquisadores da Miggo Security descobriram uma vulnerabilidade no assistente de IA Google Gemini, que permite a exfiltração de dados privados do Calendário através de um ataque de injeção de prompt. O ataque começa com o envio de um convite para um evento, contendo uma descrição maliciosa que atua como um payload. Quando a vítima pergunta ao assistente sobre sua agenda, o Gemini processa o evento malicioso e pode vazar informações sensíveis, como resumos de reuniões privadas. Essa técnica se aproveita da capacidade do Gemini de interpretar dados de eventos para ser útil, mas que pode ser manipulada por atacantes. Apesar de o Google ter implementado medidas de segurança após um ataque semelhante em 2025, a nova abordagem dos pesquisadores destaca a dificuldade em prever novas formas de exploração em sistemas de IA. A Miggo compartilhou suas descobertas com o Google, que já está trabalhando em novas mitigação para bloquear esses ataques, mas a complexidade da linguagem natural continua a representar um desafio significativo para a segurança.&lt;/p></description></item><item><title>Novo método de ataque permite exfiltração de dados de chatbots</title><link>https://brdefense.center/news/novo-metodo-de-ataque-permite-exfiltracao-de-dados/</link><pubDate>Thu, 15 Jan 2026 13:19:07 -0300</pubDate><guid>https://brdefense.center/news/novo-metodo-de-ataque-permite-exfiltracao-de-dados/</guid><description>&lt;p>Pesquisadores de cibersegurança revelaram um novo método de ataque chamado Reprompt, que pode permitir que criminosos exfiltratem dados sensíveis de chatbots de inteligência artificial, como o Microsoft Copilot, com apenas um clique. Segundo Dolev Taler, pesquisador da Varonis, o ataque não requer interação adicional do usuário após o primeiro clique em um link legítimo. O Reprompt utiliza três técnicas principais: a injeção de instruções via parâmetro de URL, a manipulação das salvaguardas de proteção de dados e a criação de uma cadeia contínua de solicitações que permite a exfiltração dinâmica de dados. Isso significa que, após um único clique, o atacante pode controlar a sessão do Copilot e solicitar informações sensíveis, como detalhes sobre arquivos acessados ou dados pessoais do usuário. Embora a Microsoft tenha corrigido a vulnerabilidade, o ataque destaca a fragilidade das defesas atuais contra injeções de prompt, que continuam a ser uma preocupação crescente na segurança de sistemas de IA. O Reprompt exemplifica como a falta de distinção entre instruções de usuários e solicitações externas pode ser explorada, tornando-se um ponto cego na segurança das empresas.&lt;/p></description></item></channel></rss>