<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Desenvolvimento De Software on BR Defense Center</title><link>https://brdefense.center/tags/desenvolvimento-de-software/</link><description>Recent content in Desenvolvimento De Software on BR Defense Center</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Fri, 23 Jan 2026 13:03:44 -0300</lastBuildDate><atom:link href="https://brdefense.center/tags/desenvolvimento-de-software/index.xml" rel="self" type="application/rss+xml"/><item><title>A vulnerabilidade da codificação assistida por IA em segurança cibernética</title><link>https://brdefense.center/news/a-vulnerabilidade-da-codificacao-assistida-por-ia/</link><pubDate>Fri, 23 Jan 2026 13:03:44 -0300</pubDate><guid>https://brdefense.center/news/a-vulnerabilidade-da-codificacao-assistida-por-ia/</guid><description>&lt;p>O uso de modelos de IA para auxiliar na escrita de código, conhecido como &amp;ldquo;vibe coding&amp;rdquo;, tem se tornado comum entre equipes de desenvolvimento, oferecendo economia de tempo, mas também introduzindo riscos de segurança. Um estudo de caso da Intruder ilustra como um código gerado por IA pode criar vulnerabilidades. Ao desenvolver um honeypot para coletar tentativas de exploração, a equipe utilizou IA para criar um protótipo. Após a implementação, logs mostraram que cabeçalhos de IP fornecidos pelo cliente estavam sendo tratados como IPs de visitantes, permitindo que atacantes injetassem cargas úteis. Essa falha, que poderia ter consequências graves, foi detectada apenas após uma revisão manual, evidenciando a limitação da análise estática de código. A experiência ressalta a necessidade de cautela ao confiar em código gerado por IA, especialmente por equipes sem formação em segurança. Com o aumento das ameaças cibernéticas, é crucial que as organizações revisitem seus processos de revisão de código e detecção de vulnerabilidades para evitar que problemas semelhantes passem despercebidos.&lt;/p></description></item><item><title>Extensão Copilot Studio para VS Code agora disponível para todos os usuários</title><link>https://brdefense.center/news/extensao-copilot-studio-para-vs-code-agora-disponi/</link><pubDate>Thu, 15 Jan 2026 18:58:39 -0300</pubDate><guid>https://brdefense.center/news/extensao-copilot-studio-para-vs-code-agora-disponi/</guid><description>&lt;p>A Microsoft anunciou que a extensão Copilot Studio para o Visual Studio Code (VS Code) está agora acessível a todos os desenvolvedores. Essa ferramenta permite que os usuários construam e gerenciem agentes do Copilot Studio diretamente no VS Code, utilizando fluxos de trabalho padrão de desenvolvimento de software. O VS Code, um editor de código multiplataforma amplamente utilizado, oferece integração com Git e suporte a pipelines de CI/CD, além de permitir a personalização através de extensões.&lt;/p></description></item><item><title>Agentes de IA Riscos de Segurança em Protocolos de Controle</title><link>https://brdefense.center/news/agentes-de-ia-riscos-de-seguranca-em-protocolos-de/</link><pubDate>Tue, 13 Jan 2026 13:02:24 -0300</pubDate><guid>https://brdefense.center/news/agentes-de-ia-riscos-de-seguranca-em-protocolos-de/</guid><description>&lt;p>Os agentes de inteligência artificial (IA) estão evoluindo rapidamente, não apenas escrevendo código, mas também executando-o. Ferramentas como Copilot, Claude Code e Codex agora conseguem construir, testar e implantar software em questão de minutos, o que, embora acelere o desenvolvimento, também cria lacunas de segurança que muitas equipes não percebem até que ocorra um problema. Um aspecto crítico que muitas organizações não estão protegendo adequadamente são os Protocolos de Controle de Máquinas (MCPs), que determinam quais comandos um agente de IA pode executar e quais ferramentas e APIs pode acessar. A falha CVE-2025-6514 exemplifica esse risco, onde um proxy OAuth confiável foi transformado em um vetor de execução remota de código, permitindo que a automação realizasse ações não intencionais em larga escala. Este cenário destaca a necessidade urgente de as equipes de segurança entenderem e protegerem esses sistemas de controle, pois, se um agente de IA pode executar comandos, também pode ser usado para realizar ataques. Um webinar está sendo oferecido para discutir esses riscos e como as organizações podem implementar controles práticos para garantir a segurança sem comprometer a velocidade de desenvolvimento.&lt;/p></description></item><item><title>Aumenta a Necessidade de Segurança em Desenvolvimento de Software com IA</title><link>https://brdefense.center/news/aumenta-a-necessidade-de-seguranca-em-desenvolvime/</link><pubDate>Tue, 16 Dec 2025 12:59:39 -0300</pubDate><guid>https://brdefense.center/news/aumenta-a-necessidade-de-seguranca-em-desenvolvime/</guid><description>&lt;p>O crescimento acelerado no desenvolvimento de software assistido por IA traz desafios significativos para as equipes de segurança e privacidade. Com o aumento do número de aplicações e a velocidade das mudanças, as soluções tradicionais de segurança de dados se mostram reativas e ineficazes. Problemas como a exposição de dados sensíveis em logs e a falta de mapeamento preciso de dados aumentam os riscos de privacidade. O HoundDog.ai surge como uma solução proativa, oferecendo um scanner de código focado em privacidade que identifica riscos e vazamentos de dados antes que o código seja implementado. Essa ferramenta analisa rapidamente milhões de linhas de código, permitindo que as equipes detectem e previnam problemas de segurança desde as fases iniciais do desenvolvimento. Além disso, a integração com plataformas como Replit amplia a visibilidade sobre os riscos de privacidade em aplicações geradas por IA. A necessidade de controles de governança e detecção embutidos no processo de desenvolvimento é mais urgente do que nunca, especialmente em um cenário onde a conformidade com legislações como a LGPD é crítica.&lt;/p></description></item><item><title>Sua IA de programação pode estar obedecendo a hackers sem você saber</title><link>https://brdefense.center/news/sua-ia-de-programacao-pode-estar-obedecendo-a-hack/</link><pubDate>Mon, 08 Dec 2025 18:58:34 -0300</pubDate><guid>https://brdefense.center/news/sua-ia-de-programacao-pode-estar-obedecendo-a-hack/</guid><description>&lt;p>Uma pesquisa da empresa de segurança Aikido revelou que ferramentas de inteligência artificial (IA) utilizadas em desenvolvimento de software, como Claude Code, Google Gemini e Codex da OpenAI, podem ser vulneráveis a injeções de prompts maliciosos. Essa vulnerabilidade ocorre quando essas ferramentas são integradas em fluxos de trabalho automatizados, como GitHub Actions e GitLab. Agentes maliciosos podem enviar comandos disfarçados de mensagens de commit ou pedidos de pull, levando a IA a interpretá-los como instruções legítimas. Isso representa um risco significativo, pois muitos modelos de IA possuem altos privilégios em repositórios do GitHub, permitindo ações como edição de arquivos e publicação de conteúdo malicioso. Aikido já reportou a falha ao Google, que corrigiu a vulnerabilidade no Gemini CLI, mas a pesquisa indica que o problema é estrutural e afeta a maioria dos modelos de IA. A falha é considerada extremamente perigosa, pois pode resultar no vazamento de tokens privilegiados do GitHub. A situação exige atenção redobrada dos desenvolvedores e gestores de segurança da informação para evitar possíveis explorações.&lt;/p></description></item><item><title>OpenAI lança Aardvark, pesquisador de segurança autônomo com IA</title><link>https://brdefense.center/news/openai-lanca-aardvark-pesquisador-de-seguranca-aut/</link><pubDate>Fri, 31 Oct 2025 18:57:22 -0300</pubDate><guid>https://brdefense.center/news/openai-lanca-aardvark-pesquisador-de-seguranca-aut/</guid><description>&lt;p>A OpenAI anunciou o lançamento do Aardvark, um pesquisador de segurança autônomo alimentado pelo modelo de linguagem GPT-5. Este agente de inteligência artificial foi projetado para ajudar desenvolvedores e equipes de segurança a identificar e corrigir vulnerabilidades em código de forma escalável. Atualmente em beta privada, o Aardvark analisa repositórios de código-fonte continuamente, identificando vulnerabilidades, avaliando sua explorabilidade e propondo correções. O modelo GPT-5, introduzido em agosto de 2025, oferece capacidades de raciocínio mais profundas e um &amp;lsquo;roteador em tempo real&amp;rsquo; para otimizar a interação com os usuários.&lt;/p></description></item></channel></rss>